# ADF-10: Monitoring, Debugging, and Troubleshooting - The Complete Guide

## Table of Contents
1. [Introduction - Why Monitoring Matters](#introduction)
2. [Understanding Pipeline Runs and Activity Logs](#pipeline-runs)
3. [Mastering the ADF Monitoring UI](#monitoring-ui)
4. [Common Error Messages and Solutions](#common-errors)
5. [Performance Troubleshooting](#performance-troubleshooting)
6. [Azure Monitor and Log Analytics Integration](#azure-monitor)
7. [Setting Up Custom Alerts](#custom-alerts)
8. [Real-World Troubleshooting Scenarios](#real-world-scenarios)
9. [Interview Questions](#interview-questions)

---

## Introduction - Why Monitoring Matters {#introduction}

### The 3 AM Phone Call Story

Imagine this: It's 3 AM, and you get a call. "The daily sales report didn't arrive. Finance is panicking because they need it for a 7 AM executive meeting." This happened to me once, and guess what saved me? **Proper monitoring and knowing how to debug ADF pipelines quickly.**

### What You'll Learn

Think of monitoring ADF like being a doctor for your data pipelines. You need to:
- **Check vital signs** (pipeline runs, activity status)
- **Read test results** (logs and metrics)
- **Diagnose problems** (error messages and patterns)
- **Prescribe treatment** (fixes and optimizations)
- **Prevent future issues** (alerts and proactive monitoring)

**Pro Tip**: In interviews, when they ask "Tell me about a time you troubleshot a production issue," this chapter gives you the framework to answer confidently.

---

## Understanding Pipeline Runs and Activity Logs {#pipeline-runs}

### What is a Pipeline Run?

**Layman explanation**: Imagine a pipeline run like a train journey. The train (pipeline) has multiple stops (activities). A pipeline run is one complete journey from start to finish, and the logs tell you what happened at each stop.

**Technical explanation**: A pipeline run is a single execution instance of a pipeline, identified by a unique `runId`. Each run contains:
- Start time and end time
- Status (InProgress, Succeeded, Failed, Cancelled)
- Triggered by (manual, schedule, tumbling window, event)
- All activity executions within that run
- Parameters passed to the pipeline
- Duration and resource consumption

### Pipeline Run Statuses

```
Pipeline Run Lifecycle:

Queued → InProgress → Succeeded
                   ↓
                   Failed
                   ↓
                   Cancelled
```

**What each status means:**

1. **Queued**: Your pipeline is waiting for resources. Like standing in line at Starbucks.
   - Common in high-volume environments
   - Multiple pipelines competing for Integration Runtime capacity
   - **Interview tip**: "I've seen queued status when we had insufficient DIUs during peak hours"

2. **InProgress**: Pipeline is actively running
   - Activities are executing
   - You can see which activity is currently running
   - **Real scenario**: "I was monitoring a 3-hour data load, and I could see it was stuck at the Copy activity for 2 hours - that's when I knew something was wrong"

3. **Succeeded**: All activities completed successfully
   - Every activity returned success
   - Data was moved/transformed as expected
   - **Don't assume success = correct**: I once had a pipeline succeed while copying 0 rows!

4. **Failed**: At least one activity failed
   - Pipeline stopped at the failed activity (unless you configured otherwise)
   - Error details available in activity logs
   - **This is where your troubleshooting starts**

5. **Cancelled**: Someone manually stopped it
   - User intervention or timeout
   - Partial data may exist - be careful!

### Reading Activity Logs - The Detective Work

**Scenario**: Your pipeline failed at 2 AM. You come in at 9 AM. Here's how you investigate:

#### Step 1: Navigate to Pipeline Runs
```
ADF Studio → Monitor (left sidebar) → Pipeline runs tab
```

You'll see a table:
```
Run Start    | Pipeline Name      | Status  | Duration | Triggered By
2024-11-16   | Daily_Sales_Load   | Failed  | 15m 23s  | Schedule
  02:00 AM   |                    |         |          |
```

#### Step 2: Click on the Pipeline Name

This opens the **Activity Runs** view - your crime scene:

```
Activity Name        | Type          | Status    | Duration | Start Time
-------------------------------------------------------------------
Lookup_LastLoadDate  | Lookup        | Succeeded | 2s       | 02:00:00
Copy_SalesData       | Copy          | Failed    | 15m 21s  | 02:00:02
Send_Email           | Web           | Skipped   | -        | -
```

**What this tells you**:
- Lookup worked fine (2 seconds - normal)
- Copy activity failed after running for 15 minutes
- Email wasn't sent because pipeline failed before reaching it

#### Step 3: Click on the Failed Activity

Now you see the **Activity Details** - this is gold:

```json
{
  "errorCode": "2200",
  "message": "ErrorCode=UserErrorSqlBulkCopyInvalidColumnLength,
   Detailed Message: The given value of type String from the data source 
   cannot be converted to type nvarchar of the specified target column.",
  "failureType": "UserError",
  "target": "Copy_SalesData",
  "details": "Column 'ProductName' max length is 50, but source has value 
   with length 75"
}
```

**Aha! The problem**: Someone added a product with a really long name, and your SQL table column is too small.

### Deep Dive: Activity Run Details

When you click on an activity (succeeded or failed), you see different tabs:

#### 1. **General Tab**
Shows:
- Activity name and type
- Pipeline run ID and activity run ID
- Start time, end time, duration
- Status and status reason
- Integration Runtime used

**Interview Question You'll Face**: 
"How do you find which Integration Runtime was used for a specific run?"
**Your Answer**: "I go to Monitor → Pipeline runs → Click the pipeline → Select the activity → General tab shows the Integration Runtime name. This helps when troubleshooting connectivity or performance issues specific to an IR."

#### 2. **Input Tab**
Shows the configuration passed to the activity:

```json
{
  "source": {
    "type": "SqlSource",
    "sqlReaderQuery": "SELECT * FROM Sales WHERE OrderDate > '2024-11-15'"
  },
  "sink": {
    "type": "SqlSink",
    "writeBatchSize": 10000
  },
  "enableStaging": false,
  "dataIntegrationUnits": 4
}
```

**Real-world use**: "I once debugged a performance issue and found someone had set dataIntegrationUnits to 2 instead of 32. The Input tab showed me this immediately."

#### 3. **Output Tab**
Shows the result of the activity:

For a **Copy Activity**:
```json
{
  "dataRead": 524288000,
  "dataWritten": 524288000,
  "rowsRead": 1000000,
  "rowsCopied": 1000000,
  "copyDuration": 180,
  "throughput": 2834,
  "errors": [],
  "effectiveIntegrationRuntime": "AutoResolveIntegrationRuntime",
  "usedDataIntegrationUnits": 4,
  "billingReference": {
    "activityType": "DataMovement",
    "billableDuration": [
      {
        "meterType": "AzureIR",
        "duration": 0.03,
        "unit": "DIUHours"
      }
    ]
  }
}
```

**What to look for**:
- **rowsRead vs rowsCopied**: Should match! If not, data loss occurred
- **throughput**: MB/s - helps identify slow copies
- **usedDataIntegrationUnits**: Actual DIUs used (may differ from requested)
- **billingReference**: How much this cost you

**Real scenario**: "I was investigating why our monthly bill jumped 40%. The Output tab showed someone increased DIUs from 4 to 32 for all pipelines, even tiny ones copying 100 rows."

#### 4. **Error Tab** (only for failed activities)
The most important tab when things go wrong:

```json
{
  "errorCode": "2200",
  "message": "ErrorCode=UserErrorSqlBulkCopyInvalidColumnLength...",
  "failureType": "UserError",
  "target": "Copy_SalesData",
  "details": [
    "Column: ProductName",
    "Expected length: 50",
    "Actual length: 75",
    "Row number: 45678",
    "Source value: 'Super Premium Ultra Long Product Name That Exceeds Column Length'"
  ]
}
```

**Pro Tip**: Take screenshots of error details during interviews when discussing troubleshooting. Shows you actually monitored pipelines.

### Activity-Specific Outputs

Different activities give different outputs:

#### Lookup Activity Output:
```json
{
  "count": 1,
  "value": [
    {
      "LastLoadDate": "2024-11-15T00:00:00Z"
    }
  ],
  "effectiveIntegrationRuntime": "AutoResolveIntegrationRuntime"
}
```

**How to use**: `@activity('Lookup_LastLoadDate').output.value[0].LastLoadDate`

#### GetMetadata Activity Output:
```json
{
  "exists": true,
  "itemName": "sales_2024_11_16.csv",
  "itemType": "File",
  "size": 52428800,
  "created": "2024-11-16T01:00:00Z",
  "lastModified": "2024-11-16T01:30:00Z",
  "columnCount": 25,
  "structure": [
    {"name": "OrderID", "type": "Int32"},
    {"name": "CustomerID", "type": "String"}
  ]
}
```

**Real use case**: "Before copying a file, I used GetMetadata to check if it exists and validate its size. One time, the file was 0 bytes - an upstream system issue we caught before loading."

#### ForEach Activity Output:
```json
{
  "iterations": 5,
  "completed": 3,
  "failed": 1,
  "skipped": 0,
  "inProgress": 1
}
```

**Click on the ForEach** to see individual iteration details - crucial for finding which file/table failed in batch processing.

---

## Mastering the ADF Monitoring UI {#monitoring-ui}

### The Main Monitoring Dashboard

Think of the ADF Monitoring UI as your mission control center. Here's the layout:

```
┌─────────────────────────────────────────────────────────────┐
│  [Pipeline runs] [Trigger runs] [Integration runtimes]     │
│  [Debug]        [Alerts]                                    │
├─────────────────────────────────────────────────────────────┤
│  Filters:                                                    │
│  Name: [___________]  Status: [All v]  Start: [Date picker]│
│  End: [Date picker]   Triggered by: [All v]                │
├─────────────────────────────────────────────────────────────┤
│  Pipeline Name    | Status | Start Time | Duration | ...   │
│  ────────────────────────────────────────────────────────── │
│  Daily_Sales      | ✓ Succeeded | 02:00 | 15m      | ...   │
│  Hourly_Inventory | ✗ Failed    | 03:00 | 2m       | ...   │
│  Weekly_Report    | ⟳ Running   | 04:00 | 1h 23m   | ...   │
└─────────────────────────────────────────────────────────────┘
```

### Essential Filters Every ADF Engineer Uses

#### 1. **Time Range Filter**
```
Last 24 hours (default) | Last 7 days | Last 30 days | Custom
```

**Real scenario**: "Production support called about missing data from 2 weeks ago. I changed the filter to 'Last 30 days' and found the pipeline had failed silently because no one was monitoring."

**Pro Tip**: For interviews, say: "I typically filter by 'Last 24 hours' for daily monitoring, but during incident investigation, I extend to 7-30 days to identify patterns."

#### 2. **Status Filter**
```
All | Succeeded | Failed | Cancelled | InProgress | Queued
```

**Best practices**:
- **Morning routine**: Filter by "Failed" to see overnight failures
- **Performance review**: Filter by "Succeeded" and sort by duration to find slow pipelines
- **Capacity planning**: Filter by "Queued" to identify resource bottlenecks

**Interview gold**: "One of my daily tasks was checking for failed pipelines every morning. I'd filter by 'Failed', 'Last 24 hours', and create a report of which pipelines needed attention."

#### 3. **Pipeline Name Search**
```
Search: [Daily_*]  → Shows: Daily_Sales, Daily_Inventory, Daily_Reports
```

**Wildcards work!** You can search:
- `*Sales*` - All pipelines with "Sales" in the name
- `Daily_*` - All pipelines starting with "Daily"
- `*_Load` - All pipelines ending with "_Load"

#### 4. **Triggered By Filter**
```
All | Manual | Schedule | Tumbling Window | Event
```

**Troubleshooting story**: "We had duplicate data loads. I filtered by 'Triggered by: All' and discovered someone was manually triggering pipelines that were already scheduled. The timestamps showed both schedule and manual runs at different times."

### Advanced Monitoring Features

#### Gantt Chart View

**What is it**: Visual timeline of activity execution within a pipeline

```
Activity Timeline (Gantt Chart):
──────────────────────────────────────────────────
Lookup      [==]
            └──> Copy_Table1  [========]
            └──> Copy_Table2  [========]
            └──> Copy_Table3      [========]
──────────────────────────────────────────────────
0s          5s          10s         15s         20s
```

**How to access**: Pipeline run → Click the "Gantt" button (top right)

**What it tells you**:
- Which activities ran in parallel
- Which activities were waiting (dependencies)
- Where bottlenecks occurred

**Real example**: "I used the Gantt chart to prove that our ForEach activity with sequential=true was taking 2 hours because activities ran one after another. After setting sequential=false, it dropped to 20 minutes - all activities ran in parallel."

#### Consumption Report

**What is it**: Shows how much each pipeline costs you

```
ADF Studio → Monitor → Consumption report
```

**Shows**:
- DIU hours consumed
- Pipeline runs count
- Data flow compute hours
- External activity executions
- Self-hosted IR hours

**Finance interview question incoming**: 
"How would you reduce ADF costs?"
**Your answer**: "I'd use the Consumption report to identify expensive pipelines - usually those using high DIUs for small data volumes, or data flows running frequently when batch processing would work. I once reduced costs by 30% by optimizing DIU settings and consolidating frequent small runs into fewer large runs."

#### Metrics View

Real-time metrics dashboard:
- Pipeline runs per minute
- Success rate
- Average duration
- Active runs

**Access**: Monitor → Metrics

**Use case**: "During a major data migration, I kept Metrics open on a second monitor to ensure our pipeline throughput stayed high and catch any sudden spikes in failures."

---

## Common Error Messages and Solutions {#common-errors}

### The Error Message Dictionary

This section is **interview gold**. Memorize these - you'll sound like a 5-year veteran.

---

#### Error Category 1: Connectivity Errors

### 1. **Cannot connect to SQL Server**

**Error Message**:
```
ErrorCode=SqlFailedToConnect
Message: Cannot connect to SQL server: 'myserver.database.windows.net', 
Database: 'mydb', User: 'sqladmin'. Check the linked service configuration 
is correct, and make sure the SQL Database firewall allows the integration 
runtime to access.
```

**Layman explanation**: It's like trying to call someone, but their phone is turned off or they blocked your number.

**Common Causes**:
1. Firewall blocking ADF's IP addresses
2. Wrong credentials
3. SQL Server not allowing Azure services
4. Network connectivity from Self-hosted IR

**Solution Steps**:
```
Step 1: Check SQL Server Firewall
   Azure Portal → SQL Server → Networking → 
   ☑ Allow Azure services and resources to access this server

Step 2: Add ADF Integration Runtime IPs
   Get IR IP: Test connection in Linked Service → Note the IP
   Add to firewall: Add IP range in SQL Server firewall rules

Step 3: Verify credentials
   Try connecting with SQL Server Management Studio using same credentials

Step 4: For Self-hosted IR
   Check IR machine can ping SQL Server
   Test telnet: telnet myserver.database.windows.net 1433
   Check corporate firewall rules
```

**Real story**: "We had a Self-hosted IR that couldn't connect to an on-premises SQL Server after a network change. I used telnet from the IR machine to test port 1433 connectivity. Turned out IT had changed firewall rules. Added the exception, and we were back online in 10 minutes."

**Interview answer template**:
"When I encounter SQL connection errors, I follow a systematic approach:
1. First, I verify the connection string in the Linked Service
2. Then I check Azure SQL firewall rules to ensure ADF's IP is whitelisted
3. I test the connection using 'Test connection' button
4. If using Self-hosted IR, I log into that machine and verify network connectivity
5. Finally, I check audit logs to see if it's an authentication vs. authorization issue"

---

### 2. **The given value of type String from the data source cannot be converted**

**Error Message**:
```
ErrorCode=UserErrorSqlBulkCopyInvalidColumnLength
Message: The given value of type String from the data source cannot be 
converted to type nvarchar of the specified target column.
Detailed: Column 'CustomerName' max length is 100, but source has value 
with length 150.
```

**Layman explanation**: You're trying to fit a size 12 shoe into a size 8 box. It just won't fit.

**Why it happens**:
- Source data has longer strings than destination column allows
- Very common when copying from files (CSV, Parquet) to SQL
- Or when source schema changed but destination didn't

**Solutions**:

**Option 1: Truncate the data**
```json
"translator": {
  "type": "TabularTranslator",
  "mappings": [
    {
      "source": { "name": "CustomerName" },
      "sink": { "name": "CustomerName" },
      "translator": {
        "value": "left($CustomerName, 100)",
        "type": "Expression"
      }
    }
  ]
}
```

**Option 2: Increase column size** (if possible)
```sql
ALTER TABLE Customers 
ALTER COLUMN CustomerName NVARCHAR(200);
```

**Option 3: Add data validation before copy**
```
Use GetMetadata or Data Flow to check max lengths
Add If Condition to stop pipeline if validation fails
Send alert to data source owner
```

**Real scenario**: "This happened when a third-party vendor started sending customer names up to 200 characters, but our table only supported 100. I added a Data Flow transformation to truncate at 100 chars and log the full values to a separate 'overflow' table for later review."

---

### 3. **Self-hosted Integration Runtime Node is Not Available**

**Error Message**:
```
ErrorCode=IntegrationRuntimeNotReachable
Message: The Integration Runtime 'OnPremisesIR' node is not available.
Type: Microsoft.DataTransfer.ClientProxy.NodeUnavailableException
```

**Layman explanation**: The bridge connecting your on-premises data to Azure is down. Like a phone line being cut.

**Common Causes**:
1. IR service stopped on the machine
2. Machine restarted and service didn't start
3. Network connectivity lost
4. Machine ran out of resources (CPU/Memory)
5. IR software needs updating

**Troubleshooting Steps**:

```
Step 1: Check IR Machine
   RDP to the machine (if you have access)
   Check Windows Services: 
      - "Microsoft Integration Runtime Service" 
      - Status should be "Running"

Step 2: Check Integration Runtime Configuration Manager
   Open the IR app on the machine
   Look at status: Connected or Disconnected
   Check "Diagnostics" tab for issues

Step 3: Check Resources
   Task Manager → Performance
   Is CPU at 100%? Is memory full? Is disk at capacity?

Step 4: Check Network
   Can machine reach *.servicebus.windows.net?
   Check proxy settings if used
   Test: Test-NetConnection -ComputerName <your-region>.frontend.clouddatahub.net -Port 443

Step 5: Check Windows Event Logs
   Event Viewer → Windows Logs → Application
   Look for Integration Runtime errors
```

**High Availability Tip**:
"I always recommend setting up Self-hosted IR in HA mode with 2-4 nodes. When one node goes down, others take over. This saved us during a Windows patch Tuesday when one IR machine rebooted unexpectedly - pipelines continued running on the second node."

**Interview scenario**:
Q: "A pipeline failed at 2 AM with IR unavailable error. How do you fix it?"
A: "First, I'd check if it's a single pipeline or all pipelines using that IR - that tells me if it's IR-wide or activity-specific. I'd RDP to the IR machine, check if the service is running, restart it if needed. I'd also check CPU and memory - I've seen IR become unavailable when the host machine ran out of resources during data-intensive operations. If it's urgent and can't be fixed quickly, I'd switch the linked service to a backup IR node if available, or temporarily use Azure IR with VNet integration if the data source allows it."

---

### 4. **Timeout Expired**

**Error Message**:
```
ErrorCode=SqlOperationTimeout
Message: Operation on target 'Copy_to_SQL' failed: Timeout expired. 
The timeout period elapsed prior to completion of the operation or 
the server is not responding.
```

**Layman explanation**: Your activity took too long, like a test that has a 1-hour time limit and you're still writing after 61 minutes.

**Common Causes**:
1. Large data volume with insufficient DIUs
2. SQL Server under heavy load
3. Network latency
4. Blocking/locking in SQL Server
5. Complex transformations in sink

**Solutions**:

**Increase Activity Timeout**:
```json
{
  "name": "Copy_LargeTable",
  "type": "Copy",
  "timeout": "2.00:00:00",  // 2 hours (format: D.HH:MM:SS)
  "policy": {
    "timeout": "2.00:00:00"
  }
}
```

**Optimize Copy Performance**:
```json
{
  "enableStaging": true,  // Use staging for large volumes
  "stagingSettings": {
    "linkedServiceName": "BlobStaging",
    "path": "staging"
  },
  "dataIntegrationUnits": 32,  // Increase DIUs
  "parallelCopies": 4  // Parallel threads
}
```

**Check SQL Server Performance**:
```sql
-- Find blocking queries
SELECT 
    blocking_session_id,
    wait_type,
    wait_duration_ms,
    command
FROM sys.dm_exec_requests
WHERE blocking_session_id <> 0;

-- Check if there are long-running queries
SELECT 
    start_time,
    status,
    command,
    DATEDIFF(SECOND, start_time, GETDATE()) as duration_seconds
FROM sys.dm_exec_requests
WHERE status = 'running'
ORDER BY start_time;
```

**Real incident**: "We had a Copy activity timing out every night at 2 AM. Turned out the SQL Server was running index rebuilds at the same time, causing locks. We moved the ADF pipeline to 3 AM, after maintenance window. Problem solved."

---

#### Error Category 2: Data Errors

### 5. **Schema Mismatch**

**Error Message**:
```
ErrorCode=UserErrorColumnNameMismatch
Message: Column 'OrderDate' does not exist in the target table. 
Please check your mapping.
```

**Why it happens**:
- Source added/removed columns
- Typo in column names
- Case sensitivity issues
- Source schema changed without notification

**Solutions**:

**Option 1: Enable Schema Drift** (Copy Activity)
```json
{
  "validateDataConsistency": false,
  "translator": {
    "type": "TabularTranslator",
    "typeConversion": true,
    "typeConversionSettings": {
      "allowDataTruncation": true,
      "treatBooleanAsNumber": false
    },
    "columnMappings": [
      // Leave empty for auto-mapping
    ]
  }
}
```

**Option 2: Use Data Flow with Schema Drift**
```
Source → Settings → Allow schema drift: ☑ Enabled
                  → Infer drifted column types: ☑ Enabled
```

**Option 3: Add Pre-Copy Validation**
```
1. GetMetadata activity to fetch source schema
2. Lookup activity to fetch target schema
3. Azure Function to compare schemas
4. If Condition: Only copy if schemas match
5. Send email if schemas differ
```

**Real-world approach**: "I built a metadata-driven framework where we stored expected schemas in a control table. Before each copy, a Lookup activity compared source schema (from GetMetadata) with expected schema. If columns were added/removed, pipeline sent an email alert to the data engineering team and the data provider, then failed gracefully with a clear message."

---

### 6. **Null Value Violation**

**Error Message**:
```
ErrorCode=SqlOperationFailed
Message: Cannot insert the value NULL into column 'CustomerID', 
table 'dbo.Orders'; column does not allow nulls. INSERT fails.
```

**Solutions**:

**Option 1: Filter Nulls in Source Query**
```json
{
  "source": {
    "type": "SqlSource",
    "sqlReaderQuery": "SELECT * FROM Orders WHERE CustomerID IS NOT NULL"
  }
}
```

**Option 2: Data Flow Transformation**
```
Source → Derived Column:
  CustomerID = iif(isNull(CustomerID), 'UNKNOWN', CustomerID)
```

**Option 3: Use Stored Procedure Sink**
```sql
CREATE PROCEDURE usp_InsertOrders
    @CustomerID VARCHAR(50),
    @OrderDate DATE
AS
BEGIN
    INSERT INTO Orders (CustomerID, OrderDate)
    VALUES (
        COALESCE(@CustomerID, 'UNKNOWN'),  -- Handle nulls
        @OrderDate
    )
END
```

---

### 7. **File Not Found**

**Error Message**:
```
ErrorCode=UserErrorFailedFileOperation
Message: The specified blob does not exist. 
Path: 'raw/sales/sales_2024_11_16.csv'
```

**Common Causes**:
1. Wrong file name/path
2. File not yet created by upstream system
3. File was moved/deleted
4. Timezone issues (file dated tomorrow in UTC)
5. Parameterization error

**Solutions**:

**Add GetMetadata Check**:
```json
{
  "name": "Check_File_Exists",
  "type": "GetMetadata",
  "typeProperties": {
    "dataset": "SalesFile",
    "fieldList": ["exists"]
  }
}

// Then If Condition:
@activity('Check_File_Exists').output.exists
```

**Use Wildcard Path**:
```json
{
  "type": "DelimitedText",
  "typeProperties": {
    "location": {
      "type": "AzureBlobStorageLocation",
      "fileName": "sales_*.csv",  // Wildcard
      "folderPath": "raw/sales"
    }
  }
}
```

**Add Retry Logic**:
```json
{
  "policy": {
    "timeout": "0.12:00:00",
    "retry": 3,
    "retryIntervalInSeconds": 300  // Wait 5 minutes between retries
  }
}
```

**Real scenario**: "We had a file expected at 6 AM but sometimes arrived at 6:05 AM. Pipeline would fail. I added a 'Wait' activity that checked every 5 minutes (up to 30 minutes) if the file existed using GetMetadata in a Until loop. Pipeline became much more resilient."

---

#### Error Category 3: Authentication & Authorization

### 8. **Forbidden / Access Denied**

**Error Message**:
```
ErrorCode=AuthorizationFailed
Message: The client '...' with object id '...' does not have 
authorization to perform action 'Microsoft.Storage/storageAccounts/blobServices/containers/blobs/read' 
over scope '/subscriptions/.../providers/Microsoft.Storage/storageAccounts/mystorage'.
```

**Layman explanation**: You're trying to enter a room, but you don't have the key card.

**Solutions**:

**Option 1: Add Managed Identity Permissions** (Recommended)
```
Step 1: Enable Managed Identity on ADF
   ADF → Manage → Managed Identity
   Note the Object ID

Step 2: Grant permissions on Storage Account
   Storage Account → Access Control (IAM) → Add role assignment
   Role: "Storage Blob Data Contributor"
   Assign access to: Managed Identity
   Select: Your ADF name

Step 3: Update Linked Service to use Managed Identity
   Linked Service → Authentication: Managed Identity
```

**Option 2: Use Key Vault** (for secrets)
```
Step 1: Store credential in Key Vault
   Key Vault → Secrets → Generate/Import
   Name: StorageAccountKey
   Value: [actual key]

Step 2: Give ADF access to Key Vault
   Key Vault → Access policies → Add access policy
   Secret permissions: Get, List
   Select principal: Your ADF Managed Identity

Step 3: Reference in Linked Service
   Account key: Select "Azure Key Vault"
   Key Vault Linked Service: [Your KV]
   Secret name: StorageAccountKey
```

**Interview answer**: "I always use Managed Identity for Azure resources - it's more secure than storing keys, automatically rotates credentials, and eliminates secret management overhead. For non-Azure resources, I store credentials in Key Vault and reference them using ADF's Key Vault integration. Never hardcode credentials in ADF."

---

### 9. **Token Expired**

**Error Message**:
```
ErrorCode=UserErrorOAuthTokenExpired
Message: OAuth token has expired. Please re-authenticate.
```

**Why it happens**:
- OAuth-based linked services (Salesforce, Dynamics, REST APIs)
- Token validity typically 1-7 days
- No refresh token configured

**Solution**:
```
Step 1: Re-authenticate Linked Service
   Linked Service → Edit → Re-enter credentials → Test connection

Step 2: Use Service Principal (if supported)
   Instead of user-based OAuth, use Service Principal
   These don't expire and are better for automation

Step 3: Set up monitoring
   Create an Azure Alert for AuthenticationFailed errors
   Get notified before production is affected
```

**Pro tip**: "For production pipelines, avoid OAuth tokens that require manual re-authentication. Use Service Principals, Managed Identities, or long-lived API keys with monitoring and rotation policies."

---

## Performance Troubleshooting {#performance-troubleshooting}

### The Performance Investigation Framework

**Scenario**: Your boss says, "Why is the nightly data load taking 4 hours? It used to take 30 minutes!"

Here's your systematic approach:

---

### Step 1: Measure and Baseline

**Get current metrics**:
```
Monitor → Pipeline runs → Select your pipeline → Activity runs
```

**Look at**:
1. **Total pipeline duration**: 4 hours
2. **Individual activity durations**:
   - Lookup: 2s ✓ Normal
   - Copy_Customers: 3h 45m ⚠️ Problem here!
   - Copy_Orders: 10m ✓ Normal
   - Data Flow: 5m ✓ Normal

3. **Activity Output** (Copy activity):
```json
{
  "rowsRead": 50000000,
  "rowsCopied": 50000000,
  "dataRead": 5368709120,  // 5 GB
  "throughput": 395,  // KB/s - THIS IS SLOW!
  "usedDataIntegrationUnits": 4,
  "copyDuration": 13500,  // seconds (3h 45m)
  "effectiveIntegrationRuntime": "AutoResolveIntegrationRuntime"
}
```

**Compare with baseline**:
```
Last month's same pipeline:
- rowsRead: 10000000 (5x less data!)
- throughput: 2048 KB/s (5x faster)
- copyDuration: 1800s (30 minutes)
```

**Aha moment**: Data volume increased 5x, but we're using the same configuration!

---

### Step 2: Identify Bottlenecks

#### Bottleneck Type 1: **Low Throughput (MB/s)**

**Symptoms**:
- throughput < 1 MB/s for cloud-to-cloud copies
- throughput < 0.5 MB/s for hybrid copies

**Causes & Solutions**:

**A. Insufficient DIUs (Data Integration Units)**

Default is 4 DIUs, but you can go up to 256 for large copies.

```json
{
  "name": "Copy_BigTable",
  "type": "Copy",
  "typeProperties": {
    "source": {...},
    "sink": {...},
    "dataIntegrationUnits": 32,  // Increase from 4
    "parallelCopies": 8  // Parallel degree
  }
}
```

**DIU Guidelines**:
- < 1 GB data: 4 DIUs (default)
- 1-10 GB: 8-16 DIUs
- 10-100 GB: 16-64 DIUs
- > 100 GB: 64-256 DIUs

**Real example**: "I increased DIUs from 4 to 32 for a 50 GB copy from Blob to SQL. Duration dropped from 2 hours to 15 minutes. Cost increased by $0.50 per run, but the faster processing was worth it."

**B. Source/Sink Throttling**

Check the Output for warnings:
```json
{
  "warnings": [
    {
      "message": "Source SQL database is being throttled. 
      Consider increasing DTUs or upgrading to higher tier."
    }
  ]
}
```

**Solutions**:
- **Azure SQL**: Upgrade tier or increase DTUs during load window
- **Blob Storage**: Check if you're hitting account limits (20,000 requests/sec)
- **Cosmos DB**: Increase RU/s during data load
- **On-premises DB**: Check CPU, memory, disk I/O on source server

**C. Network Latency**

If copying across regions or from on-premises:

```json
{
  "enableStaging": true,  // Use staging for cross-region
  "stagingSettings": {
    "linkedServiceName": "StagingBlobInSameRegion",
    "path": "staging/tempdata"
  }
}
```

**Staging benefits**:
- Compresses data during transfer
- Reduces network round trips
- Can improve throughput by 2-10x

**Interview gold**: "When copying large volumes from on-premises SQL Server to Azure, I always enable staging with PolyBase or COPY command. This reduced our nightly load from 6 hours to 45 minutes by using bulk insert instead of row-by-row inserts."

---

#### Bottleneck Type 2: **Sequential Processing**

**Problem**: Activities running one after another when they could run in parallel.

**Bad design**:
```
Copy_Table1 (15 min) → Copy_Table2 (15 min) → Copy_Table3 (15 min)
Total: 45 minutes
```

**Good design**:
```
                    ┌→ Copy_Table1 (15 min) ┐
Lookup_Metadata ────┼→ Copy_Table2 (15 min) ┼→ Success_Email
                    └→ Copy_Table3 (15 min) ┘
Total: 16 minutes (Lookup + longest copy)
```

**How to achieve parallel execution**:

**Method 1: Use ForEach with sequential=false**
```json
{
  "name": "ForEach_Copy_Tables",
  "type": "ForEach",
  "typeProperties": {
    "items": "@variables('TableList')",
    "isSequential": false,  // KEY: Run in parallel
    "batchCount": 10,  // Max 10 parallel iterations
    "activities": [
      {
        "name": "Copy_Table",
        "type": "Copy"
      }
    ]
  }
}
```

**Method 2: Multiple activities without dependencies**
```json
// Don't add "dependsOn" if activities can run independently
{
  "activities": [
    {"name": "Copy_Table1"},  // No dependencies
    {"name": "Copy_Table2"},  // No dependencies
    {"name": "Copy_Table3"}   // No dependencies
  ]
}
```

**Real scenario**: "We had 20 tables to copy, each taking 5 minutes. Original pipeline: 100 minutes sequential. After ForEach with sequential=false and batchCount=10: 10 minutes (2 batches of 10 parallel copies)."

---

#### Bottleneck Type 3: **Inefficient Queries**

**Bad SQL query in Copy source**:
```sql
-- Scans entire table, returns everything
SELECT * FROM Sales

-- No indexes, table scan
SELECT * FROM Orders WHERE CONVERT(DATE, OrderDate) = '2024-11-16'
```

**Good SQL query**:
```sql
-- Partition-aware, uses indexes
SELECT OrderID, CustomerID, OrderDate, TotalAmount
FROM Sales
WHERE OrderDate >= '2024-11-16' AND OrderDate < '2024-11-17'
  AND PartitionKey = '2024-11-16'
```

**Performance tips**:

1. **Always specify columns** (not SELECT *)
2. **Use indexed columns in WHERE clause**
3. **Avoid functions on indexed columns** (like CONVERT, CAST)
4. **Use partitioning** if available
5. **Add WHERE clause** for incremental loads

**Check query performance**:
```sql
-- Before using query in ADF, test in SSMS with execution plan
SET STATISTICS TIME ON;
SET STATISTICS IO ON;

-- Your query here
SELECT ...

-- Look at "Elapsed time" and "Logical reads"
```

**Interview scenario**:
Q: "A Copy activity is slow. How do you optimize the source query?"
A: "First, I'd look at the actual query being executed - check if it's doing SELECT * or has inefficient filters. I'd copy that query to SSMS and run it with execution plan enabled. Common issues I've seen: missing indexes, scanning entire tables when only recent data is needed, using functions on indexed columns. For example, WHERE YEAR(OrderDate) = 2024 prevents index usage - better to use WHERE OrderDate >= '2024-01-01' AND OrderDate < '2025-01-01'. I'd also check if the table is partitioned and ensure the query leverages that."

---

#### Bottleneck Type 4: **Sink Performance**

**Symptoms**: Fast read (rowsRead shows in output), slow write (copyDuration is high)

**Common causes**:

**A. Row-by-row inserts instead of bulk**

**Bad** (default for some sinks):
```json
{
  "sink": {
    "type": "SqlSink",
    "writeBatchSize": 1000  // Small batches
  }
}
```

**Good** (bulk insert):
```json
{
  "sink": {
    "type": "SqlSink",
    "writeBatchSize": 50000,  // Larger batches
    "writeBatchTimeout": "00:30:00",
    "sqlWriterStoredProcedureName": "spBulkInsert",
    "tableOption": "autoCreate"  // Creates table if missing
  }
}
```

**Even better** (PolyBase/COPY):
```json
{
  "enableStaging": true,
  "stagingSettings": {
    "linkedServiceName": "AzureBlobStorage",
    "path": "staging"
  },
  "sink": {
    "type": "SqlDWSink",
    "allowPolyBase": true,
    "polyBaseSettings": {
      "rejectType": "percentage",
      "rejectValue": 10
    }
  }
}
```

**Performance comparison (10 million rows)**:
- Row-by-row: 2-3 hours
- Batch insert (50K): 30-45 minutes
- PolyBase/COPY: 5-10 minutes

**B. Indexes and constraints during load**

**Problem**: Indexes slow down inserts

**Solution pattern**:
```
Pipeline flow:
1. Stored Procedure: Drop indexes on target table
2. Copy Activity: Bulk load data
3. Stored Procedure: Rebuild indexes
```

```sql
-- Pre-copy stored procedure
CREATE PROCEDURE usp_DropIndexes_Sales
AS
BEGIN
    DROP INDEX IX_Sales_CustomerID ON Sales;
    DROP INDEX IX_Sales_OrderDate ON Sales;
END

-- Post-copy stored procedure
CREATE PROCEDURE usp_RebuildIndexes_Sales
AS
BEGIN
    CREATE INDEX IX_Sales_CustomerID ON Sales(CustomerID);
    CREATE INDEX IX_Sales_OrderDate ON Sales(OrderDate);
END
```

**Real example**: "For a nightly 50 million row load to SQL, I added pre/post stored procedure activities to drop and rebuild indexes. Load time went from 4 hours to 1 hour. The 15 minutes to rebuild indexes was negligible compared to the 3-hour savings."

---

### Step 3: Integration Runtime Performance

**Check which IR is being used**:

```
Activity output → effectiveIntegrationRuntime
```

**Common issues**:

#### Issue 1: AutoResolveIntegrationRuntime choosing wrong region

**Problem**: Your data is in East US, but AutoResolve chose West Europe IR

**Solution**: Create region-specific Azure IR
```
Manage → Integration Runtimes → New → Azure IR
Name: EastUS-IR
Region: East US (not Auto Resolve)

Use this IR in Linked Services for East US resources
```

**Benefit**: Reduces network latency, improves throughput

#### Issue 2: Self-hosted IR overloaded

**Symptoms**:
- High CPU/Memory on IR machine
- Activities queuing
- Slow throughput

**Check IR metrics**:
```
Monitor → Integration Runtimes → Select your Self-hosted IR
```

**Metrics to watch**:
- CPU utilization %
- Available memory
- Concurrent jobs running
- Queue length

**Solutions**:
1. **Scale up**: Add more CPU/RAM to IR machine
2. **Scale out**: Add more IR nodes (HA cluster)
3. **Optimize**: Reduce parallel copies per node
4. **Schedule**: Stagger pipeline start times

**Real scenario**: "We had 4 pipelines all starting at midnight, each with 10 parallel copies. Self-hosted IR CPU hit 100%, everything became slow. I staggered the schedules: 12:00 AM, 12:30 AM, 1:00 AM, 1:30 AM. Much smoother execution."

---

### Step 4: Data Flow Performance

**If using Mapping Data Flows**, performance tuning is different:

#### Check Data Flow Metrics
```
Activity output for Data Flow:
{
  "computeType": "General",
  "coreCount": 8,
  "runDuration": 1800,  // 30 minutes
  "sinkProcessingTime": 1200,  // 20 minutes writing
  "transformationStats": {
    "filter": 120,  // 2 min in filter
    "aggregate": 480  // 8 min in aggregate
  }
}
```

#### Performance Tips:

**A. Right-size compute**
```json
{
  "compute": {
    "computeType": "MemoryOptimized",  // For aggregations
    "coreCount": 32  // Increase for large data
  }
}
```

**Compute types**:
- **General**: Default, balanced
- **MemoryOptimized**: Aggregations, joins, sorts
- **ComputeOptimized**: Complex transformations

**B. Optimize partitioning**
```
Each transformation → Optimize tab
Partitioning: 
- Round robin (default) - even distribution
- Hash - for joins and aggregates
- Key range - for sorted data
- Single - use sparingly (bottleneck)
```

**C. Use projection**
```
Source → Projection tab
Select only needed columns
Define schema to avoid schema drift overhead
```

**D. Enable staging**
```
Sink → Settings → Enable staging: ✓
```

**Real example**: "Data Flow was taking 45 minutes to aggregate 100M rows. Changed compute from General (8 cores) to MemoryOptimized (32 cores), and changed partitioning from Round Robin to Hash on the aggregate key. Time dropped to 8 minutes."

---

### The Performance Troubleshooting Checklist

When facing slow pipelines, go through this checklist:

```
☐ 1. Measure baseline - What was the duration before?
☐ 2. Identify slowest activity - Use Gantt chart
☐ 3. Check activity output - Look at throughput
☐ 4. Data volume - Did it increase?
☐ 5. DIUs - Are you using enough?
☐ 6. Parallelization - Can activities run in parallel?
☐ 7. Source query - Is it efficient?
☐ 8. Sink settings - Using bulk insert?
☐ 9. Staging - Should you enable it?
☐ 10. Integration Runtime - Right region? Enough resources?
☐ 11. Network - Any latency issues?
☐ 12. Source/Sink resources - Being throttled?
☐ 13. Indexes - Should you drop during load?
☐ 14. Data Flow - Right compute and partitioning?
```

**Interview answer template**:
"When troubleshooting pipeline performance, I start by identifying which activity is slow using the Gantt view. Then I check the activity output for throughput metrics. Common optimizations I apply: increasing DIUs for large copies, enabling staging for cross-region transfers, using PolyBase for SQL data warehouse loads, parallelizing independent activities with ForEach sequential=false, and optimizing source queries to use indexes. I also check Integration Runtime metrics to ensure it's not a resource constraint. Finally, I compare current metrics with historical baselines to understand if it's data growth or configuration degradation."

---

## Azure Monitor and Log Analytics Integration {#azure-monitor}

### Why Azure Monitor?

**Think of it like this**: ADF's built-in monitoring is like looking at your car's dashboard. Azure Monitor is like having a detailed mechanic's diagnostic report with historical trends.

**What you get**:
1. **Longer retention**: ADF UI keeps 45 days, Log Analytics keeps years
2. **Custom queries**: KQL (Kusto Query Language) for complex analysis
3. **Cross-resource correlation**: Combine ADF logs with SQL, Storage, etc.
4. **Advanced alerting**: Complex conditions, action groups
5. **Dashboards**: Custom visualizations for management

---

### Setting Up Diagnostic Settings

**Step-by-step setup**:

```
Step 1: Enable Diagnostic Settings
ADF → Monitoring → Diagnostic settings → Add diagnostic setting

Step 2: Configure what to log
Name: ADF-to-LogAnalytics
Logs to enable:
  ☑ ActivityRuns
  ☑ PipelineRuns
  ☑ TriggerRuns
  ☑ SandboxPipelineRuns
  ☑ SandboxActivityRuns
  ☑ SSISPackageEventMessages
  ☑ SSISPackageExecutableStatistics
  ☑ SSISPackageEventMessageContext
  ☑ SSISPackageExecutionComponentPhases
  ☑ SSISPackageExecutionDataStatistics
  ☑ SSISIntegrationRuntimeLogs

Metrics:
  ☑ AllMetrics

Step 3: Choose destination
Destination: Send to Log Analytics workspace
Subscription: [Your subscription]
Log Analytics workspace: [Your workspace]

Step 4: Save
```

**Cost consideration**: "Diagnostic logs have costs. For cost-sensitive projects, I only enable ActivityRuns and PipelineRuns, which capture 90% of troubleshooting needs. Full SSIS logging is expensive and only needed if you're running SSIS packages."

---

### Useful KQL Queries

#### Query 1: Failed Pipeline Runs (Last 7 Days)

```kusto
ADFPipelineRun
| where TimeGenerated > ago(7d)
| where Status == "Failed"
| project 
    TimeGenerated,
    PipelineName,
    Status,
    FailureType,
    ErrorCode,
    ErrorMessage,
    ResourceId
| order by TimeGenerated desc
```

**When to use**: Morning review of overnight failures

---

#### Query 2: Pipeline Duration Trends

```kusto
ADFPipelineRun
| where TimeGenerated > ago(30d)
| where PipelineName == "Daily_Sales_Load"
| where Status == "Succeeded"
| extend DurationMinutes = (End - Start) / 1m
| summarize 
    AvgDuration = avg(DurationMinutes),
    MaxDuration = max(DurationMinutes),
    MinDuration = min(DurationMinutes),
    Count = count()
    by bin(TimeGenerated, 1d)
| render timechart
```

**When to use**: Identifying performance degradation over time

**Interview story**: "I used this query to prove to management that our nightly load was getting progressively slower over 3 months - from 45 minutes to 3 hours. The chart showed a clear upward trend. This justified getting budget for performance optimization."

---

#### Query 3: Most Expensive Pipelines (by DIU consumption)

```kusto
ADFActivityRun
| where TimeGenerated > ago(7d)
| where ActivityType == "Copy"
| extend DIUHours = todouble(Output.billingReference.billableDuration[0].duration)
| summarize TotalDIUHours = sum(DIUHours) by PipelineName
| order by TotalDIUHours desc
| take 10
```

**When to use**: Monthly cost review, identifying optimization opportunities

**Real use**: "CFO asked why our Azure bill jumped 40%. This query showed one pipeline was consuming 80% of our DIU hours because someone set it to 256 DIUs for copying 1000 rows. Easy fix, instant savings."

---

#### Query 4: Activity Failure Patterns

```kusto
ADFActivityRun
| where TimeGenerated > ago(30d)
| where Status == "Failed"
| summarize FailureCount = count() by ActivityName, ErrorCode
| order by FailureCount desc
```

**When to use**: Finding systemic issues

**Example insight**: "This query revealed 500 failures in 'Lookup_Customer' activity all with ErrorCode 'SqlTimeout'. Pointed us to a SQL Server performance issue."

---

#### Query 5: Peak Concurrency Analysis

```kusto
ADFPipelineRun
| where TimeGenerated > ago(7d)
| extend Hour = bin(TimeGenerated, 1h)
| summarize ConcurrentPipelines = dcount(RunId) by Hour
| order by ConcurrentPipelines desc
| take 20
```

**When to use**: Capacity planning, understanding when your ADF is busiest

---

#### Query 6: Self-Hosted IR Health Check

```kusto
ADFIntegrationRuntimeNodeHeartbeat
| where TimeGenerated > ago(1d)
| summarize LastHeartbeat = max(TimeGenerated) by NodeName
| extend MinutesSinceLastHeartbeat = datetime_diff('minute', now(), LastHeartbeat)
| where MinutesSinceLastHeartbeat > 5  // Alert if no heartbeat in 5+ minutes
```

**When to use**: Monitoring Self-hosted IR availability

---

### Creating a Monitoring Dashboard

**Use case**: Executive dashboard showing ADF health

**Steps**:
```
Step 1: Create queries in Log Analytics

Step 2: Pin to Dashboard
Run query → Click "Pin to dashboard" → Choose dashboard

Step 3: Create tiles for:
- Pipeline success rate (last 24 hours)
- Failed pipelines count (last 7 days)
- Average pipeline duration trend
- Top 5 longest running pipelines
- DIU consumption by pipeline
- Integration Runtime availability
```

**Sample dashboard layout**:
```
┌─────────────────────────────────────────────────┐
│  ADF Production Health Dashboard               │
├──────────────┬──────────────┬──────────────────┤
│ Success Rate │ Failed Runs  │ Avg Duration     │
│   98.5%      │     12       │   45 min         │
├──────────────┴──────────────┴──────────────────┤
│  Pipeline Duration Trend (30 days)             │
│  [Line chart showing duration over time]       │
├─────────────────────────────────────────────────┤
│  Top Failed Pipelines                           │
│  1. Daily_Sales_Load: 8 failures               │
│  2. Hourly_Inventory: 3 failures               │
│  3. Weekly_Report: 1 failure                   │
└─────────────────────────────────────────────────┘
```

**Interview gold**: "I created an executive dashboard in Azure showing real-time ADF health. This gave visibility to management and reduced 'is the pipeline running?' questions from 10 per day to zero."

---

### Correlating Logs Across Services

**Powerful capability**: Correlate ADF failures with other Azure resources

**Example**: ADF Copy failed - was it SQL Server's fault?

```kusto
// ADF failures
let ADFFailures = ADFActivityRun
| where TimeGenerated > ago(1d)
| where ActivityName == "Copy_to_SQL"
| where Status == "Failed"
| project ADFTime = TimeGenerated, PipelineName, ErrorCode;

// SQL Server DTU usage at same time
let SQLMetrics = AzureMetrics
| where TimeGenerated > ago(1d)
| where ResourceProvider == "MICROSOFT.SQL"
| where MetricName == "dtu_consumption_percent"
| project SQLTime = TimeGenerated, DTUPercent = Average;

// Join them
ADFFailures
| join kind=leftouter (SQLMetrics) on $left.ADFTime == $right.SQLTime
| where DTUPercent > 95  // SQL was maxed out!
| project ADFTime, PipelineName, ErrorCode, DTUPercent
```

**Insight**: "All 5 ADF failures happened when SQL DTU was >95%. SQL Server was the bottleneck, not ADF."

---

## Setting Up Custom Alerts {#custom-alerts}

### Alert Strategy

**Think like a doctor**: You want to know about problems before the patient (business user) notices.

**Alert tiers**:
1. **Critical (P1)**: Page on-call engineer, 24/7
2. **High (P2)**: Email + SMS during business hours
3. **Medium (P3)**: Email only, reviewed daily
4. **Low (P4)**: Dashboard only, reviewed weekly

---

### Alert Type 1: Pipeline Failure

**Scenario**: Any production pipeline fails

**Setup**:
```
Step 1: Navigate to Alerts
ADF → Monitoring → Alerts → New alert rule

Step 2: Define condition
Signal: Pipeline runs (Metric)
Condition: 
  - Aggregation: Count
  - Operator: Greater than
  - Threshold: 0
  - Dimension filters:
    - Status: Failed
    - PipelineName: * (all) or specific pipeline

Step 3: Define action group
Action group: ADF-Production-Failures
Actions:
  - Email: dataengineering@company.com
  - SMS: +1-555-0100 (on-call engineer)
  - Azure Function: LogToIncidentManagement

Step 4: Alert details
Name: PROD - Pipeline Failed
Severity: Critical (Sev 0)
Description: Production ADF pipeline has failed
Enable: Yes

Step 5: Create
```

**Real configuration**:
```json
{
  "condition": {
    "allOf": [
      {
        "metricName": "PipelineFailedRuns",
        "operator": "GreaterThan",
        "threshold": 0,
        "timeAggregation": "Count",
        "dimensions": [
          {
            "name": "PipelineName",
            "operator": "Include",
            "values": ["*"]  // All pipelines
          },
          {
            "name": "FailureType",
            "operator": "Include",
            "values": ["UserError", "SystemError"]
          }
        ]
      }
    ]
  },
  "windowSize": "PT5M",  // 5-minute window
  "evaluationFrequency": "PT1M"  // Check every minute
}
```

---

### Alert Type 2: Long Running Pipeline

**Scenario**: Pipeline taking longer than usual

**Why it matters**: Catch performance issues before SLAs are breached

**Setup with KQL**:
```
Step 1: Create Log Analytics alert
Azure Monitor → Alerts → New alert rule

Step 2: Scope
Resource: Your Log Analytics workspace

Step 3: Condition
Signal: Custom log search
Query:
ADFPipelineRun
| where TimeGenerated > ago(5m)
| where PipelineName == "Daily_Sales_Load"
| where Status == "InProgress"
| extend DurationMinutes = (now() - Start) / 1m
| where DurationMinutes > 60  // Alert if running >1 hour
| project PipelineName, DurationMinutes, Start

Alert logic:
  - Based on: Number of results
  - Operator: Greater than
  - Threshold: 0

Step 4: Actions
Action group: ADF-Performance-Alerts
Actions:
  - Email: Team lead
  - Webhook: Slack channel #data-engineering

Step 5: Details
Name: Pipeline Running Too Long
Severity: Warning (Sev 2)
```

**Interview tip**: "I set up alerts for pipelines running 50% longer than their average duration. This caught a slowly degrading SQL Server before it caused complete failures. Proactive monitoring is key."

---

### Alert Type 3: No Pipelines Running (Expected Schedule)

**Scenario**: Schedule trigger didn't fire

**Why it matters**: Catch trigger configuration issues

**KQL Query**:
```kusto
let ExpectedRunTime = datetime(2024-11-16 02:00);
ADFPipelineRun
| where TimeGenerated between (ExpectedRunTime .. (ExpectedRunTime + 10m))
| where PipelineName == "Daily_Sales_Load"
| where Status in ("Succeeded", "Failed", "InProgress")
| summarize RunCount = count()
| where RunCount == 0  // No runs found!
```

**Setup**:
- Schedule alert to check at 02:10 AM daily
- Alert if query returns results (meaning no pipeline ran)

---

### Alert Type 4: High Failure Rate

**Scenario**: More than 10% of pipelines failing

**Why it matters**: Indicates systemic issue, not isolated failure

**Metric alert**:
```json
{
  "condition": {
    "allOf": [
      {
        "metricName": "PipelineSucceededRuns",
        "metricNamespace": "Microsoft.DataFactory/factories",
        "operator": "LessThan",
        "threshold": 90,  // Success rate < 90%
        "timeAggregation": "Average",
        "windowSize": "PT1H"  // Over 1 hour
      }
    ]
  }
}
```

---

### Alert Type 5: Integration Runtime Unavailable

**Scenario**: Self-hosted IR goes down

**Critical alert** because all pipelines using that IR will fail

**Setup**:
```
Signal: Integration runtime available node count
Condition:
  - Metric: Available node count
  - Operator: Less than
  - Threshold: 1
  - Duration: 5 minutes

Action: Page on-call engineer immediately
```

---

### Alert Type 6: Cost Spike

**Scenario**: ADF costs suddenly increase

**KQL Query**:
```kusto
ADFActivityRun
| where TimeGenerated > ago(1d)
| where ActivityType == "Copy"
| extend DIUHours = todouble(Output.billingReference.billableDuration[0].duration)
| summarize DailyDIUHours = sum(DIUHours)
| extend Baseline = 1000  // Your normal daily DIU hours
| where DailyDIUHours > (Baseline * 1.5)  // 50% increase
```

**Why it matters**: Catch configuration mistakes (someone set 256 DIUs) before the monthly bill arrives

---

### Best Practices for Alerting

#### 1. **Avoid Alert Fatigue**

**Bad**: Alert on every single failure
**Good**: Alert on patterns (3 failures in 1 hour)

```kusto
// Alert if same pipeline fails 3+ times in 1 hour
ADFPipelineRun
| where TimeGenerated > ago(1h)
| where Status == "Failed"
| summarize FailureCount = count() by PipelineName
| where FailureCount >= 3
```

#### 2. **Alert Suppression**

**Scenario**: Planned maintenance window

```
Alert rule → Suppression
Add suppression rule:
  - Every Saturday 2 AM - 6 AM
  - Reason: Weekly maintenance
```

#### 3. **Runbooks in Alert Descriptions**

**Good alert description**:
```
Pipeline: Daily_Sales_Load has failed.

Runbook:
1. Check Monitor → Pipeline runs for error details
2. If ErrorCode 2200 (SQL issue): Check SQL Server availability
3. If ErrorCode 2201 (Blob issue): Check storage account keys
4. For timeout errors: Check Self-hosted IR status
5. Escalate to Team Lead if unresolved in 30 minutes

Oncall Rotation: https://wiki.company.com/oncall
Known Issues: https://wiki.company.com/adf-known-issues
```

**Why this is powerful**: Junior engineer gets paged at 3 AM, has clear steps to follow

#### 4. **Test Your Alerts**

**Before going to production**:
```
Step 1: Create a test pipeline that will fail
Step 2: Set up alert
Step 3: Trigger the test pipeline
Step 4: Verify you receive alert within expected time
Step 5: Verify alert content is clear and actionable
```

**Interview answer**: "I always test alerts in non-prod first. I create scenarios that trigger the alert and verify the right people get notified with the right information. Nothing worse than finding out your production alert doesn't work when there's an actual incident."

#### 5. **Alert Grouping**

**Problem**: 20 pipelines fail due to SQL Server down → 20 separate alerts

**Solution**: Use action group smart grouping
```
Action group → Smart grouping: Enabled
Group by: Resource (SQL Server)
Result: Single notification: "20 pipelines affected by SQL Server outage"
```

---

## Real-World Troubleshooting Scenarios {#real-world-scenarios}

These are actual scenarios that you'll face (and can mention in interviews as "your experience").

---

### Scenario 1: The Midnight Mystery

**Situation**:
"Every night at midnight, our main ETL pipeline fails with 'Connection timeout' to on-premises SQL Server. But when we manually trigger it at 8 AM, it works fine."

**Your investigation**:

```
Step 1: Check pattern
Monitor → Pipeline runs → Filter last 30 days
Result: Failed at 00:00 every night, succeeded every manual test

Step 2: Check what else happens at midnight
Log Analytics query:
ADFPipelineRun
| where TimeGenerated > ago(30d)
| where format_datetime(TimeGenerated, 'HH:mm') == '00:00'
| summarize count() by PipelineName
Result: 15 pipelines all start at midnight!

Step 3: Check Self-hosted IR metrics
Monitor → Integration Runtimes → OnPremises-IR
Metrics at midnight: CPU 98%, Queue length: 47
Metrics at 8 AM: CPU 12%, Queue length: 0

Step 4: Check SQL Server
Contact DBA: "Is there anything scheduled at midnight?"
DBA: "Yes, index rebuilds run from 11:45 PM to 2 AM"
```

**Root cause**: 
- 15 pipelines competing for Self-hosted IR resources
- SQL Server under load from index rebuilds
- Perfect storm of resource contention

**Solution**:
```
1. Stagger pipeline schedules:
   - Group 1: 00:00 (5 critical pipelines)
   - Group 2: 00:30 (5 medium priority)
   - Group 3: 01:00 (5 low priority)

2. Coordinate with DBA:
   - Move index rebuilds to 2 AM

3. Scale out Self-hosted IR:
   - Add second node for high availability
   - Distribute load across nodes

4. Increase timeouts for midnight runs:
   "timeout": "01:00:00"  // 1 hour instead of default 30 min
```

**Result**: 100% success rate restored

**Interview gold**: "This taught me the importance of looking at the bigger picture. It wasn't just one pipeline's problem - it was a resource contention issue affecting the entire environment. I now always check what else is running concurrently when troubleshooting."

---

### Scenario 2: The Invisible Data Loss

**Situation**:
"Finance reports numbers don't match source system. Pipeline shows 'Succeeded'. No errors in logs."

**The horror**: Data is silently missing!

**Your investigation**:

```
Step 1: Check pipeline run
Status: Succeeded ✓
Duration: Normal ✓
Error: None ✓

Step 2: Check Copy activity output
{
  "rowsRead": 1000000,
  "rowsCopied": 750000,  // ⚠️ NOT THE SAME!
  "errors": []
}

Step 3: Why no error if 250,000 rows failed?
Check Copy activity configuration:
{
  "enableSkipIncompatibleRow": true,  // ⚠️ HERE'S THE PROBLEM
  "logSettings": {
    "enableCopyActivityLog": false  // NOT LOGGING SKIPPED ROWS!
  }
}
```

**Root cause**: 
- Copy activity set to skip incompatible rows
- No logging enabled for skipped rows
- Pipeline "succeeds" even though 25% of data is missing

**Solution**:

**Immediate fix**:
```json
{
  "enableSkipIncompatibleRow": false,  // Fail on incompatible rows
  "validateDataConsistency": true  // Validate row counts match
}
```

**Better long-term solution**:
```json
{
  "enableSkipIncompatibleRow": true,
  "redirectIncompatibleRowSettings": {
    "linkedServiceName": "ErrorLoggingBlob",
    "path": "errors/sales/{RunId}"
  },
  "logSettings": {
    "enableCopyActivityLog": true,
    "copyActivityLogSettings": {
      "logLevel": "Warning",  // Log incompatible rows
      "enableReliableLogging": true
    },
    "logLocationSettings": {
      "linkedServiceName": "LoggingBlob",
      "path": "logs/copyactivity"
    }
  }
}

// Add validation activity after Copy
{
  "name": "Validate_RowCount",
  "type": "Lookup",
  "typeProperties": {
    "source": {
      "type": "SqlSource",
      "sqlReaderQuery": "
        DECLARE @SourceCount INT = @{activity('Copy_Sales').output.rowsRead}
        DECLARE @SinkCount INT = @{activity('Copy_Sales').output.rowsCopied}
        
        IF @SourceCount != @SinkCount
          THROW 51000, 'Row count mismatch!', 1
        
        SELECT @SourceCount as SourceRows, @SinkCount as SinkRows
      "
    }
  }
}
```

**Monitoring addition**:
```kusto
// Alert on row count mismatches
ADFActivityRun
| where ActivityType == "Copy"
| extend RowsRead = toint(Output.rowsRead)
| extend RowsCopied = toint(Output.rowsCopied)
| where RowsRead != RowsCopied
| project TimeGenerated, PipelineName, ActivityName, RowsRead, RowsCopied
```

**Interview lesson**: "This was a critical learning moment. 'Succeeded' doesn't always mean 'correct'. I now always add validation activities after critical Copy operations and monitor the rowsRead vs rowsCopied metrics. For financial data, I never enable skip incompatible rows - better to fail and investigate than silently lose data."

---

### Scenario 3: The Performance Cliff

**Situation**:
"Pipeline that used to take 30 minutes now takes 6 hours. No code changes. Data volume increased slightly (20%)."

**Your investigation**:

```
Step 1: Compare activity outputs (now vs 30 days ago)

30 days ago:
{
  "rowsRead": 10000000,
  "throughput": 2048,  // KB/s
  "usedDataIntegrationUnits": 4,
  "copyDuration": 1800  // 30 minutes
}

Today:
{
  "rowsRead": 12000000,  // 20% more data
  "throughput": 95,  // KB/s ⚠️ HUGE DROP!
  "usedDataIntegrationUnits": 4,
  "copyDuration": 21600  // 6 hours
}

Step 2: Why is throughput so low?
Check source query:
SELECT * FROM Sales 
WHERE OrderDate >= '2024-01-01'

Check SQL execution plan:
⚠️ Table scan! No index on OrderDate

Step 3: When did index disappear?
Check with DBA: 
"We rebuilt the database from backup 2 weeks ago. 
Some custom indexes might not have been restored."
```

**Root cause**: Missing index on source table caused full table scans

**Solution**:

```sql
-- Recreate missing index
CREATE NONCLUSTERED INDEX IX_Sales_OrderDate 
ON Sales(OrderDate) 
INCLUDE (CustomerID, OrderAmount);

-- Verify with execution plan
SET STATISTICS IO ON;
SELECT * FROM Sales WHERE OrderDate >= '2024-01-01';
-- Now shows "Index Seek" instead of "Table Scan"
```

**Additional improvements**:
```json
{
  "source": {
    "type": "SqlSource",
    "sqlReaderQuery": "
      SELECT 
        OrderID, CustomerID, OrderDate, OrderAmount
      FROM Sales WITH (NOLOCK)  -- Avoid blocking
      WHERE OrderDate >= '2024-01-01'
        AND OrderDate < '2025-01-01'  -- Explicit range
    ",
    "queryTimeout": "02:00:00"
  },
  "dataIntegrationUnits": 16  // Increased from 4
}
```

**Result**: Duration back to 28 minutes

**Proactive monitoring added**:
```kusto
// Alert if throughput drops below threshold
ADFActivityRun
| where ActivityType == "Copy"
| where ActivityName == "Copy_Sales"
| extend Throughput = toint(Output.throughput)
| where Throughput < 1000  // Alert if < 1 MB/s
```

**Interview story**: "This scenario taught me to always have a baseline for performance metrics. When troubleshooting performance, I compare current metrics with historical averages. In this case, the throughput drop from 2 MB/s to 95 KB/s immediately pointed to a source problem, not an ADF configuration issue. I now document expected throughput ranges for all critical pipelines."

---

### Scenario 4: The Cascade Failure

**Situation**:
"One small lookup table load failed. Now 47 downstream pipelines are failing. Exec team is panicking."

**The cascade**:
```
Lookup_RefTable (Failed)
  ↓
30 pipelines depend on RefTable
  ↓
17 pipelines depend on those 30
  ↓
Total: 47 failed pipelines
```

**Your investigation**:

```
Step 1: Identify root cause
Monitor → Find first failure: "Lookup_RefTable" at 02:00 AM
Error: "Table 'RefTable' does not exist"

Step 2: Why doesn't it exist?
Check upstream process:
- RefTable loaded by external system
- That system had downtime last night
- Table was truncated but not reloaded

Step 3: Assess impact
Log Analytics:
ADFPipelineRun
| where TimeGenerated > ago(1d)
| where Status == "Failed"
| extend ErrorDetails = tostring(Output)
| where ErrorDetails contains "RefTable"
| summarize FailedPipelines = dcount(PipelineName)
Result: 47 pipelines affected
```

**Immediate action**:

```
Step 1: Stop the bleeding
Disable all triggers temporarily to prevent more failures

Step 2: Fix root cause
Contact external system team → Get RefTable reloaded

Step 3: Validate fix
Run Lookup_RefTable pipeline manually → Success

Step 4: Replay failed pipelines
Use this PowerShell script:

$failedPipelines = Get-AzDataFactoryV2PipelineRun `
  -ResourceGroupName "rg-prod" `
  -DataFactoryName "adf-prod" `
  -LastUpdatedAfter "2024-11-16" `
  -LastUpdatedBefore "2024-11-16T12:00:00" `
  | Where-Object {$_.Status -eq "Failed"}

foreach ($run in $failedPipelines) {
  Invoke-AzDataFactoryV2Pipeline `
    -ResourceGroupName "rg-prod" `
    -DataFactoryName "adf-prod" `
    -PipelineName $run.PipelineName
}

Step 5: Re-enable triggers
Monitor for 1 hour to ensure stability
```

**Prevention measures implemented**:

**1. Circuit breaker pattern**:
```json
{
  "name": "Pipeline_with_CircuitBreaker",
  "activities": [
    {
      "name": "Check_RefTable_Health",
      "type": "Lookup",
      "typeProperties": {
        "source": {
          "type": "SqlSource",
          "sqlReaderQuery": "
            IF NOT EXISTS (SELECT 1 FROM RefTable)
              THROW 51000, 'RefTable is empty!', 1
            
            IF (SELECT MAX(LastUpdated) FROM RefTable) < DATEADD(hour, -2, GETDATE())
              THROW 51001, 'RefTable is stale!', 1
            
            SELECT 'OK' as Status
          "
        }
      }
    },
    {
      "name": "Proceed_Only_If_Healthy",
      "type": "IfCondition",
      "dependsOn": [{"activity": "Check_RefTable_Health"}],
      "typeProperties": {
        "expression": {
          "value": "@equals(activity('Check_RefTable_Health').output.firstRow.Status, 'OK')",
          "type": "Expression"
        },
        "ifTrueActivities": [
          // Normal pipeline activities
        ],
        "ifFalseActivities": [
          {
            "name": "Send_Alert_RefTable_Unhealthy",
            "type": "WebActivity"
          }
        ]
      }
    }
  ]
}
```

**2. Dependency mapping documentation**:
```
Created confluence page:
"Pipeline Dependencies Map"

RefTable
├── Daily_Sales_Load
│   ├── Sales_Summary_Report
│   └── Sales_Analytics_Cube
├── Hourly_Inventory_Load
│   ├── Inventory_Dashboard
│   └── Reorder_Alerts
...

Critical Path Pipelines (must succeed):
1. Lookup_RefTable
2. Load_Customer_Master
3. Load_Product_Master
```

**3. Improved alerting**:
```kusto
// Alert if critical pipeline fails
ADFPipelineRun
| where PipelineName in ("Lookup_RefTable", "Load_Customer_Master")
| where Status == "Failed"
| project TimeGenerated, PipelineName, ErrorCode
```

**Interview answer**: "This incident taught me about blast radius in data pipelines. One small failure can cascade to dozens of pipelines. I now implement health checks before critical operations and maintain clear dependency documentation. For critical reference data loads, I added validation activities that check data freshness and completeness before allowing downstream pipelines to proceed. This 'circuit breaker' pattern prevents cascade failures."

---

### Scenario 5: The Mysterious Success

**Situation**:
"Pipeline shows 'Succeeded', but users report no data in target table. You check SQL Server: table is empty."

**Your investigation**:

```
Step 1: Check pipeline
Status: Succeeded ✓
All activities: Succeeded ✓

Step 2: Check Copy activity output
{
  "rowsRead": 0,  // ⚠️ Zero rows!
  "rowsCopied": 0,
  "dataRead": 0,
  "copyDuration": 2,
  "throughput": 0
}

Step 3: Check source query
{
  "source": {
    "type": "SqlSource",
    "sqlReaderQuery": "
      SELECT * FROM Orders 
      WHERE OrderDate > '@{variables('LastLoadDate')}'
    "
  }
}

Step 4: Check variable value
Variable 'LastLoadDate': 2024-11-16

Problem: The query became:
SELECT * FROM Orders WHERE OrderDate > '2024-11-16'

But OrderDate only has dates through 2024-11-15!
```

**Root cause**: Query returned 0 rows (legitimately), Copy activity "succeeded" with 0 rows

**Why this is dangerous**: 
- No error was raised
- Business assumes data was loaded
- Reports show outdated data
- Nobody notices until user complains

**Solution**:

**Add zero-row validation**:
```json
{
  "name": "Copy_Orders",
  "type": "Copy",
  "typeProperties": {
    // ... normal config
  }
},
{
  "name": "Validate_Not_Zero",
  "type": "IfCondition",
  "dependsOn": [{"activity": "Copy_Orders"}],
  "typeProperties": {
    "expression": {
      "value": "@greater(activity('Copy_Orders').output.rowsCopied, 0)",
      "type": "Expression"
    },
    "ifFalseActivities": [
      {
        "name": "Fail_Pipeline_No_Data",
        "type": "Wait",
        "typeProperties": {
          "waitTimeInSeconds": 1
        }
      },
      {
        "name": "Send_Alert_Zero_Rows",
        "type": "WebActivity",
        "typeProperties": {
          "url": "https://logic-app-alert.azurewebsites.net/api/Alert",
          "method": "POST",
          "body": {
            "message": "Copy activity succeeded but copied 0 rows!",
            "pipeline": "@{pipeline().Pipeline}",
            "activity": "Copy_Orders"
          }
        }
      }
    ]
  }
}
```

**Even better - add expected range validation**:
```json
{
  "name": "Validate_Row_Count_Range",
  "type": "IfCondition",
  "typeProperties": {
    "expression": {
      "value": "@and(greater(activity('Copy_Orders').output.rowsCopied, 1000), less(activity('Copy_Orders').output.rowsCopied, 1000000))",
      "type": "Expression"
    },
    "ifFalseActivities": [
      {
        "name": "Alert_Unusual_Row_Count",
        "type": "WebActivity"
      }
    ]
  }
}
```

**Monitoring query**:
```kusto
// Alert on successful copies with 0 rows
ADFActivityRun
| where ActivityType == "Copy"
| where Status == "Succeeded"
| extend RowsCopied = toint(Output.rowsCopied)
| where RowsCopied == 0
| where PipelineName !contains "Test"  // Ignore test pipelines
| project TimeGenerated, PipelineName, ActivityName, RowsCopied
```

**Interview gold**: "This taught me that 'Succeeded' doesn't mean 'Correct'. For critical data loads, I always add validation activities that check not just for errors, but for expected data patterns. Zero rows, unusually high/low counts, or missing expected files should all raise alerts even if technically the activity succeeded."

---

### Scenario 6: The Ghost Connection

**Situation**:
"Pipeline fails with 'Cannot connect to SQL Server', but you can connect fine from SSMS using the same credentials."

**Your investigation**:

```
Step 1: Test connection in ADF
Linked Service → Test connection → ❌ Failed
Error: "A network-related or instance-specific error..."

Step 2: Test from SSMS on your machine
Server: sql-prod.database.windows.net
Login: Success ✓

Step 3: Test from Self-hosted IR machine
RDP to IR machine
Open SSMS
Try to connect: ❌ Failed! Same error

Step 4: Check firewall
Azure Portal → SQL Server → Networking → Firewall rules
Your office IP: ✓ Added
Self-hosted IR IP: ❌ NOT ADDED!

Step 5: Find Self-hosted IR public IP
On IR machine:
Invoke-RestMethod -Uri 'https://api.ipify.org?format=json'
Result: 203.0.113.45

Step 6: Add to SQL firewall
SQL Server → Networking → Firewall rules
Add rule: "ADF-Self-Hosted-IR", 203.0.113.45
```

**Additional discoveries**:

```
Check 1: NSG (Network Security Group) rules
Sometimes firewall is open but NSG blocks

Check 2: SQL Server "Allow Azure services"
SQL Server → Networking
☐ Allow Azure services and resources to access this server
^ This needs to be checked for Azure IR

Check 3: Private Endpoint
If SQL uses private endpoint, Self-hosted IR needs VNet access
```

**Prevention - Document IR IP addresses**:
```markdown
# Self-Hosted Integration Runtimes

## OnPremises-IR-Node1
- Machine: VM-ADF-IR-01
- Public IP: 203.0.113.45
- Private IP: 10.0.1.10
- Firewall rules needed in: SQL-Prod, SQL-UAT, Azure Storage

## OnPremises-IR-Node2
- Machine: VM-ADF-IR-02
- Public IP: 203.0.113.46
- Private IP: 10.0.1.11
- Firewall rules needed in: SQL-Prod, SQL-UAT, Azure Storage
```

**Interview answer**: "Connection issues between ADF and data sources are common. My troubleshooting approach: First, verify credentials by testing from SQL Management Studio. Second, check network connectivity - can the Integration Runtime machine actually reach the target? For Self-hosted IR, I RDP to the machine and test connectivity directly. Third, verify firewall rules at all layers: Azure SQL firewall, NSGs, corporate firewalls, and on-premises firewalls. Document all IR IP addresses and required firewall rules for quick reference during incidents."

---

### Scenario 7: The Quota Exceeded

**Situation**:
"Multiple pipelines failing with 'Resource quota exceeded'. ADF portal seems slow."

**Error message**:
```
ErrorCode=QuotaExceeded
Message: The resource quota has been exceeded. 
Retry after some time or reduce the parallel degree.
```

**Your investigation**:

```
Step 1: Check what's running
Monitor → Pipeline runs → Status: InProgress
Count: 127 pipelines running simultaneously!

Step 2: Check ADF limits
Azure limits per Data Factory:
- Max concurrent pipeline runs per IR: 
  - Azure IR: 1000 (not the issue)
  - Self-hosted IR: 50 per node (potential issue!)
  
- Max concurrent activity runs per pipeline: 50
- Max activities per pipeline: 200

Step 3: Check Self-hosted IR
Monitor → Integration Runtimes → OnPremises-IR
Concurrent jobs: 50/50 (maxed out!)
Queue length: 77

Step 4: Why so many jobs?
Check recently changed pipelines:
- Someone added ForEach with 100 iterations
- Each iteration uses Self-hosted IR
- Running sequentially=false
- 100 activities all trying to run at once!
```

**Solutions**:

**Immediate fix**:
```json
{
  "name": "ForEach_Load_Tables",
  "type": "ForEach",
  "typeProperties": {
    "items": "@variables('TableList')",
    "isSequential": false,
    "batchCount": 10  // ← Limit to 10 concurrent instead of unlimited
  }
}
```

**Long-term fixes**:

**1. Scale out Self-hosted IR**:
```
Add second node to IR:
- Download and install IR on second machine
- Use same authentication key
- Now 100 concurrent jobs possible (50 per node)
```

**2. Implement throttling pattern**:
```json
{
  "name": "Controlled_Parallel_Processing",
  "activities": [
    {
      "name": "Get_Tables_Batch1",
      "type": "Lookup"
    },
    {
      "name": "ForEach_Batch1",
      "type": "ForEach",
      "dependsOn": [{"activity": "Get_Tables_Batch1"}],
      "typeProperties": {
        "isSequential": false,
        "batchCount": 5
      }
    },
    {
      "name": "Wait_Between_Batches",
      "type": "Wait",
      "dependsOn": [{"activity": "ForEach_Batch1"}],
      "typeProperties": {
        "waitTimeInSeconds": 60
      }
    },
    {
      "name": "ForEach_Batch2",
      "type": "ForEach",
      "dependsOn": [{"activity": "Wait_Between_Batches"}],
      "typeProperties": {
        "isSequential": false,
        "batchCount": 5
      }
    }
  ]
}
```

**3. Resource monitoring alert**:
```kusto
// Alert when Self-hosted IR is near capacity
AzureMetrics
| where ResourceProvider == "MICROSOFT.DATAFACTORY"
| where MetricName == "IntegrationRuntimeAvailableNodeCount"
| where Average < 1
| project TimeGenerated, Resource, Average
```

**Interview lesson**: "This taught me about ADF resource limits and the importance of throttling. Just because you can run 100 things in parallel doesn't mean you should. I now always set batch counts in ForEach activities and stagger pipeline schedules to avoid resource contention. For high-volume processing, I scale out Self-hosted IR with multiple nodes rather than trying to push everything through a single node."

---

## Interview Questions {#interview-questions}

### Basic Level Questions

**Q1: What's the difference between Pipeline runs and Activity runs?**

**Your Answer**: "A Pipeline run is a single execution of a pipeline from start to finish, identified by a unique runId. It contains one or more Activity runs. For example, if my pipeline has a Lookup, Copy, and Stored Procedure activity, one pipeline run will have three activity runs. When troubleshooting, I first identify the failed pipeline run, then drill down to see which specific activity failed. The pipeline run shows overall status and duration, while activity runs show detailed inputs, outputs, and error messages for each step."

---

**Q2: Where do you find the error message when a pipeline fails?**

**Your Answer**: "I go to Monitor → Pipeline runs → Click on the failed pipeline name → This shows Activity runs → Click on the failed activity → Go to the 'Error' tab. This tab contains the errorCode, message, failureType, and detailed information. The error message usually tells me exactly what went wrong. For example, ErrorCode 2200 typically means a data type or length mismatch, while connection errors have codes in the 2300 range. I also check the 'Output' tab for additional context like which row failed or what data caused the issue."

---

**Q3: How do you check if a pipeline ran today?**

**Your Answer**: "In ADF Studio, I go to Monitor → Pipeline runs, then use the filters: set the date range to 'Today' and search for the pipeline name. I can see all runs with their status, start time, and duration. If I need to verify it ran at a specific time, I look at the 'Triggered by' column to see if it was scheduled or manual. For programmatic checking, I can also query Log Analytics if diagnostic settings are enabled, using KQL to filter by pipeline name and date."

---

### Intermediate Level Questions

**Q4: A Copy activity shows 'Succeeded' but your users report missing data. How do you investigate?**

**Your Answer**: "I'd check the Copy activity output, specifically comparing rowsRead versus rowsCopied. If these numbers don't match, data was lost during the copy. I'd also check if 'enableSkipIncompatibleRow' is set to true - this lets the activity succeed even when skipping rows. To prevent this, I add validation activities after critical copies that check row counts and fail the pipeline if they don't match expectations. I also enable copy activity logging to capture skipped rows for investigation. This happened to me once where rowsRead was 1 million but rowsCopied was only 750K - turned out column length mismatches were being silently skipped."

---

**Q5: How do you find which pipelines are consuming the most cost?**

**Your Answer**: "I use Log Analytics with a KQL query that extracts DIU hours from activity outputs and summarizes by pipeline name. The query looks at ADFActivityRun table, extracts billing information from the Output column, and sums DIU hours. I typically look at the last 30 days to identify trends. I can also use the Consumption report in ADF Monitor section. In one project, this analysis showed that one pipeline was using 80% of our budget because someone had set 256 DIUs for copying a tiny table - easy fix with huge cost savings."

---

**Q6: A pipeline that used to take 30 minutes now takes 3 hours. How do you troubleshoot?**

**Your Answer**: "I start by identifying which activity is slow using the Gantt chart view in the pipeline run details. Then I compare current activity output with historical baselines - specifically looking at throughput (MB/s), rows processed, and DIUs used. Common causes I check: data volume increase without adjusting DIUs, source or sink performance degradation, missing indexes on source tables, network issues if using Self-hosted IR, or resource contention if many pipelines run simultaneously. I once found a performance cliff was caused by a missing index that got dropped during a database restore - throughput dropped from 2MB/s to 95KB/s. Recreated the index and performance returned to normal."

---

### Advanced Level Questions

**Q7: You're getting paged at 3 AM about pipeline failures. Walk me through your troubleshooting process.**

**Your Answer**: "First, I assess the scope - is it one pipeline or multiple? Multiple failures often indicate a systemic issue like Integration Runtime down or a shared resource like SQL Server unavailable. I check Monitor → Pipeline runs filtered by 'Failed' and 'Last 2 hours'. I look for patterns - same error code across pipelines points to a common cause. For a single pipeline, I drill into the failed activity's error tab to get the errorCode and message. Common quick fixes: SQL connection issues - check firewall and SQL availability; file not found - check if upstream system ran; timeout errors - check if source system is slow. I also check if it's a one-time glitch or consistent failure by looking at historical runs. I keep a runbook wiki with common error codes and solutions for faster resolution. If I can't resolve in 30 minutes, I escalate but provide all diagnostic info I've gathered."

---

**Q8: How do you set up monitoring to catch issues before users report them?**

**Your Answer**: "I implement multiple layers of monitoring. First, diagnostic settings sending pipeline and activity runs to Log Analytics for long-term retention and analysis. Second, Azure Monitor alerts for critical scenarios: pipeline failures (P1 alert), pipelines running longer than baseline plus 50% (P2 alert), and Self-hosted IR unavailability (P1 alert). Third, I add validation activities in pipelines themselves - checking row counts, data freshness, and expected value ranges, failing the pipeline if validations don't pass. Fourth, a morning health check dashboard showing overnight pipeline success rates, any failed runs, and performance trends. Finally, weekly KQL queries to identify degrading patterns like increasing durations or rising failure rates. This proactive approach has caught issues hours before users would notice - like degrading SQL performance or files arriving later than usual."

---

**Q9: How do you handle a cascade failure where one upstream failure caused 50 downstream pipelines to fail?**

**Your Answer**: "First, stop the bleeding by temporarily disabling triggers to prevent more failures while fixing the root cause. Second, identify the root cause by finding the earliest failure - usually the upstream dependency. Fix that first. Third, implement a circuit breaker pattern going forward - add health check activities at the start of pipelines that validate dependencies before proceeding. For example, before loading data that depends on a reference table, check if that table exists and is recent. Fourth, document pipeline dependencies so I know the blast radius of any failure. Fifth, replay failed pipelines in dependency order - fix upstream first, then let downstream pipelines run. I use PowerShell or REST API to trigger reruns. This happened to me when a reference table load failed and 47 pipelines dependent on it also failed. After fixing the root cause, I replayed them in batches while monitoring for any new issues. Finally, I added monitoring alerts for critical upstream pipelines so failures are caught immediately."

---

**Q10: Explain how you would investigate a situation where pipelines are slow only between midnight and 2 AM.**

**Your Answer**: "Time-specific performance issues usually indicate resource contention. I'd check several things: First, how many pipelines are running concurrently at midnight versus other times - use a KQL query to count concurrent runs by hour. Second, check Integration Runtime metrics at midnight - is CPU maxed? Memory full? High queue length? Third, check if there's scheduled maintenance on source/sink systems at that time - SQL index rebuilds, backup jobs, or batch processes. Fourth, check if network bandwidth is constrained during those hours - maybe backup systems are using the network. Fifth, look at the Gantt chart for midnight runs - are activities queuing? Running sequentially when they should be parallel? In one case, I found 15 pipelines all starting exactly at midnight competing for a Self-hosted IR with only 50 concurrent job capacity. Solution was staggering schedules every 30 minutes. Another time, SQL Server was running index rebuilds midnight-2 AM causing locks and timeouts. Coordinated with DBA to move maintenance to 2-4 AM after our loads completed."

---

### Scenario-Based Questions

**Q11: A business user says 'I got yesterday's data again today'. How do you investigate?**

**Your Answer**: "This suggests an incremental load isn't working correctly. First, I check if the pipeline ran today by looking at pipeline runs filtered by the pipeline name and today's date. If it ran, I check the Copy activity output to see how many rows were copied - if it's copying the same count as yesterday, the incremental logic isn't working. I'd review the source query to see what watermark or date filter is being used. Common issues: the watermark isn't being updated after each run, or it's reading from a variable/parameter that isn't being set correctly. I'd check the Lookup activity that retrieves the last watermark value - verify it's reading from the correct table/column. Then check if there's a Stored Procedure activity at the end that updates the watermark - maybe it's failing silently or not running. I'd also check the actual data in the watermark table to see if dates are being written. Quick fix might be to manually update the watermark to yesterday and rerun. Long-term fix is adding validation activities to ensure watermark gets updated and alerting if it doesn't change between runs."

---

**Q12: Your manager asks: 'Why did our Azure bill go up 40% this month?' How do you investigate?**

**Your Answer**: "I'd start with the Consumption report in ADF Monitor section to see overall usage trends. Then I'd use Log Analytics to query which pipelines consumed the most DIU hours in the current month versus last month. The query extracts billing information from activity outputs and compares month-over-month. I'd look for: pipelines that started using more DIUs than necessary, new pipelines that were added, pipelines running more frequently, or data volumes that increased significantly. I'd also check if someone enabled data flows with large compute clusters. In one case, I found a developer had set all Copy activities to 256 DIUs during testing and forgot to change it back - even tiny tables were using maximum resources. Another time, a pipeline that used to run once daily was changed to run every 5 minutes. I'd present findings to management with specific recommendations: right-size DIUs based on data volume, consolidate frequent small runs into fewer large batches, use tumbling windows instead of frequent schedules, and enable monitoring alerts for unusual consumption spikes."

---

**Q13: A pipeline intermittently fails - succeeds 70% of the time, fails 30%. How do you troubleshoot?**

**Your Answer**: "Intermittent failures are the hardest to debug. I'd gather data first - look at the last 30 days of runs and note the failure pattern. Is it random? Same time of day? Same day of week? Specific to certain data? I'd export all failed runs to Excel: date, time, duration before failure, error message. Common causes I've seen: source system is under load sometimes (check if failures correlate with peak hours), network timeouts during large data transfers (check if failures happen with larger file sizes), race conditions in parallel processing (if using ForEach with sequential=false), or source data occasionally having bad records. I'd check activity outputs for failed vs successful runs - is the error always the same or different each time? If network-related, I'd add retry policies with longer intervals. If data-related, I'd add data validation before copy and enable logging of skipped rows. If timing-related, I'd add a Wait activity or delay the schedule slightly. One time I had a pipeline that failed every Monday because a weekly batch job on the source system was still running when our pipeline started. Added a dependency check that waited for the batch job to complete."

---

**Q14: You need to migrate 500 pipelines from DEV to PROD. How do you ensure monitoring and alerting work correctly in PROD?**

**Your Answer**: "This requires a comprehensive deployment checklist. First, ensure diagnostic settings are configured in PROD ADF to send logs to Log Analytics - this should be done via ARM template or Terraform, not manually. Second, create environment-specific alert rules - PROD alerts go to on-call rotation, DEV alerts go to team email. Third, update all notification activities in pipelines to use environment-specific parameters - PROD sends to ops teams, DEV sends to dev team. Fourth, test critical path pipelines manually after deployment before enabling triggers. Fifth, enable triggers in phases - start with low-risk pipelines, monitor for 24 hours, then enable high-risk pipelines. Sixth, have a rollback plan - keep the previous ADF version in a backup branch. Seventh, document all Integration Runtime configurations and ensure PROD has sufficient capacity. I'd also create a PROD monitoring dashboard and have someone watch it for the first 48 hours after deployment. Finally, schedule the deployment for a low-activity time like Saturday morning with the team on standby. I learned this after a deployment where we didn't test alerts in PROD and discovered during an incident that notifications were still going to DEV email addresses."

---

### Expert-Level Questions

**Q15: Design a complete monitoring and alerting strategy for a mission-critical ADF environment supporting a Fortune 500 company.**

**Your Answer**: 

"I'd design a multi-layered monitoring strategy:

**Layer 1: Pipeline Self-Monitoring**
Every pipeline includes validation activities that check data quality, row counts, data freshness, and business rules. Pipelines fail fast with clear error messages rather than succeeding with bad data. Critical pipelines have health checks that validate dependencies before proceeding.

**Layer 2: ADF Platform Monitoring**
- Diagnostic settings send all logs to Log Analytics (45-day retention minimum)
- Azure Monitor metrics track pipeline runs, activity runs, trigger runs per minute
- Integration Runtime health monitoring with 5-minute heartbeat checks
- Resource quota and throttling monitoring

**Layer 3: Alert Configuration**
P1 (Page immediately):
- Any production pipeline failure in critical path
- Self-hosted IR unavailability
- SQL Server connection failures
- Data freshness violations (data older than SLA)

P2 (Email + SMS business hours):
- Pipeline duration exceeding baseline by 50%
- Copy activity with row count mismatches
- Multiple retry attempts
- Unusual cost spikes (>20% daily increase)

P3 (Email only):
- Pipeline warnings
- Performance degradation trends
- Approaching quota limits

**Layer 4: Proactive Monitoring**
- Daily morning report: overnight failures, performance trends, cost summary
- Weekly review: success rate by pipeline, top 10 slowest pipelines, month-to-date costs
- Monthly capacity planning: IR utilization trends, peak concurrency analysis
- Quarterly performance baselines review and SLA assessment

**Layer 5: Business Monitoring**
- Data quality dashboards for business users
- SLA compliance tracking (e.g., 'Sales data available by 8 AM')
- Business metric anomaly detection (e.g., 'Sales volume dropped 50%')

**Layer 6: Integration and Automation**
- Integrate alerts with incident management system (ServiceNow)
- Auto-create tickets for P2/P3 alerts
- Runbook automation for common issues
- Slack/Teams integration for team notifications

**Documentation Requirements**:
- Runbooks for all alert types
- Escalation matrix with on-call rotations
- Known issues and workarounds wiki
- Pipeline dependency maps
- Recovery time objectives (RTO) and recovery point objectives (RPO) for each pipeline

**Testing**:
- Quarterly disaster recovery drills
- Monthly alert validation (trigger test alerts)
- Regular review of alert fatigue (alerts that get ignored)

This strategy has prevented 95% of issues from reaching end users in my experience."

---

**Q16: Walk me through how you'd troubleshoot a complex performance issue where a Data Flow that processes 100 million rows is taking 4 hours when it should take 30 minutes.**

**Your Answer**:

"I'd approach this systematically:

**Phase 1: Establish Baseline**
- What was the duration when it was performing well?
- Has data volume changed?
- Has data distribution/skew changed?
- Any recent changes to the Data Flow logic?

**Phase 2: Analyze Current Performance**
From the Data Flow activity output, I'd examine:
```
- computeType: General / MemoryOptimized / ComputeOptimized
- coreCount: How many cores?
- sinkProcessingTime: Is most time in writing?
- transformationStats: Which transformation is slowest?
```

**Phase 3: Debug Mode Analysis**
Enable Data Flow debug and run with sampling:
- Check data preview at each transformation
- Look for data skew (one partition has 90% of data)
- Check partition counts between transformations
- Examine row count reduction through pipeline

**Phase 4: Common Performance Issues**

**Issue 1: Data Skew**
Symptom: One partition has 90M rows, others have 1M
Solution: Change partitioning strategy
```
Before Sort transformation:
Optimize → Partitioning → Hash
Hash on key with good distribution (not customer ID if few customers)
```

**Issue 2: Insufficient Compute**
Symptom: sinkProcessingTime is 80% of total time
Solution: Increase compute
```
Current: General, 8 cores
Change to: MemoryOptimized, 32 cores
Cost increase: ~$5 per run
Time saved: 3.5 hours
ROI: Easily justified
```

**Issue 3: Complex Aggregations**
Symptom: Aggregate transformation taking 90% of time
Solution: 
- Pre-aggregate in source query if possible
- Use appropriate partitioning (Hash on GROUP BY keys)
- Consider moving heavy aggregations to SQL stored procedure

**Issue 4: Many Columns (Wide Tables)**
Symptom: Moving unnecessary columns through entire pipeline
Solution: Add Select transformation early
```
Source → Select (choose only needed columns) → Rest of pipeline
Reduced data movement from 50 columns to 10 columns
Throughput increased 5x
```

**Issue 5: Sink Performance**
Symptom: Data processing fast but writing slow
Solution:
- Enable staging for Sink
- Increase sink write batch size
- For SQL targets: use bulk insert, consider PolyBase
- Partition sink writes if target supports it

**Phase 5: Optimization Applied**
Based on findings, I'd:
1. Right-size compute cluster
2. Optimize partitioning strategy
3. Add Select early to reduce columns
4. Enable sink staging
5. Fix any data skew issues

**Phase 6: Validate and Monitor**
- Run optimized version and compare metrics
- Document baseline performance expectations
- Set up alert if duration exceeds 45 minutes
- Schedule monthly review of performance trends

**Real Example**: In one case, a Data Flow was slow because someone had copied 100 columns but only needed 5. Added a Select transformation after Source, duration dropped from 2 hours to 12 minutes. Another time, changing compute from 8 cores to 32 cores and optimizing partitioning reduced a 4-hour Data Flow to 25 minutes."

---

## Pro Tips and Best Practices Summary

### Daily Monitoring Routine

**Morning checklist** (15 minutes):
```
☐ Check Monitor → Filter: Failed, Last 24 hours
☐ Review any failed pipelines
☐ Check Integration Runtime health
☐ Review Log Analytics alert dashboard
☐ Quick glance at long-running pipelines
```

### Documentation Standards

**For every production pipeline, document**:
- Purpose and business owner
- Source systems and dependencies
- Expected row counts (min/max)
- Expected duration (baseline)
- Alert configuration
- Runbook for common failures

### Error Handling Patterns

**Always implement**:
1. Retry policies on transient errors
2. Validation activities for critical operations
3. Notification activities for failures
4. Logging of skipped/failed rows
5. Row count validation (read vs. written)

### Performance Optimization Checklist

**Before marking pipeline "production-ready"**:
```
☐ DIUs appropriate for data volume
☐ Parallel execution where possible
☐ Staging enabled for large cross-region copies
☐ Source queries use indexes
☐ Sink configured for bulk operations
☐ Integration Runtime in correct region
☐ Data Flow compute sized appropriately
☐ Partitioning strategy validated
```

### Cost Optimization Tips

1. **Don't over-provision**: Start with 4 DIUs, increase if needed
2. **Consolidate runs**: Better to copy 10 files once than 1 file 10 times
3. **Use appropriate compute**: General compute for most workloads
4. **Schedule wisely**: Run during off-peak if possible
5. **Monitor consumption**: Set up cost spike alerts
6. **Review monthly**: Identify optimization opportunities

### Alerting Philosophy

**Good alerts are**:
- Actionable (clear what to do)
- Specific (tells you the problem)
- Timely (notifies before users complain)
- Accurate (low false positive rate)
- Prioritized (P1/P2/P3 severity)

**Bad alerts**:
- Too noisy (ignored)
- Vague ("Something failed")
- Too late (users already complaining)
- False positives (cry wolf syndrome)

---

## Troubleshooting Quick Reference

### Error Code Quick Guide

| Error Code | Meaning | Common Cause | Quick Fix |
|------------|---------|--------------|-----------|
| 2200 | Data type mismatch | Column too small | Increase column size or truncate data |
| 2201 | Null constraint violation | NULL in NOT NULL column | Filter NULLs or provide defaults |
| 2300 | Connection failed | Firewall or credentials | Check firewall, test credentials |
| 2301 | Timeout | Query too slow or resource busy | Optimize query, increase timeout |
| 2352 | File not found | Wrong path or file missing | Verify path, check upstream system |
| 2353 | No permissions | Authorization issue | Grant access, check managed identity |
| 2400 | IR unavailable | IR service stopped | Check IR machine, restart service |
| 2401 | Query failed | SQL syntax error | Review query, test in SSMS |

### Performance Troubleshooting Decision Tree

```
Is throughput low (<1 MB/s)?
├─ YES → Increase DIUs, enable staging
└─ NO → Is specific activity slow?
    ├─ YES → Which activity?
    │   ├─ Copy → Check source query, sink settings
    │   ├─ Data Flow → Check compute, partitioning
    │   └─ Lookup → Optimize SQL query
    └─ NO → Are activities sequential?
        ├─ YES → Enable parallel execution
        └─ NO → Check IR resources, network latency
```

### Monitoring Tool Decision Matrix

| Need | Tool | Why |
|------|------|-----|
| Real-time monitoring | ADF Monitor UI | Quick access, recent runs |
| Historical analysis | Log Analytics | Long retention, complex queries |
| Custom dashboards | Azure Dashboard + KQL | Executive visibility |
| Alerting | Azure Monitor Alerts | Automated notifications |
| Cost analysis | Log Analytics + Consumption Report | Detailed billing analysis |
| Performance trends | Log Analytics | Historical performance data |
| Dependency tracking | Documentation + Manual mapping | No built-in tool |

---

## Final Interview Tips

### What Makes a Great Answer

**Structure your answers like this**:
1. **Acknowledge the question**: "That's a common issue I've dealt with..."
2. **Describe your approach**: "My troubleshooting process starts with..."
3. **Provide specific example**: "For instance, one time I had a pipeline that..."
4. **State the outcome**: "After applying the fix, duration dropped from X to Y..."
5. **Mention lessons learned**: "This taught me to always..."

### Phrases That Show Experience

- "In my experience..."
- "One pattern I've noticed is..."
- "This happened to me once when..."
- "I've found that..."
- "A common mistake I see is..."
- "The root cause was usually..."
- "I typically start by checking..."

### Red Flags to Avoid

❌ "I've never had that issue"
✓ "That's an interesting scenario. Based on the error pattern, I would..."

❌ "I'd just restart the pipeline"
✓ "I'd first investigate the root cause by checking the activity output and error logs..."

❌ "I don't know"
✓ "I haven't encountered that specific scenario, but my approach would be similar to how I troubleshoot X..."

❌ Blaming tools: "ADF is buggy"
✓ Taking ownership: "I'd check my configuration..."

### Topics to Mention (Shows Depth)

- DIU optimization and cost management
- Integration Runtime strategy (when to use which type)
- Managed Identity for security
- Key Vault integration
- CI/CD with Git
- Monitoring and alerting strategy
- Performance baseline tracking
- Disaster recovery and business continuity
- Data validation and quality checks
- Row count verification
- Error handling patterns
- Retry and timeout strategies

---

## Conclusion

Monitoring, debugging, and troubleshooting are **critical skills** that separate junior ADF developers from senior engineers. The difference isn't just knowing how to build pipelines - it's knowing how to:

- **Anticipate** problems before they occur
- **Detect** issues quickly when they happen  
- **Diagnose** root causes systematically
- **Resolve** problems efficiently
- **Prevent** recurrence through better design

**Remember**: In interviews, they're not just testing if you know ADF - they're testing if you can **keep systems running in production**. The scenarios in this guide give you real-world experience to draw from.

When you can confidently say, "When a pipeline failed at 3 AM, here's exactly what I did..." - that's when you sound like someone with 5 years of experience.

**You're now ready to:**
- ✅ Monitor production ADF environments
- ✅ Troubleshoot any ADF issue systematically  
- ✅ Set up comprehensive alerting
- ✅ Optimize pipeline performance
- ✅ Answer advanced interview questions
- ✅ Sound like an experienced ADF engineer

**Next Steps**:
1. Practice navigating the ADF Monitor UI
2. Set up a Log Analytics workspace and practice KQL queries
3. Create alerts for common scenarios
4. Document your own troubleshooting experiences
5. Build a runbook for your team

**Good luck with your interviews!** 🚀

---

*This guide is part of the comprehensive ADF documentation series. Refer to other files for pipeline development, activities reference, and real-world implementation scenarios.*