SK = Surrogate Key (auto-incrementing)
```

**Implementation Approach 1: Using Data Flow (Recommended)**

**Step-by-step Data Flow:**

```
Step 1: Source - New Records
  Source Dataset: Daily customer feed
  
Step 2: Source - Existing Dimension
  Source Dataset: DimCustomer table
  Filter: IsCurrent = 1 (only current records)

Step 3: Lookup Transformation
  Primary Stream: New Records
  Lookup Stream: Existing Dimension
  Lookup Conditions: NewRecords.CustomerID == ExistingDim.CustomerID
  
  Result: Joined dataset with both old and new values

Step 4: Derived Column - Detect Changes
  Expression:
    HasChanged = or(
      NewCity != OldCity,
      NewState != OldState,
      NewName != OldName
    )
    
    IsNew = isNull(OldCustomerID)  // Customer doesn't exist yet
    
    IsUnchanged = and(
      not(isNull(OldCustomerID)),
      not(HasChanged)
    )

Step 5: Conditional Split
  - Stream 1 (NewCustomers): IsNew == true
  - Stream 2 (ChangedCustomers): HasChanged == true
  - Stream 3 (UnchangedCustomers): IsUnchanged == true (no action needed)

Step 6a: Process New Customers
  Derived Column:
    IsCurrent = 1
    EffectiveFrom = currentDate()
    EffectiveTo = toDate('9999-12-31')
  
  Surrogate Key:
    Generate SK (auto-increment from max existing + 1)
  
  Sink: Insert into DimCustomer

Step 6b: Process Changed Customers - Part 1 (Close Old Record)
  Select: Take only old dimension columns
  
  Alter Row:
    Update policy: true()  // Mark all for update
  
  Derived Column:
    IsCurrent = 0
    EffectiveTo = addDays(currentDate(), -1)  // Yesterday
  
  Sink: Update DimCustomer WHERE SK = OldSK

Step 6c: Process Changed Customers - Part 2 (Insert New Record)
  Select: Take new values
  
  Derived Column:
    IsCurrent = 1
    EffectiveFrom = currentDate()
    EffectiveTo = toDate('9999-12-31')
  
  Surrogate Key:
    Generate new SK
  
  Alter Row:
    Insert policy: true()
  
  Sink: Insert into DimCustomer
```

**Complete Data Flow Visual:**

```
┌──────────────┐         ┌──────────────┐
│ Source:      │         │ Source:      │
│ NewCustomers │         │ DimCustomer  │
│              │         │(IsCurrent=1) │
└──────┬───────┘         └──────┬───────┘
       │                        │
       └────────┬───────────────┘
                │
         ┌──────▼───────┐
         │   Lookup     │
         │(Join on ID)  │
         └──────┬───────┘
                │
         ┌──────▼───────┐
         │DerivedColumn │
         │DetectChanges │
         └──────┬───────┘
                │
         ┌──────▼───────────┐
         │ConditionalSplit  │
         └──┬──────┬───────┬┘
            │      │       │
     ┌──────▼┐ ┌──▼───┐ ┌▼────────┐
     │  New  │ │Changed│ │Unchanged│
     │       │ │       │ │(Skip)   │
     └───┬───┘ └──┬────┘ └─────────┘
         │        │
         │        ├──────┐
         │        │      │
    ┌────▼────┐ ┌▼────┐ │
    │AddSK &  │ │Close│ │
    │Dates    │ │Old  │ │
    └────┬────┘ └┬────┘ │
         │       │      │
    ┌────▼───┐ ┌▼────┐ │
    │ Sink:  │ │Sink:│ │
    │ INSERT │ │UPDATE│ │
    └────────┘ └─────┘ │
                       │
                  ┌────▼────┐
                  │AddSK &  │
                  │New Dates│
                  └────┬────┘
                       │
                  ┌────▼───┐
                  │ Sink:  │
                  │ INSERT │
                  └────────┘
```

**Implementation Approach 2: Using Stored Procedure (Alternative)**

```sql
CREATE PROCEDURE usp_SCD_Type2_Customer
AS
BEGIN
    SET NOCOUNT ON;
    
    -- Step 1: Close expired records (UPDATE existing)
    UPDATE t
    SET 
        t.IsCurrent = 0,
        t.EffectiveTo = DATEADD(day, -1, GETDATE()),
        t.UpdatedDate = GETDATE()
    FROM dbo.DimCustomer t
    INNER JOIN staging.CustomerSource s
        ON t.CustomerID = s.CustomerID
    WHERE t.IsCurrent = 1
        AND (
            t.Name != s.Name OR
            t.City != s.City OR
            t.State != s.State
        );
    
    -- Step 2: Insert new versions (for changed records)
    INSERT INTO dbo.DimCustomer 
        (CustomerID, Name, City, State, IsCurrent, EffectiveFrom, EffectiveTo)
    SELECT 
        s.CustomerID,
        s.Name,
        s.City,
        s.State,
        1 AS IsCurrent,
        CAST(GETDATE() AS DATE) AS EffectiveFrom,
        '9999-12-31' AS EffectiveTo
    FROM staging.CustomerSource s
    INNER JOIN dbo.DimCustomer t
        ON s.CustomerID = t.CustomerID
    WHERE t.IsCurrent = 0  -- Just closed
        AND t.EffectiveTo = DATEADD(day, -1, GETDATE());  -- Today
    
    -- Step 3: Insert completely new customers
    INSERT INTO dbo.DimCustomer 
        (CustomerID, Name, City, State, IsCurrent, EffectiveFrom, EffectiveTo)
    SELECT 
        s.CustomerID,
        s.Name,
        s.City,
        s.State,
        1 AS IsCurrent,
        CAST(GETDATE() AS DATE) AS EffectiveFrom,
        '9999-12-31' AS EffectiveTo
    FROM staging.CustomerSource s
    WHERE NOT EXISTS (
        SELECT 1 
        FROM dbo.DimCustomer t 
        WHERE t.CustomerID = s.CustomerID
    );
END;
```

**ADF Pipeline Orchestration:**

```
Pipeline: SCD_Type2_Customer_Daily

Activity 1: Copy Activity
  Source: Customer source (database, file, API)
  Sink: Staging table (staging.CustomerSource)
  Purpose: Land raw data

Activity 2: Data Flow Activity (Approach 1)
  OR
Activity 2: Stored Procedure Activity (Approach 2)
  Stored Procedure Name: usp_SCD_Type2_Customer
  Purpose: Process SCD Type 2 logic

Activity 3: Stored Procedure Activity
  Purpose: Log processing metadata
  Parameters:
    - RecordsProcessed
    - NewRecords
    - ChangedRecords
    - Timestamp
```

**Real-World Example:**

"In my retail analytics project, we implemented SCD Type 2 for Product dimension:

**Business Requirement:**
- Track product price changes over time
- Track product category reassignments
- Answer questions like: 'What was product price on Black Friday 2023?'
- 50,000 products, daily updates

**Challenges Faced:**

**Challenge 1: Performance**
- Initial implementation: 45 minutes for daily load
- Bottleneck: Lookup transformation on 50k × 50k records

**Solution:**
- Partitioned both source and dimension by ProductCategory (20 categories)
- Hash partitioning on ProductID
- Reduced to 12 minutes

**Challenge 2: Unchanged Records**
- Initial: Processing all 50k products daily (even unchanged)
- Waste of compute and storage

**Solution:**
- Added hash column (MD5 of all tracked attributes)
- Compare hashes: Only process if hash changed
- Reduced daily changes from 50k to ~2k (96% reduction)

**Challenge 3: Late-Arriving Data**
- Sometimes price change dated 3 days ago arrived today
- Broke history (gaps in timeline)

**Solution:**
- Used EffectiveFrom from source (not currentDate())
- Added validation: If EffectiveFrom < existing EffectiveTo, reject with alert
- Manual intervention for backdated changes

**Implementation:**

```
Data Flow: SCD_Product

Source 1: New products with hash
  Derived Column: 
    SourceHash = md5(concat(ProductName, Category, Price, Supplier))

Source 2: Current dimension
  Filter: IsCurrent = 1
  Derived Column:
    DimHash = md5(concat(ProductName, Category, Price, Supplier))

Lookup: New JOIN Current ON ProductID

Derived Column:
  HasChanged = SourceHash != DimHash
  IsNew = isNull(DimProductID)

Conditional Split:
  - New: IsNew == true
  - Changed: HasChanged == true AND IsNew == false
  - Unchanged: Skip

[Rest of SCD logic as described above]

Monitoring:
  - Log to control table:
      Date, NewProducts, ChangedProducts, UnchangedProducts, Duration
```

**Results:**
- Daily load: 12 minutes (down from 45)
- Storage: Only 4% growth per day (vs 100% if storing all daily snapshots)
- Query performance: Fast (IsCurrent flag + indexed SK)
- Historical accuracy: 100% (can recreate any point in time)

**Advanced SCD Variations:**

**SCD Type 1 (Overwrite):**
```
Data Flow:
  Lookup: Existing records
  Alter Row: Update if exists
  Sink: Upsert on CustomerID
  
Result: Only current state, no history
```

**SCD Type 3 (Limited History):**
```
Table Structure:
CustomerID | CurrentCity | PreviousCity | CurrentState | PreviousState

Data Flow:
  Derived Column:
    If changed:
      PreviousCity = OldCity
      CurrentCity = NewCity
  
  Sink: Update record

Result: Only last change tracked, limited history
```

**SCD Type 4 (Separate History Table):**
```
Current Table: DimCustomer (only current)
History Table: DimCustomerHistory (all versions)

Data Flow:
  Branch 1: Insert to History (before update)
  Branch 2: Update Current table
  
Result: Clean current table, complete history separate
```

**SCD Type 6 (Hybrid 1+2+3):**
```
Table Structure:
SK | CustomerID | Name | City | PreviousCity | IsCurrent | EffectiveFrom | EffectiveTo

Combines:
- Type 1: Overwrite on some columns
- Type 2: History with dates
- Type 3: Track previous value

Use case: Want history BUT also quick access to previous value
```

**Monitoring SCD Health:**

```sql
-- Query: Check for gaps in history
SELECT 
    CustomerID,
    EffectiveTo,
    LEAD(EffectiveFrom) OVER (PARTITION BY CustomerID ORDER BY EffectiveFrom) AS NextEffectiveFrom
FROM DimCustomer
WHERE EffectiveTo != '9999-12-31'
    AND EffectiveTo != DATEADD(day, -1, LEAD(EffectiveFrom) OVER (PARTITION BY CustomerID ORDER BY EffectiveFrom))
-- Should return 0 rows (no gaps)

-- Query: Check for overlapping records
SELECT 
    CustomerID,
    COUNT(*) AS CurrentRecords
FROM DimCustomer
WHERE IsCurrent = 1
GROUP BY CustomerID
HAVING COUNT(*) > 1
-- Should return 0 rows (only 1 current per customer)

-- Query: Check orphaned historical records
SELECT *
FROM DimCustomer t1
WHERE IsCurrent = 0
    AND NOT EXISTS (
        SELECT 1 
        FROM DimCustomer t2 
        WHERE t2.CustomerID = t1.CustomerID 
        AND t2.IsCurrent = 1
    )
-- Should return 0 rows (every historical record should have a current record)
```

**Trap question - "Why use SCD Type 2 instead of just keeping all daily snapshots?"**

**Answer**: "Daily snapshots have major problems:

1. **Storage Explosion:**
   - 10,000 customers × 365 days = 3.65 million rows/year
   - Most customers don't change daily
   - SCD Type 2: Only store when changed
   - My project: 3.65M rows → 150K rows (97% reduction)

2. **Query Complexity:**
   - Snapshots: 'Give me customer data for June 15' = easy
   - 'When did customer move?' = scan all snapshots, compare (slow)
   - SCD Type 2: Dates built-in, queries are simpler and faster

3. **Data Quality:**
   - Snapshots: Miss the exact change date
   - Customer moved on June 15, but we only snapshot at midnight
   - SCD Type 2: Capture exact change timestamp

4. **Cost:**
   - Snapshots: 365× storage cost
   - SCD Type 2: ~5-10× storage (only changes)
   - Snapshots: Query full table always
   - SCD Type 2: Query with IsCurrent = 1 (indexed, fast)

**When snapshots ARE better:**
- Compliance requirements (SOX, GDPR) mandate daily snapshots
- Need exact state at specific time regardless of changes
- Inexpensive storage, query performance not critical

For most data warehouse scenarios, SCD Type 2 is the industry standard for good reason."

**Pro Tip:** Always mention you validate SCD implementation with the health check queries above and monitor for anomalies - shows you understand data quality is ongoing, not one-time.

---

This is comprehensive interview preparation content. Would you like me to continue with more questions covering Performance Optimization, Troubleshooting, Advanced Scenarios, or specific trap questions?# ADF-11: Common Interview Questions - Master Your ADF Interview

## Table of Contents
1. [Basic Fundamentals (Questions 1-25)](#basic-fundamentals)
2. [Integration Runtime Deep Dive (Questions 26-40)](#integration-runtime-deep-dive)
3. [Activities and Pipelines (Questions 41-60)](#activities-and-pipelines)
4. [Data Flows and Transformations (Questions 61-75)](#data-flows-and-transformations)
5. [Advanced Scenarios (Questions 76-90)](#advanced-scenarios)
6. [Performance and Optimization (Questions 91-100)](#performance-and-optimization)
7. [Troubleshooting and Production Issues (Questions 101-110)](#troubleshooting-and-production-issues)
8. [Trap Questions and How to Handle Them](#trap-questions)

---

## Basic Fundamentals

### Q1: What is Azure Data Factory and why would you use it over other ETL tools?

**Layman Answer:**
Think of Azure Data Factory as a smart data moving service. Imagine you have data sitting in 10 different places - some files in cloud storage, some in databases, some coming from APIs. ADF is like a professional moving company that knows exactly how to pack, transport, and unpack your data to wherever it needs to go.

**Technical Answer:**
Azure Data Factory is a cloud-based data integration service that allows you to create data-driven workflows for orchestrating data movement and transforming data at scale. It's a fully managed, serverless data integration solution.

**Why choose ADF:**
- **Cloud-native**: Built for Azure ecosystem, seamless integration with Azure services
- **Serverless**: No infrastructure to manage, automatic scaling
- **Code-free/low-code**: Visual interface for 90% of tasks, code when needed
- **Cost-effective**: Pay only for what you use (pipeline runs, data movement, data flows)
- **Hybrid connectivity**: Can connect on-premises and cloud data sources via Self-hosted IR
- **90+ native connectors**: Supports most data sources out-of-the-box

**When interviewer asks follow-up "Why not use Databricks or Synapse Pipelines?":**
"Great question! It depends on the use case:
- **Use ADF when**: You need simple to moderate ETL, orchestration across services, code-free development, or hybrid connectivity
- **Use Databricks when**: You need complex transformations, ML integration, or Spark-based processing
- **Use Synapse Pipelines when**: Your workload is tightly coupled with Synapse Analytics warehousing
- In my experience, I've often used ADF to orchestrate Databricks jobs - they complement each other rather than compete."

**Pro Tip:** Always mention you've used ADF in combination with other tools, showing you understand the ecosystem.

---

### Q2: Explain the difference between Control Flow and Data Flow in ADF.

**Layman Answer:**
Control Flow is like the traffic controller at an airport - it decides which planes (activities) take off, when, and in what order, but it doesn't care what's inside the planes. Data Flow is like the baggage handling system - it actually moves and processes the luggage (data) inside.

**Technical Answer:**
- **Control Flow**: Orchestration layer that manages the execution logic
  - Defines the sequence and conditions of activity execution
  - Handles branching (If Condition), looping (ForEach, Until), and dependencies
  - Minimal data processing - mostly metadata operations
  - Examples: Execute Pipeline, Lookup, GetMetadata, Wait, Web activity

- **Data Flow**: Data transformation layer that processes actual data
  - Spark-based distributed data transformation engine
  - Handles row-level transformations at scale
  - Visual ETL designer with transformations like Join, Aggregate, Derive
  - Executes on Azure Databricks/Integration Runtime clusters

**Real-world example you can mention:**
"In one of my projects, we had to load customer data from 50+ source tables. The Control Flow used ForEach to loop through a metadata table listing all sources, and for each source, it triggered a Data Flow that performed the actual transformations like deduplication, type casting, and business rule application."

**Interview Red Flag to Avoid:** Don't say "Data Flow is just for transformations." It's more nuanced - Copy Activity also moves data but isn't a Data Flow. Be precise.

---

### Q3: What are the three types of Integration Runtime and when do you use each?

**Answer with confidence:**

**1. Azure Integration Runtime (Azure IR)**
- **What it is**: Microsoft-managed compute in Azure data centers
- **When to use**: 
  - Copying between cloud data sources (Blob, ADLS, SQL DB, Cosmos)
  - Running Data Flows
  - Dispatching activities to cloud services (Databricks, HDInsight)
- **Key point**: No setup required, auto-resolves region closest to data source
- **Limitation**: Cannot access on-premises data or virtual network resources

**2. Self-hosted Integration Runtime (SHIR)**
- **What it is**: Software you install on your own machine/VM
- **When to use**:
  - Accessing on-premises data sources (SQL Server behind firewall)
  - Accessing data in virtual networks without public endpoints
  - File system access on private networks
  - When you need specific drivers or libraries
- **Key point**: Acts as a bridge between cloud and on-premises
- **Setup consideration**: Requires port 443 outbound to Azure, can setup HA with multiple nodes

**3. Azure-SSIS Integration Runtime**
- **What it is**: Managed SSIS (SQL Server Integration Services) package execution environment
- **When to use**:
  - Migrating existing SSIS packages to cloud (lift-and-shift)
  - Need SSIS-specific transformations not available in ADF
  - Organization has heavy investment in SSIS and wants cloud execution
- **Key point**: Most expensive option, only for SSIS compatibility

**Story you can tell:**
"In my last project with a retail client, we used all three. Azure IR handled cloud-to-cloud data movement from Blob to Synapse. We set up a Self-hosted IR on an on-premises VM to extract data from their legacy SQL Server 2012 database that couldn't be exposed to internet. And we had Azure-SSIS IR for about 30 existing SSIS packages that handled complex transformations - migration would've taken months, so we did lift-and-shift."

**Trap question variation**: "Can't you just use Azure IR for everything?"
**Answer**: "No, Azure IR cannot connect to on-premises resources or VNet-protected resources. It only works with publicly accessible endpoints. For secure on-premises access, SHIR is mandatory."

---

### Q4: What is a Linked Service? How is it different from a Dataset?

**Layman Answer:**
A Linked Service is like having the phone number and address of a restaurant. A Dataset is like knowing what's on the menu at that restaurant. The Linked Service tells ADF how to connect; the Dataset tells ADF what data to access.

**Technical Answer:**

**Linked Service:**
- Connection string to a data source
- Contains authentication credentials (connection string, key, managed identity, service principal)
- Defines WHERE to connect
- Reusable across multiple datasets and pipelines
- Examples: "MyAzureSQLDatabase", "MyBlobStorageAccount"

**Dataset:**
- Represents the data structure within a data source
- Points to specific data (table, file, folder)
- Defines WHAT data to access
- References a Linked Service
- Can be parameterized for dynamic paths
- Examples: "CustomerTable", "SalesFiles", "JsonAPI"

**Configuration example you can mention:**
```
Linked Service: "OnPremSQL"
- Connection: myserver.database.windows.net
- Auth: SQL Authentication
- Integration Runtime: SelfHostedIR

Dataset: "CustomerData"
- Linked Service: OnPremSQL
- Table: dbo.Customers
- Parameters: @TableName (for dynamic table selection)
```

**Interview follow-up - "Can you have a Dataset without a Linked Service?"**
**Answer**: "No, that's not possible. Every Dataset must reference a Linked Service because the Dataset needs to know where to connect. However, you can have a Linked Service without a Dataset if you're using activities that connect directly, like a Web Activity calling an API using an HTTP Linked Service."

**Pro Tip:** Always mention parameterization - "In my projects, I always parameterize Dataset file paths and table names so the same Dataset can be reused across environments and scenarios."

---

### Q5: Explain pipeline parameters vs variables. When do you use each?

**This is a common trap question - answer carefully:**

**Pipeline Parameters:**
- **Scope**: Passed INTO the pipeline from outside (trigger, parent pipeline, manual run)
- **Read-only**: Cannot be modified during pipeline execution
- **Use cases**:
  - Environment-specific values (ServerName, DatabaseName)
  - Values passed from parent to child pipelines
  - Date ranges for data loads (StartDate, EndDate)
  - File paths or table names
- **Declaration**: Defined at pipeline level before execution
- **Access**: `@pipeline().parameters.ParameterName`

**Variables:**
- **Scope**: Internal to the pipeline, set and used within execution
- **Read-write**: Can be modified using Set Variable activity
- **Use cases**:
  - Storing intermediate results (row count from Lookup)
  - Building dynamic strings (concatenating file paths)
  - Counters in loops
  - Passing values between activities
- **Declaration**: Defined at pipeline level, initialized with default value
- **Access**: `@variables('VariableName')`

**Critical difference:**
Parameters are for INPUT, Variables are for PROCESSING.

**Real scenario you can describe:**
"I built an incremental load pipeline where the parent pipeline passed `@pipeline().parameters.SourceTable` and `@pipeline().parameters.WatermarkDate` to child pipelines. Inside the child pipeline, I used a variable `@variables('RowsProcessed')` to store the count from GetMetadata, then used it in an If Condition to decide whether to proceed with the load or send an alert."

**Trap variation - "Why not just use parameters for everything?"**
**Answer**: "Parameters are read-only. If you need to change a value during execution - like incrementing a counter in a ForEach loop or storing a dynamic result from a Lookup - you MUST use variables. You can't modify parameters mid-execution."

**Common mistake to avoid:** Don't say "variables are better" or "parameters are better" - they serve different purposes. Show you understand when to use each.

---

### Q6: What is a tumbling window trigger and how is it different from a schedule trigger?

**Layman Answer:**
A schedule trigger is like setting an alarm clock that goes off every day at 9 AM, whether you're ready or not. A tumbling window trigger is like a relay race where each runner (pipeline) waits for the previous runner to finish their leg before starting - it ensures your 9 AM run completes before the 10 AM run starts.

**Technical Answer:**

**Schedule Trigger:**
- **Fire and forget**: Triggers at specified time regardless of previous runs
- **No dependency**: Each run is independent
- **Use case**: When you don't care if previous run is still running
- **Limitation**: Can cause overlapping runs
- **Recurrence**: Cron expressions (flexible scheduling)
- **Example**: Daily report generation that must run at 8 AM sharp

**Tumbling Window Trigger:**
- **Sequential execution**: Waits for previous window to complete
- **Time-based windows**: Each run covers a specific time period (window)
- **Backfilling**: Automatically catches up on missed runs
- **Dependency tracking**: Can depend on other tumbling windows
- **Use case**: Incremental loads, time-series processing, preventing overlaps
- **Window parameters**: Automatically passes WindowStart and WindowEnd to pipeline
- **Example**: Hourly data ingestion where each hour must complete before next begins

**Key differences table you can draw/mention:**

| Feature | Schedule Trigger | Tumbling Window |
|---------|------------------|-----------------|
| Execution | Parallel (can overlap) | Sequential |
| Time awareness | No window concept | Passes WindowStart/End |
| Backfill | Manual | Automatic |
| Dependencies | No | Yes (on other windows) |
| Pipeline re-runs | Full pipeline | Specific window only |

**Scenario you MUST mention:**
"In my e-commerce project, we ingested hourly sales data. I used Tumbling Window because:
1. Each hour's data had to complete before next hour started (bank reconciliation requirement)
2. If we had downtime, the trigger automatically backfilled missing hours
3. We set up dependencies - the aggregation pipeline depended on the ingestion pipeline's window completing
4. When we needed to reprocess 2 PM data due to bad records, we could rerun just that 2-3 PM window without affecting other hours."

**Follow-up question - "Why not just use Schedule with sequential ForEach?"**
**Answer**: "That's possible but has limitations. Tumbling Window gives you automatic backfill, window-based parameters without manual calculation, and built-in dependency management between pipelines. Plus, you get window-level monitoring - you can see exactly which hour succeeded or failed, making troubleshooting much easier."

**Interview Red Flag:** Don't say "Tumbling Window is always better." Schedule triggers are simpler for basic scenarios and have lower overhead.

---

### Q7: How do you implement error handling in ADF pipelines?

**This is a critical question - show you understand production-ready patterns:**

**Pattern 1: Activity-Level Failure Handling**
```
1. Set activity policy:
   - Timeout: 7 days (default, adjust based on SLA)
   - Retry: 3 attempts
   - Retry interval: 30 seconds
   
2. Add dependency from failed output:
   Activity A --[On Failure]--> Web Activity (send alert)
   Activity A --[On Success]--> Next Activity
```

**Pattern 2: Try-Catch Using If Condition**
```
Try Block (Execute Pipeline Activity):
  - Execute risky operation
  - On Success: Set @variables('Status') = 'Success'
  - On Failure: Set @variables('Status') = 'Failed'

If Condition:
  - Expression: @equals(variables('Status'), 'Success')
  - True: Continue normal flow
  - False: Execute error handling pipeline
```

**Pattern 3: Metadata-Driven Error Handling**
```
ForEach Loop (process multiple sources):
  - Inside loop: Lookup activity gets source config
  - Copy activity attempts load
  - On Failure: Stored Procedure logs error to control table
  - Pipeline continues to next source (don't fail entire batch)
```

**Real implementation you can describe:**
"In my data warehouse project, I implemented a three-tier error handling strategy:

**Tier 1 - Activity Level**: Every Copy Activity had retry policy of 3 attempts with 1-minute intervals for transient network issues.

**Tier 2 - Pipeline Level**: Used Execute Pipeline activity with try-catch pattern. If main pipeline failed, error handling pipeline:
- Logged error details to SQL table (timestamp, pipeline name, error message, activity name)
- Sent email via Logic App (integrated using Web Activity)
- Wrote failed records to quarantine folder for later investigation

**Tier 3 - Framework Level**: Built a monitoring pipeline that ran every hour:
- Queried ADF monitoring APIs
- Identified stuck pipelines (running > 4 hours)
- Sent escalation alerts
- Automatically triggered kill commands for runaway jobs

For critical pipelines, I also implemented:
- Pre-validation: GetMetadata to check if source files exist before copying
- Post-validation: Row count comparison between source and destination
- Rollback mechanism: If validation failed, Stored Procedure activity truncated destination and marked load as failed"

**Specific techniques to mention:**

1. **Dependency Paths:**
   - Green path: On Success → Next activity
   - Red path: On Failure → Error handler
   - Blue path: On Skip → (conditional)
   - Yellow path: On Completion → (runs regardless)

2. **Error Information Capture:**
   ```
   @activity('CopyData').error.message
   @activity('CopyData').error.errorCode
   @pipeline().RunId
   @utcnow()
   ```

3. **Notification Mechanisms:**
   - Logic Apps (email, Teams messages)
   - Web Activity to custom webhook
   - Azure Function for complex error processing
   - EventGrid for event-driven alerts

**Trap question - "Can you use try-catch blocks in ADF?"**
**Answer**: "ADF doesn't have traditional try-catch syntax like programming languages. Instead, we simulate try-catch using activity dependencies. You execute an activity, then based on its output status (success/failure), you branch to different paths using activity dependency conditions. For more complex scenarios, you can use Execute Pipeline activity combined with If Condition activities to create try-catch-like behavior."

**Pro Tip:** Always mention you log errors to a database for audit trails and troubleshooting - shows production mindset.

---

### Q8: Explain the Copy Activity in detail. What are the key configurations?

**This is a foundational question - demonstrate deep knowledge:**

**What is Copy Activity:**
Think of Copy Activity as a specialized data moving truck. It knows how to read from 90+ source types, understands different file formats, can apply basic transformations during transit, and deliver to 90+ destination types.

**Key Configurations (mention these in order):**

**1. Source Settings:**
- **Dataset**: Points to source data
- **Query type**: Table, Query, Stored Procedure
- **Query timeout**: How long to wait for query execution
- **Partition options**: Physical partitions, dynamic range partitioning (for parallelism)
- **Additional columns**: Add metadata columns (filename, last modified, etc.)
- **Max concurrent connections**: Limit concurrent connections to source

**2. Sink (Destination) Settings:**
- **Dataset**: Points to destination
- **Write behavior**: Insert, Upsert, Stored Procedure
- **Pre-copy script**: Execute SQL before copy (truncate, disable indexes)
- **Table option**: Auto-create table, none
- **Batch size**: Rows per batch (performance tuning)
- **Max concurrent connections**: Parallel writes

**3. Mapping:**
- **Auto-map**: Automatic column mapping by name
- **Explicit mapping**: Manual column-to-column mapping
- **Type conversion**: Automatic or manual type handling
- **Collection reference**: For array/nested data

**4. Settings (Performance & Behavior):**
- **Data Integration Units (DIU)**: 2-256 (auto or manual)
  - Default: Auto (starts at 4, scales up)
  - More DIUs = faster copy but higher cost
- **Degree of parallelism**: Number of parallel copies
- **Fault tolerance**: Skip incompatible rows, log errors
- **Staging**: Enable staged copy (via Blob for large transfers)
- **Data consistency verification**: Validate copied data
- **Compression**: Enable/disable based on format

**5. User Properties:**
- Custom properties for monitoring/searching
- Example: SourceTable, DestinationTable, DataDate

**Real scenario to describe:**

"In my healthcare data migration project, I configured Copy Activity for moving 500GB patient records:

**Source (On-prem SQL Server):**
- Dataset: PatientRecords pointing to SHIR-connected SQL
- Query type: Query (used WHERE clause to filter by date range)
- Partition option: Dynamic range on PatientID (10 parallel reads)
- Additional columns: Added @@SERVERNAME and GETDATE() as source metadata

**Sink (Azure SQL Database):**
- Dataset: PatientRecords_Cloud
- Write behavior: Upsert on PatientID (to handle duplicates)
- Pre-copy script: DISABLE NONCLUSTERED INDEXES (performance)
- Batch size: 10,000 rows (tuned after testing)
- Post-copy: Re-enabled indexes via Stored Procedure activity

**Settings:**
- DIU: 32 (manually set after testing - 16 was too slow, 64 wasn't faster)
- Parallelism: 10 (matched partition count)
- Fault tolerance: Skip incompatible rows, log to Blob
- Staging: Enabled via Blob storage (on-prem to cloud best practice)
- Data consistency: MD5 hash verification enabled

**Result**: Copy took 45 minutes for 500GB (down from 6 hours with default settings)"

**Advanced features to mention:**

1. **Schema Drift Handling:**
   - "Allow schema drift" for dynamic column additions
   - Used in scenarios where source schema changes frequently

2. **Binary Copy:**
   - Fast file-to-file copy without parsing
   - Use case: Backing up entire folders, file replication

3. **Delta Detection:**
   - Specify "last modified" columns
   - Only copy changed records

4. **PolyBase/COPY Command:**
   - For Azure Synapse destinations
   - 10x faster than bulk insert

**Trap question - "Is Copy Activity enough for all data movement needs?"**
**Answer**: "No. Copy Activity is excellent for simple to moderate transformations (column mapping, basic filters), but:
- For complex transformations (joins, aggregations, window functions), use Data Flows
- For row-by-row processing with custom logic, use ForEach with activities inside
- For very large-scale transformations, consider Databricks
- Copy Activity is optimized for movement, not heavy computation."

**Pro Tip:** Always mention you test with smaller datasets first to tune DIU and parallelism settings - shows cost consciousness and testing mindset.

---

### Q9: What is GetMetadata activity and what are common use cases?

**Layman Answer:**
GetMetadata is like sending a scout before moving an army. Instead of copying all the data, you send a quick query to check: Does the file exist? How big is it? When was it last updated? Is the folder empty? This helps you make decisions before expensive operations.

**Technical Answer:**

**GetMetadata Activity:**
- Retrieves metadata information about data stores without reading actual data
- Lightweight operation (no DIU consumption)
- Returns metadata as output that can be used in expressions
- Supports 60+ data sources

**Metadata Fields You Can Retrieve:**

**For Files/Blobs:**
- `exists`: Boolean - does file/folder exist
- `itemName`: Name of file/folder
- `itemType`: File or Folder
- `size`: File size in bytes
- `lastModified`: Timestamp of last modification
- `created`: Creation timestamp
- `childItems`: List of files in folder (names only)
- `contentMD5`: MD5 hash of file content

**For Database Tables:**
- `exists`: Does table exist
- `tableName`: Table name
- `structure`: Schema (column names and types)
- `columnCount`: Number of columns

**Common Use Cases:**

**1. File Existence Check (Pre-validation)**
```
GetMetadata Activity:
  - Get: exists
  - Dataset: InputFile

If Condition:
  - Expression: @activity('GetMetadata1').output.exists
  - True: Proceed with Copy
  - False: Send alert (file not found)
```

**2. Dynamic File Processing**
```
GetMetadata Activity:
  - Get: childItems
  - Dataset: InputFolder

ForEach Activity:
  - Items: @activity('GetMetadata1').output.childItems
  - Inside: Copy each file dynamically
```

**3. Incremental Load Based on LastModified**
```
GetMetadata Activity:
  - Get: lastModified
  
If Condition:
  - Expression: @greater(activity('GetMetadata1').output.lastModified, 
                         variables('LastProcessedDate'))
  - True: Copy file (it's new/updated)
  - False: Skip (already processed)
```

**4. Empty Folder Check**
```
GetMetadata Activity:
  - Get: childItems
  
If Condition:
  - Expression: @greater(length(activity('GetMetadata1').output.childItems), 0)
  - True: Process files
  - False: Skip or alert (no files to process)
```

**5. Schema Validation**
```
GetMetadata Activity:
  - Get: structure
  
Custom Activity or Script:
  - Compare retrieved schema with expected schema
  - Fail pipeline if mismatch (schema drift detection)
```

**Real project scenario:**

"In my financial services project, we received daily transaction files from 20+ bank partners. The challenge was files arrived at different times, some partners occasionally missed files, and file sizes varied (indicating potential issues).

My solution using GetMetadata:

**Step 1 - File Arrival Check (every 15 minutes)**
- GetMetadata on each expected file path
- If exists = false after 2 hours past SLA → Alert operations team
- If exists = true → Proceed to Step 2

**Step 2 - File Validation**
- GetMetadata retrieves: size, lastModified, childItems (in case it's a folder)
- Validate size > 1KB (empty files indicate errors)
- Check lastModified is within last 24 hours (stale file detection)
- Log metadata to SQL control table for audit

**Step 3 - Schema Validation** (for CSV/Parquet)
- GetMetadata retrieves structure
- Compare against expected schema stored in SQL table
- If mismatch → Quarantine file, alert data team
- If match → Proceed to Copy Activity

**Step 4 - Post-Processing Check**
- After successful copy, GetMetadata again to confirm file still exists
- Archive file by appending timestamp
- Update control table with processed timestamp

This approach reduced data quality issues by 70% because we caught problems before expensive copy operations."

**Advanced Patterns:**

**Pattern 1: Watermark Management**
```
GetMetadata to get lastModified of source
→ Lookup to get last watermark from control table
→ If Condition: sourceLastModified > watermark
→ Copy data
→ Update watermark
```

**Pattern 2: Dynamic Pipeline Generation**
```
GetMetadata to list all tables in database
→ ForEach table
→ GetMetadata to get table structure
→ Generate Copy Activity dynamically
```

**Pattern 3: Data Lineage Tracking**
```
GetMetadata before copy: size, lastModified, MD5
→ Copy Activity
→ GetMetadata after copy: size, lastModified, MD5
→ Stored Procedure: Log lineage data
```

**Trap question - "Why not just try to copy and handle error if file doesn't exist?"**
**Answer**: "That's inefficient and poor practice because:
1. Copy Activity consumes DIUs even if it fails - costs money
2. Error handling after failed Copy is messier than prevention
3. GetMetadata is much faster (seconds vs minutes)
4. You can't get metadata like 'childItems' or 'structure' from Copy Activity failure
5. In production, you want fail-fast validation before expensive operations - it's better for SLAs and cost management."

**Pro Tip:** Mention you always log GetMetadata output to control tables for audit and troubleshooting - shows production maturity.

---

### Q10: Explain ForEach activity. How do you optimize it for performance?

**Layman Answer:**
ForEach is like having a to-do list where you process each item one by one (or hire multiple workers to process items in parallel). You can load 100 files, call 50 APIs, or process 20 database tables - all in a loop without duplicating your pipeline logic.

**Technical Answer:**

**ForEach Activity:**
- Iterates over a collection and executes a set of activities for each item
- Collection can be static array or dynamic (from Lookup, GetMetadata)
- Can run sequentially or in parallel
- Has its own variable scope inside the loop

**Key Configurations:**

**1. Items (Required):**
- Array to iterate over
- Expression: `@activity('Lookup1').output.value` (from Lookup)
- Expression: `@pipeline().parameters.TableList` (from parameter)
- Static: `['table1', 'table2', 'table3']`

**2. Sequential (Performance Critical):**
- `false` (default): Parallel execution (up to 50 concurrent iterations)
- `true`: One-by-one execution
- **Decision point**: Use sequential when:
  - Target system can't handle parallel connections
  - Order matters
  - You need to control resource usage

**3. Batch Count (Parallel Only):**
- When Sequential = false
- Controls how many iterations run concurrently (1-50)
- Default: 20
- **Tuning**: Start low, increase based on target system capacity

**4. Activities Inside:**
- Any activities can go inside (Copy, Execute Pipeline, Web, etc.)
- Access current item: `@item()` or `@item().PropertyName`
- Nested ForEach is supported (but use carefully)

**Performance Optimization Strategies:**

**Strategy 1: Parallel Execution**
```
Sequential: false
Batch Count: 30
Result: 100 files processed in 10 minutes instead of 5 hours
```
**When to use**: Target system can handle concurrent connections, no dependencies between items

**Strategy 2: Execute Pipeline Inside ForEach**
```
ForEach (Sequential: false, Batch: 20)
  → Execute Pipeline Activity (child pipeline)
      → Complex processing logic
```
**Benefit**: Each child pipeline runs on its own compute, true parallelism, easier debugging

**Strategy 3: Batch Processing (Avoid ForEach)**
```
Instead of:
  ForEach 1000 files → Copy each file

Do:
  Copy Activity with wildcard → Process all files in one go
```
**When applicable**: Files have same schema and destination

**Strategy 4: Partition the Work**
```
Parent Pipeline
  → ForEach [Monday, Tuesday, ..., Sunday]
      → Execute Pipeline (parameter: @item())
          → Inside: ForEach files for that day
```
**Benefit**: Two-level parallelism, better monitoring granularity

**Real-world scenario:**

"In my retail analytics project, we had 200 store databases (SQL Server) that needed daily extraction to data lake. Initial design:

**Bad Approach (what NOT to do):**
```
ForEach 200 stores (Sequential: true)
  → Copy Activity
  → Data Flow (transformations)
  
Result: 17 hours total (5 minutes per store × 200)
Missed SLA: Had to finish in 4 hours
```

**Optimized Approach:**
```
Master Pipeline:
  → Lookup: Get list of stores with priority flag
  → ForEach (Sequential: false, Batch: 50)
      → Execute Pipeline 'ProcessStore'
          → Parameter: @item().StoreName, @item().Priority
          
Inside ProcessStore Pipeline:
  → Copy Activity (parallel degree: 4)
  → Data Flow (optimized compute)
  
Result: 2.5 hours total
200 stores / 50 batch = 4 waves
Each wave processes 50 stores in parallel
Avg 3 minutes per store
4 waves × 3 minutes = ~12 minutes per wave × 4 = ~48 minutes + overhead
```

**Further optimizations I implemented:**

1. **Prioritization**: High-priority stores in first batch
2. **Skip Empty Stores**: Pre-filter in Lookup (WHERE RecordCount > 0)
3. **Resource Allocation**: Critical stores got higher DIU settings
4. **Monitoring**: Parent pipeline tracked which stores succeeded/failed
5. **Error Handling**: One store failure didn't stop others

**Advanced Patterns:**

**Pattern 1: Metadata-Driven ForEach**
```
Lookup Activity:
  - Query: SELECT SourceTable, DestTable, WatermarkColumn 
           FROM ETL_Config WHERE IsActive = 1

ForEach:
  - Items: @activity('Lookup1').output.value
  - Inside: 
      → Copy Activity
      → Source Table: @item().SourceTable
      → Dest Table: @item().DestTable
      → Watermark: @item().WatermarkColumn
```

**Pattern 2: Conditional Processing Inside ForEach**
```
ForEach files:
  → GetMetadata (size, lastModified)
  → If Condition
      → If size > 100MB: Execute pipeline with Data Flow
      → Else: Simple Copy Activity
```

**Pattern 3: Nested ForEach (Use Sparingly)**
```
ForEach Countries:
  → ForEach Cities in that Country:
      → Process data for City
      
Warning: Max depth is usually 2 levels
Better: Flatten into single ForEach with composite key
```

**Performance Tuning Tips:**

1. **Right-Size Batch Count:**
   - Too low: Underutilizes resources
   - Too high: Overwhelms target system
   - Sweet spot: Test with 10, 20, 30, 50

2. **Use Execute Pipeline for Heavy Workloads:**
   - Each child pipeline gets its own execution context
   - Better isolation and debugging
   - Can be triggered independently for re-runs

3. **Monitor Concurrency at Target:**
   - Example: Azure SQL has connection limits
   - Batch count × concurrent connections per pipeline = total connections
   - Stay below target limits

4. **Distribute Load:**
   - Instead of 1 ForEach with 500 items
   - 5 pipelines each with ForEach of 100 items
   - Triggered in parallel via Event Grid or multiple triggers

**Trap question - "Why not just create 100 separate pipelines instead of ForEach?"**
**Answer**: "That's technically possible but operationally terrible because:
1. **Maintenance nightmare**: Change logic = update 100 pipelines
2. **No scalability**: Add new item = create new pipeline manually
3. **Monitoring complexity**: 100 pipelines to track vs 1 ForEach pipeline
4. **Metadata-driven impossible**: Can't dynamically add items
5. **Code duplication**: Violates DRY principle

ForEach provides dynamic, metadata-driven, maintainable looping. However, if you have 5-10 completely different processes, separate pipelines might be better for clarity."

**Pro Tip:** Always mention you log each iteration's status (success/failed) to a control table for audit and reprocessing - shows production-ready thinking.

---

## Integration Runtime Deep Dive

### Q11: You have an on-premises SQL Server behind a firewall. How do you connect it to ADF?

**This tests practical implementation knowledge:**

**Answer with step-by-step confidence:**

**Step 1: Install Self-Hosted Integration Runtime (SHIR)**

"First, I need to set up a Self-Hosted IR because Azure IR cannot reach on-premises resources. Here's my approach:

1. **Provision SHIR in ADF Portal:**
   - Go to Manage → Integration Runtimes → New
   - Choose Self-Hosted
   - Give it a name like 'OnPremSQL-SHIR'
   - Copy the authentication key (you get 2 keys for HA)

2. **Install SHIR Software:**
   - Download Microsoft Integration Runtime from ADF portal
   - Install on a machine that can reach the SQL Server
   - **Key decision**: Install on:
     - Dedicated VM in same network as SQL (preferred for production)
     - SQL Server itself (only if lightweight workload)
     - Jump box / bastion host if security requires
   
3. **Register SHIR:**
   - Paste authentication key during installation
   - SHIR registers with ADF via HTTPS outbound (port 443)
   - Status changes to 'Running' in portal

**Step 2: Network Configuration**

Network requirements I ensure:
- **Outbound**: SHIR machine needs port 443 to *.servicebus.windows.net and *.frontend.clouddatahub.net
- **Inbound**: SQL Server must allow connection from SHIR machine (port 1433 or custom)
- **Firewall rules**: Whitelist Azure data center IPs if organization requires
- **Proxy**: If corporate proxy exists, configure in SHIR settings

**Step 3: Create Linked Service**

```
Linked Service Configuration:
- Type: SQL Server
- Integration Runtime: OnPremSQL-SHIR (select the SHIR created)
- Server name: SQLSERVER01\INSTANCE (or IP: 10.10.10.50)
- Database name: SalesDB
- Authentication: 
  * Option 1: SQL Authentication (username/password in Key Vault)
  * Option 2: Windows Authentication (run SHIR service as domain account)
  * Option 3: Managed Identity (if SQL supports Azure AD auth)

Test Connection: CRITICAL - always test before saving
```

**Step 4: Security Best Practices**

1. **Credentials in Key Vault:**
   - Never hardcode passwords
   - Store SQL credentials in Azure Key Vault
   - Linked Service references Key Vault secret

2. **Least Privilege:**
   - SQL user only has read permissions (for source)
   - Or write permissions only to staging schema (for sink)

3. **Encryption:**
   - Enable TLS encryption on SQL connection
   - Encrypt SHIR communication (default, but verify)

4. **Network Security:**
   - Place SHIR in DMZ if possible
   - Use private endpoints for Azure resources
   - Restrict SQL Server to accept only SHIR machine IP

**Real scenario I handled:**

"In my banking project, we had 15 on-premises SQL Servers across different data centers. My implementation:

**Challenge 1: High Availability**
- Set up 4 SHIR nodes in 2 data centers (2 nodes per DC)
- All nodes registered with same SHIR name
- ADF automatically load-balances across nodes
- If one node fails, traffic routes to others

**Challenge 2: Performance Bottleneck**
- Initial setup: 1 SHIR node, 100 Mbps network → 5 hours for 50GB transfer
- Optimization:
  - Upgraded SHIR machine: 8 CPU, 32GB RAM
  - Increased network bandwidth to 1 Gbps
  - Added 2 more SHIR nodes for parallel processing
  - Result: Same 50GB in 45 minutes

**Challenge 3: Security Compliance**
- SQL passwords stored in Key Vault with 90-day rotation policy
- SHIR machines in hardened subnet with NSG rules
- All traffic logged to Log Analytics
- Disabled public access to SHIR machines
- Used Windows Authentication with service accounts (AD-managed)

**Challenge 4: Monitoring**
- Set up diagnostic settings on SHIR
- Monitored CPU, memory, network on SHIR machines via Azure Monitor
- Created alerts:
  - SHIR disconnected > 5 minutes → Page on-call engineer
  - CPU > 80% for 10 minutes → Investigate
  - Memory > 90% → Auto-restart SHIR service (via runbook)"

**Advanced SHIR Configurations:**

1. **Concurrent Jobs:**
   - Default: 4 concurrent jobs per node
   - Can increase to 16 on powerful machines
   - Setting in SHIR Configuration Manager

2. **Port Configuration:**
   - Default: Random high port for node communication
   - Can specify fixed port (e.g., 8060) for firewall rules
   - Configured in dmgcmd.exe utility

3. **Credential Store:**
   - Credentials encrypted on SHIR machine
   - Uses Windows DPAPI for encryption
   - Only SHIR service account can decrypt

**Trap questions and answers:**

**Q: "Can't you just open SQL Server to internet and use Azure IR?"**
**A**: "Absolutely not, for multiple reasons:
1. Security risk: Exposing production databases to internet is a breach waiting to happen
2. Compliance: Most industries (finance, healthcare) prohibit public database access
3. Performance: SHIR in same network = low latency, fewer network hops
4. Control: SHIR gives you control over which data leaves your network
Even if technically possible, it's professionally irresponsible."

**Q: "What if SHIR machine goes down?"**
**A**: "That's why we implement High Availability:
- 2+ SHIR nodes registered with same name
- ADF load-balances automatically
- If primary fails, secondary takes over within seconds
- For critical pipelines, I also set up:
  - Monitoring alerts on SHIR status
  - Auto-recovery scripts to restart SHIR service
  - Infrastructure-as-Code to quickly provision new SHIR if needed"

**Q: "Can one SHIR connect to multiple SQL Servers?"**
**A**: "Yes, absolutely! One SHIR can connect to multiple:
- SQL Servers in the same network
- Different databases on same server
- Even different data sources (Oracle, File Systems, etc.)
You create multiple Linked Services, all pointing to the same SHIR but different targets. This is common pattern - one SHIR per data center, connecting to all sources in that DC."

**Pro Tip:** Always mention you document SHIR architecture, keep runbooks for SHIR failures, and test failover scenarios - shows operational maturity.

---

### Q12: Explain Data Integration Units (DIU). How do you optimize DIU usage?

**Critical cost and performance question:**

**Layman Answer:**
DIU is like horsepower for your data moving engine. 1 DIU = 1 unit of compute power. Moving 1GB with 2 DIUs is like having a small truck, while 256 DIUs is like having a massive convoy. More DIUs = faster (and more expensive), but only up to a point where you hit other bottlenecks.

**Technical Deep Dive:**

**What is DIU:**
- **Definition**: Data Integration Unit = 4 vCPUs + 16GB RAM
- **Applies to**: 
  - Copy Activity (cloud-to-cloud only)
  - Data Flow execution
  - External activities dispatching
- **Does NOT apply to**: Self-hosted IR activities (uses your compute)
- **Range**: 2-256 DIUs (must be power of 2: 2, 4, 8, 16, 32, 64, 128, 256)

**Default Behavior:**
- **Auto mode** (default, recommended): ADF starts with 4 DIUs, auto-scales up based on data pattern
- **Manual mode**: You specify exact DIU count

**Cost Model:**
```
DIU-hour price: ~$0.25 (varies by region)
Example: 
- 32 DIUs for 1 hour = $8
- 4 DIUs for 8 hours = $8
(Same cost, but 32 DIUs finishes faster)
```

**When DIUs Matter (and when they don't):**

**DIUs HELP:**
- Large file copy (> 1GB)
- Cloud-to-cloud transfers
- Parallel data movement
- Compressed files that need decompression
- Format conversions (CSV to Parquet)

**DIUs DON'T HELP:**
- Small files (< 100MB) - overhead > benefit
- Network-bound transfers (bottleneck is bandwidth, not compute)
- Source/sink throttling (database can't read/write faster)
- Self-hosted IR scenarios (DIU not applicable)

**Optimization Strategy I Use:**

**Step 1: Benchmark with Auto Mode**
```
First run: Let ADF auto-scale
Monitor: 
- DIU usage in pipeline run details
- Duration
- Data throughput MB/s

Example result:
- Data: 10GB
- Auto-selected: 16 DIUs
- Duration: 8 minutes
- Cost: 16 DIUs × 0.133 hours × $0.25 = $0.53
```

**Step 2: Test Manual Tuning**
```
Test runs with: 8, 16, 32, 64 DIUs
Measure: Duration and cost

Example results:
- 8 DIUs:  20 min, $0.67
- 16 DIUs: 8 min,  $0.53
- 32 DIUs: 7 min,  $0.93
- 64 DIUs: 6 min,  $1.60

Sweet spot: 16 DIUs (best duration/cost ratio)
64 DIUs only 14% faster but 3× cost
```

**Step 3: Profile Your Workload**
```
Look at Copy Activity execution details:
- Source read time
- Sink write time  
- Network transfer time

If source read = 80% of time → Source is bottleneck, DIUs won't help much
If network = 80% → Bandwidth is bottleneck
If balanced → DIUs can help
```

**Real-world optimization example:**

"In my data lake ingestion project, we moved 500GB daily from Blob to ADLS Gen2:

**Initial (naive) approach:**
- Single Copy Activity, Auto DIU
- Result: 4 hours, $15/day

**After analysis:**
- Source read: Fast (Blob is performant)
- Network: 10 Gbps available
- Sink write: Fast (ADLS Gen2 performant)
- Bottleneck: Single large file → no parallelism

**Optimization 1: Partition Large Files**
- Split source: 500GB → 50 files of 10GB each
- Configure: Copy Activity with wildcard
- Enable: Parallel degree = 10
- DIU: Set to 32 manually
- Result: 1.5 hours, $12/day (faster + cheaper)

**Optimization 2: Dynamic DIU Allocation**
- Small files (< 1GB): 4 DIUs
- Medium files (1-10GB): 16 DIUs  
- Large files (> 10GB): 32 DIUs
- Implementation: If Condition based on GetMetadata file size
- Result: 1.5 hours, $8/day (30% cost reduction)

**Optimization 3: Schedule for Cost**
- Non-critical loads: Overnight with 8 DIUs (slower but cheaper)
- Critical loads: Business hours with 32 DIUs (faster, SLA met)
- Result: $5/day average"

**Advanced DIU Patterns:**

**Pattern 1: Parallel Copies**
```
Instead of:
  Single Copy with 64 DIUs → 10 minutes

Do:
  4 parallel Copies with 16 DIUs each → 10 minutes, 60% cheaper
  (Using ForEach with partitioned datasets)
```

**Pattern 2: Staged Copy**
```
For on-prem to cloud:
  Source (on-prem) → SHIR → Blob (staging) → ADLS (final)
  
DIU only applies to Blob → ADLS leg
SHIR → Blob uses self-hosted compute (no DIU)
Benefit: Can tune DIU independently for cloud leg
```

**Pattern 3: Compression Before Transfer**
```
On-prem: Compress files to ZIP/GZ
Transfer: SHIR → Blob (compressed)
ADF: Copy with decompression → Destination (auto-decompression uses DIU)

If compression ratio is 5:1:
- Transfer 5× less data (faster, cheaper network)
- Decompression uses DIU (still net positive)
```

**Common Mistakes to Avoid:**

1. **Over-provisioning:**
   - Setting 256 DIUs for 100MB file
   - Overhead > benefit, just wastes money

2. **Ignoring Bottlenecks:**
   - Source database can only deliver 10 MB/s
   - 256 DIUs won't help - database is the limit

3. **Not Testing:**
   - Assuming more DIU = better
   - Always benchmark: 16 vs 32 vs 64

4. **Forgetting Self-Hosted Scenarios:**
   - DIU doesn't apply to SHIR
   - Scale SHIR by adding nodes, not DIUs

**Monitoring DIU Usage:**

"I always set up monitoring:
1. **Pipeline-level**: Log DIU used per run to SQL table
2. **Cost tracking**: Query ADF metrics for monthly DIU-hours
3. **Optimization reports**: Weekly review of high-DIU pipelines
4. **Alerts**: If pipeline uses > 128 DIUs, alert for review"

**Trap question - "Why not always use 256 DIUs for fastest performance?"**
**Answer**: "Three reasons:
1. **Cost**: 256 DIUs is 64× more expensive than 4 DIUs per hour. For non-critical loads, slower + cheaper is better.
2. **Diminishing returns**: Beyond a certain point (often 32-64 DIUs), you hit other bottlenecks like network bandwidth or source/sink throughput. 256 DIUs won't magically make a 100 Mbps network transfer faster.
3. **Quota limits**: Azure subscriptions have DIU quotas. Using 256 DIUs on every pipeline would exhaust quotas, blocking other pipelines.

I always follow the principle: Use the minimum DIUs that meet SLA. Test incrementally: 4 → 8 → 16 → 32, and stop when adding more DIUs doesn't meaningfully reduce duration."

**Pro Tip:** Always mention you track cost per pipeline and optimize high-cost pipelines first - shows business acumen, not just technical knowledge.

---

## Activities and Pipelines

### Q13: What is Lookup activity and how is it different from GetMetadata?

**Both retrieve information, but serve different purposes:**

**Lookup Activity:**
- **Purpose**: Query data sources to retrieve actual data values
- **Returns**: Rows of data (up to 5000 rows by default)
- **Common use cases**:
  - Retrieve control table data (configuration, watermarks)
  - Get list of tables to process
  - Fetch last processed timestamp
  - Read parameters from database
  - Count rows for validation

**GetMetadata Activity:**
- **Purpose**: Retrieve metadata about data stores (structure, not content)
- **Returns**: Metadata properties (exists, size, schema, etc.)
- **Common use cases**:
  - Check if file/table exists
  - Get file size or last modified time
  - List files in folder
  - Retrieve table schema
  - Validate structure before copy

**Key Differences Table:**

| Aspect | Lookup | GetMetadata |
|--------|--------|-------------|
| Reads | Data rows | Metadata only |
| Query | SQL queries, stored procs | Predefined metadata fields |
| Row limit | 5000 (default, configurable) | N/A |
| Use for | Business logic, config | Validation, structure checks |
| Performance | Reads actual data (slower) | Reads metadata (faster) |

**Lookup Configuration:**

```json
{
  "name": "LookupWatermark",
  "type": "Lookup",
  "typeProperties": {
    "source": {
      "type": "AzureSqlSource",
      "sqlReaderQuery": "SELECT MAX(LastModified) AS Watermark FROM dbo.ControlTable WHERE TableName = 'Customers'"
    },
    "dataset": {
      "referenceName": "ControlDB"
    },
    "firstRowOnly": true  // Returns single row (default)
  }
}
```

**Accessing Lookup Output:**
```
First row only (firstRowOnly: true):
  @activity('LookupWatermark').output.firstRow.Watermark

Multiple rows (firstRowOnly: false):
  @activity('LookupWatermark').output.value[0].TableName
  @activity('LookupWatermark').output.count (row count)
```

**Real-world scenarios:**

**Scenario 1: Metadata-Driven Pipeline**
```
Lookup Activity:
  Query: SELECT SourceTable, DestTable, KeyColumn 
         FROM ETL_Config 
         WHERE IsActive = 1
  firstRowOnly: false (returns all rows)

ForEach Activity:
  Items: @activity('LookupConfig').output.value
  Inside loop:
    Copy Activity
      Source: @item().SourceTable
      Destination: @item().DestTable
```

**Scenario 2: Incremental Load with Watermark**
```
Step 1 - Lookup Old Watermark:
  Query: SELECT Watermark FROM dbo.WatermarkTable WHERE TableName = 'Sales'
  Output: @activity('LookupOldWatermark').output.firstRow.Watermark
  Example: 2024-01-15 10:30:00

Step 2 - Lookup New Watermark:
  Query: SELECT MAX(ModifiedDate) AS NewWatermark FROM Sales
  Output: @activity('LookupNewWatermark').output.firstRow.NewWatermark
  Example: 2024-01-16 14:25:00

Step 3 - Copy Activity:
  Query: SELECT * FROM Sales 
         WHERE ModifiedDate > '@{activity('LookupOldWatermark').output.firstRow.Watermark}'
         AND ModifiedDate <= '@{activity('LookupNewWatermark').output.firstRow.NewWatermark}'

Step 4 - Stored Procedure:
  Update WatermarkTable 
  SET Watermark = '@{activity('LookupNewWatermark').output.firstRow.NewWatermark}'
  WHERE TableName = 'Sales'
```

**Scenario 3: Dynamic Table List**
```
Lookup Activity:
  Query: SELECT TABLE_NAME 
         FROM INFORMATION_SCHEMA.TABLES 
         WHERE TABLE_SCHEMA = 'dbo' 
         AND TABLE_TYPE = 'BASE TABLE'
  firstRowOnly: false

Output example:
[
  {"TABLE_NAME": "Customers"},
  {"TABLE_NAME": "Orders"},
  {"TABLE_NAME": "Products"}
]

ForEach:
  Items: @activity('LookupTables').output.value
  Process each: @item().TABLE_NAME
```

**Scenario 4: Row Count Validation**
```
Lookup on Source:
  Query: SELECT COUNT(*) AS SourceCount FROM SourceTable
  Output: @activity('SourceCount').output.firstRow.SourceCount

Copy Activity: Source → Destination

Lookup on Destination:
  Query: SELECT COUNT(*) AS DestCount FROM DestTable
  Output: @activity('DestCount').output.firstRow.DestCount

If Condition:
  Expression: @equals(
    activity('SourceCount').output.firstRow.SourceCount,
    activity('DestCount').output.firstRow.DestCount
  )
  True: Success notification
  False: Alert - row count mismatch
```

**Advanced Lookup Patterns:**

**Pattern 1: Multi-Column Control Table**
```
Lookup:
  Query: SELECT 
           SourceServer,
           SourceDB,
           SourceTable,
           DestContainer,
           DestFolder,
           LoadType,
           WatermarkColumn
         FROM ETL_Config
         WHERE Environment = '@{pipeline().globalParameters.Environment}'

ForEach:
  Dynamic configuration for each table based on multiple parameters
```

**Pattern 2: Conditional Processing**
```
Lookup:
  Query: SELECT 
           CASE WHEN GETDATE() > NextRunTime 
                THEN 'Run' 
                ELSE 'Skip' 
           END AS Action
         FROM ScheduleTable
         WHERE PipelineName = '@{pipeline().Pipeline}'

If Condition:
  @equals(activity('CheckSchedule').output.firstRow.Action, 'Run')
```

**Pattern 3: Lookup with Parameters**
```
Lookup:
  Query: "SELECT * FROM dbo.GetConfig(@TableName, @Environment)"
  Parameters:
    - @TableName: @pipeline().parameters.TableName
    - @Environment: @pipeline().globalParameters.Env
```

**Performance Considerations:**

1. **Limit Rows:**
   - Default: 5000 rows
   - For more, set `maxRows` property
   - But consider: If you need > 5000 rows, maybe wrong tool
   - Alternative: Export to file, use Copy Activity

2. **Optimize Query:**
   - Use WHERE clauses to filter
   - Create indexes on lookup columns
   - Avoid SELECT * - specify columns

3. **FirstRowOnly:**
   - When you need single value, always set `firstRowOnly: true`
   - Faster execution, simpler output access

**Common Mistakes:**

1. **Wrong Output Access:**
   ```
   Wrong: @activity('Lookup1').output.Watermark
   Right: @activity('Lookup1').output.firstRow.Watermark
   ```

2. **Forgetting FirstRowOnly:**
   ```
   If firstRowOnly: false, output is array even for 1 row
   Access: .output.value[0].ColumnName
   ```

3. **Large Result Sets:**
   - Lookup returning 10,000 rows → Pipeline fails or times out
   - Solution: Paginate or use Copy Activity instead

**Lookup vs GetMetadata Decision Tree:**

```
Question: What do I need?

→ Actual data values (row counts, config data, watermarks)
  → Use Lookup

→ File/table structure information (exists, size, schema)
  → Use GetMetadata

→ List of files to process
  → Use GetMetadata (childItems)

→ List of tables from INFORMATION_SCHEMA
  → Use Lookup (it's data, not metadata)

→ Check if today's file arrived
  → Use GetMetadata (exists property)

→ Get last processed timestamp from control table
  → Use Lookup (it's a data value)
```

**Trap question - "Can I use Lookup to check if a file exists?"**
**Answer**: "Technically you could try to Lookup a file and catch the error if it doesn't exist, but that's the wrong approach. Use GetMetadata with 'exists' field - it's designed for that purpose, faster, cleaner error handling, and doesn't consume unnecessary resources trying to read a file that might not exist."

**Pro Tip:** Always mention you log Lookup results to control tables for audit trails and debugging - shows production mindset.

---

### Q14: Explain If Condition activity with real-world examples.

**Layman Answer:**
If Condition is like a fork in the road. Based on a yes/no question (Does file exist? Is row count > 0? Is it past 5 PM?), your pipeline takes one path or another. It's how you build intelligence into pipelines.

**Technical Details:**

**If Condition Activity:**
- Evaluates a boolean expression (true/false)
- Executes different activities based on result
- Supports complex expressions using ADF expression language
- Can be nested for multi-level logic

**Structure:**
```
If Condition Activity
├─ Expression: @equals(...) or @greater(...) etc.
├─ If True Activities: [Activity1, Activity2, ...]
└─ If False Activities: [Activity3, Activity4, ...]
```

**Expression Functions:**

```
Comparison:
- @equals(value1, value2)
- @greater(value1, value2)
- @greaterOrEquals(value1, value2)
- @less(value1, value2)
- @lessOrEquals(value1, value2)

Logical:
- @and(condition1, condition2)
- @or(condition1, condition2)
- @not(condition)

Type checks:
- @empty(value) - checks if null or empty
- @contains(collection, value)
```

**Real-World Scenarios:**

**Scenario 1: File Existence Check**
```
GetMetadata Activity:
  - Get field: exists
  - Dataset: ExpectedFile

If Condition:
  Expression: @activity('GetMetadata1').output.exists
  
  If True:
    → Copy Activity (process the file)
    → Stored Procedure (log success)
  
  If False:
    → Web Activity (send alert email)
    → Set Variable (ErrorMessage = "File not found")
```

**Scenario 2: Row Count Validation**
```
Lookup Activity:
  Query: SELECT COUNT(*) AS RowCount FROM SourceTable

If Condition:
  Expression: @greater(activity('Lookup1').output.firstRow.RowCount, 0)
  
  If True (has data):
    → Copy Activity
    → Data Flow (transformations)
  
  If False (empty table):
    → Web Activity (notify: no data to process)
    → Skip processing (saves cost)
```

**Scenario 3: Time-Based Logic**
```
Set Variable (CurrentHour):
  Value: @formatDateTime(utcnow(), 'HH')

If Condition:
  Expression: @less(int(variables('CurrentHour')), 12)
  
  If True (before noon):
    → Execute Pipeline (Morning_ETL)
  
  If False (afternoon):
    → Execute Pipeline (Afternoon_ETL)
```

**Scenario 4: Environment-Based Configuration**
```
If Condition:
  Expression: @equals(pipeline().globalParameters.Environment, 'PROD')
  
  If True (Production):
    → Copy with validation and logging
    → Send success email to stakeholders
    → Archive source files
  
  If False (Dev/Test):
    → Simple copy without validation
    → No notifications
```

**Scenario 5: Error Recovery**
```
Copy Activity (attempt data load)
  → On Failure path → Set Variable (Status = 'Failed')
  → On Success path → Set Variable (Status = 'Success')

If Condition:
  Expression: @equals(variables('Status'), 'Failed')
  
  If True:
    → Wait Activity (30 seconds)
    → Copy Activity (retry)
    → If still fails → Web Activity (alert)
  
  If False:
    → Continue to next step
```

**Scenario 6: Conditional File Processing**
```
GetMetadata Activity:
  - Get fields: size, lastModified

If Condition (check size):
  Expression: @greater(activity('GetMetadata1').output.size, 1048576)
  // 1048576 bytes = 1 MB
  
  If True (large file):
    → Execute Pipeline (Process_With_DataFlow)
      // Use Data Flow for complex transformations
  
  If False (small file):
    → Copy Activity (simple copy)
      // No need for expensive Data Flow
```

**Scenario 7: Business Day Check**
```
Set Variable (DayOfWeek):
  Value: @formatDateTime(utcnow(), 'dddd')

If Condition:
  Expression: @and(
    not(equals(variables('DayOfWeek'), 'Saturday')),
    not(equals(variables('DayOfWeek'), 'Sunday'))
  )
  
  If True (weekday):
    → Execute business pipelines
  
  If False (weekend):
    → Skip processing OR
    → Execute weekend-specific pipelines
```

**Scenario 8: Multi-Condition Logic**
```
Lookup (check configuration):
  Query: SELECT LoadType, Priority, IsActive FROM Config

If Condition 1:
  Expression: @equals(activity('Lookup1').output.firstRow.IsActive, true)
  
  If True:
    If Condition 2 (nested):
      Expression: @equals(activity('Lookup1').output.firstRow.LoadType, 'Full')
      
      If True:
        → Full Load Pipeline
      
      If False:
        → Incremental Load Pipeline
  
  If False:
    → Skip (table not active)
```

**Advanced Patterns:**

**Pattern 1: Try-Catch Simulation**
```
// Try block
Execute Pipeline Activity (risky operation)
  → On Success: Set Variable (Result = 'Success')
  → On Failure: Set Variable (Result = 'Failed')

// Catch block
If Condition:
  Expression: @equals(variables('Result'), 'Failed')
  
  If True:
    → Error handling activities
    → Logging
    → Notifications
```

**Pattern 2: Watermark Comparison**
```
Lookup (Old Watermark):
  Query: SELECT Watermark FROM WatermarkTable

Lookup (New Watermark):
  Query: SELECT MAX(ModifiedDate) AS Watermark FROM SourceTable

If Condition:
  Expression: @greater(
    activity('NewWatermark').output.firstRow.Watermark,
    activity('OldWatermark').output.firstRow.Watermark
  )
  
  If True (new data exists):
    → Copy incremental data
    → Update watermark
  
  If False (no new data):
    → Skip copy (saves cost and time)
    → Log: No new data
```

**Pattern 3: Data Quality Gate**
```
Data Flow (quality checks):
  → Outputs: @activity('DataFlow1').output.dataFlowRunDetails.qualityScore

If Condition:
  Expression: @greaterOrEquals(
    activity('DataFlow1').output.dataFlowRunDetails.qualityScore,
    95
  )
  
  If True (quality acceptable):
    → Load to production tables
    → Notify success
  
  If False (quality issues):
    → Load to quarantine
    → Alert data quality team
    → Don't update watermark (will reprocess)
```

**Complex Expression Examples:**

```
// Check if file modified today
@equals(
  formatDateTime(activity('GetMetadata1').output.lastModified, 'yyyy-MM-dd'),
  formatDateTime(utcnow(), 'yyyy-MM-dd')
)

// Check if within business hours (9 AM - 5 PM)
@and(
  greaterOrEquals(int(formatDateTime(utcnow(), 'HH')), 9),
  less(int(formatDateTime(utcnow(), 'HH')), 17)
)

// Check multiple conditions
@and(
  equals(activity('Lookup1').output.firstRow.Status, 'Active'),
  greater(activity('Lookup1').output.firstRow.RowCount, 0),
  not(empty(activity('Lookup1').output.firstRow.Config))
)

// Contains check
@contains(activity('Lookup1').output.firstRow.ApprovedTables, 'Customers')

// Null/empty check
@not(empty(activity('Lookup1').output.firstRow.FilePath))
```

**Real project example:**

"In my insurance claims processing pipeline:

**Requirement:** 
- Process claims files from 50 partners
- Each partner has different SLA (some real-time, some daily)
- Large files (> 100MB) need special handling
- Only process if file is complete (no partial uploads)

**Implementation:**

```
Step 1 - Get File Info:
  GetMetadata:
    - size
    - lastModified
    - itemName

Step 2 - Check File Completeness:
  Wait Activity (30 seconds)
  GetMetadata again
  
  If Condition:
    Expression: @equals(
      activity('GetMetadata1').output.size,
      activity('GetMetadata2').output.size
    )
    // If size unchanged = file is complete
    
    If True: Proceed
    If False: Wait and retry (file still uploading)

Step 3 - Lookup Partner Config:
  Query: SELECT Priority, ProcessingType 
         FROM PartnerConfig 
         WHERE PartnerName = '@{activity('GetMetadata1').output.itemName}'

Step 4 - Priority Routing:
  If Condition (High Priority):
    Expression: @equals(activity('LookupConfig').output.firstRow.Priority, 'High')
    
    If True:
      → Execute Pipeline (RealTime_Processing)
      → DIU: 64
      → Send immediate notification
    
    If False:
      If Condition (check file size):
        Expression: @greater(activity('GetMetadata1').output.size, 104857600)
        // 100 MB
        
        If True (large file):
          → Execute Pipeline (Batch_DataFlow)
        
        If False (small file):
          → Copy Activity (simple copy)
```

Result: Reduced processing time by 60% and costs by 40% through intelligent routing."

**Common Mistakes to Avoid:**

1. **Type Mismatch:**
   ```
   Wrong: @greater(activity('Lookup1').output.firstRow.Count, '100')
   // Comparing number to string
   
   Right: @greater(activity('Lookup1').output.firstRow.Count, 100)
   // Or: @greater(int(activity('Lookup1').output.firstRow.Count), 100)
   ```

2. **Null Handling:**
   ```
   Dangerous: @equals(activity('Lookup1').output.firstRow.Value, 'ABC')
   // Fails if Value is null
   
   Safe: @and(
     not(empty(activity('Lookup1').output.firstRow.Value)),
     equals(activity('Lookup1').output.firstRow.Value, 'ABC')
   )
   ```

3. **Complex Nested Logic:**
   - More than 3 levels of nested If Conditions → Hard to maintain
   - Solution: Use Switch-like pattern with multiple If Conditions in parallel
   - Or: Move logic to Stored Procedure/Azure Function

**Performance Tips:**

1. **Fail Fast:**
   - Put validation If Conditions early in pipeline
   - Exit quickly if conditions not met
   - Don't waste time/cost on activities that won't be used

2. **Minimize If Condition Activities:**
   - Each If Condition = orchestration cost
   - Combine multiple conditions into one expression using @and/@or
   - Example: Instead of 3 separate If Conditions, use one with complex expression

3. **Cache Lookup Results:**
   - Don't repeat same Lookup in multiple If Conditions
   - Store in variable, reference variable in conditions

**Trap question - "Why not use Switch activity instead of multiple If Conditions?"**
**Answer**: "ADF doesn't have a Switch activity like traditional programming. To simulate switch-case:

**Option 1: Chain If Conditions**
```
If (value == 'A'): Do A
Else: If (value == 'B'): Do B
      Else: If (value == 'C'): Do C
            Else: Do Default
```

**Option 2: Lookup + Execute Pipeline Pattern**
```
Lookup: Get mapping from control table (Value → PipelineName)
Execute Pipeline: @{activity('Lookup1').output.firstRow.PipelineName}
```

Option 2 is more scalable when you have many cases (10+), because:
- Adding new case = new row in table, no pipeline change
- Easier to maintain
- More readable

But for 2-5 cases, chained If Conditions are simpler and more transparent."

**Pro Tip:** Always mention you log the If Condition result (which branch was taken) for debugging and audit - shows production thinking.

---

### Q15: What is Execute Pipeline activity? When and how do you use it?

**Layman Answer:**
Execute Pipeline is like delegating work. Instead of one massive pipeline doing everything, you break work into smaller, specialized pipelines. The parent pipeline calls child pipelines, passes them instructions (parameters), and coordinates their work. It's like a project manager delegating tasks to team members.

**Technical Details:**

**Execute Pipeline Activity:**
- Calls another pipeline from within a pipeline
- Creates parent-child relationship
- Passes parameters from parent to child
- Can wait for child completion or fire-and-forget
- Child pipeline runs independently (has its own run ID)

**Configuration:**

```json
{
  "name": "ExecuteChildPipeline",
  "type": "ExecutePipeline",
  "typeProperties": {
    "pipeline": {
      "referenceName": "ChildPipelineName"
    },
    "waitOnCompletion": true,  // true = wait, false = fire-and-forget
    "parameters": {
      "SourceTable": "@pipeline().parameters.TableName",
      "LoadDate": "@utcnow()",
      "Environment": "@pipeline().globalParameters.Env"
    }
  }
}
```

**Key Properties:**

1. **waitOnCompletion:**
   - `true` (default): Parent waits for child to complete
     - Success/failure propagates to parent
     - Can use child's output in subsequent activities
   - `false`: Parent continues immediately
     - Fire-and-forget pattern
     - Parent doesn't know child's final status

2. **Parameters:**
   - Pass values from parent to child
   - Child must have corresponding parameters defined
   - Can pass constants, variables, activity outputs

**Why Use Execute Pipeline?**

**Reason 1: Reusability**
```
Instead of:
  Pipeline_A: Copy Table1
  Pipeline_B: Copy Table2  (duplicate logic)
  Pipeline_C: Copy Table3  (duplicate logic)

Do:
  Pipeline_CopyTable (reusable, parameterized)
  Pipeline_Orchestrator:
    → Execute Pipeline_CopyTable (@TableName = 'Table1')
    → Execute Pipeline_CopyTable (@TableName = 'Table2')
    → Execute Pipeline_CopyTable (@TableName = 'Table3')
```

**Reason 2: Modularity**
```
Master Pipeline:
  → Execute Pipeline_Validation
  → Execute Pipeline_Extraction
  → Execute Pipeline_Transformation
  → Execute Pipeline_Loading
  → Execute Pipeline_Notification

Each child pipeline is independently:
- Testable
- Maintainable
- Reusable
- Can be triggered manually
```

**Reason 3: Parallel Execution**
```
Parent Pipeline:
  → ForEach (sequential: false)
      → Execute Pipeline_ProcessRegion
      → Parameter: @item().RegionName

Result: All regions processed in parallel
Each child pipeline runs independently
Better than putting all logic in ForEach
```

**Reason 4: Error Isolation**
```
If one child fails, others can continue:
Execute Pipeline_Region1 → On Failure: Log error, continue
Execute Pipeline_Region2 → Independent
Execute Pipeline_Region3 → Independent
```

**Real-World Scenarios:**

**Scenario 1: Metadata-Driven ETL Framework**

```
Master Pipeline:
  1. Lookup Activity:
     Query: SELECT TableName, SourceDB, DestDB, LoadType
            FROM ETL_Config
            WHERE IsActive = 1
  
  2. ForEach (sequential: false, batch: 20):
     Items: @activity('Lookup1').output.value
     
     Inside ForEach:
       Execute Pipeline: "Generic_Table_Load"
       Parameters:
         - TableName: @item().TableName
         - SourceDB: @item().SourceDB
         - DestDB: @item().DestDB
         - LoadType: @item().LoadType
       waitOnCompletion: true

Generic_Table_Load Pipeline:
  Parameters: TableName, SourceDB, DestDB, LoadType
  
  Activities:
    1. If Condition (LoadType == 'Full'):
       True: Full load logic
       False: Incremental load logic
    
    2. Copy Activity (uses parameters)
    
    3. Stored Procedure (update control table)
```

**Benefits:**
- Add new table = add row to config table (no pipeline changes)
- One generic pipeline handles all tables
- Parallel processing of 20 tables at a time
- Each table load isolated (one failure doesn't stop others)

**Scenario 2: Environment Promotion Pattern**

```
Master_Pipeline (same across DEV/UAT/PROD):
  
  Execute Pipeline: "Get_Configuration"
  Parameters:
    - Environment: @pipeline().globalParameters.Environment
  waitOnCompletion: true
  
  Output: Configuration specific to environment
  
  Execute Pipeline: "Data_Extraction"
  Parameters:
    - SourceServer: @activity('GetConfig').output.pipelineReturnValue.SourceServer
    - SourceDB: @activity('GetConfig').output.pipelineReturnValue.SourceDB
  
  Execute Pipeline: "Data_Transformation"
  
  Execute Pipeline: "Data_Loading"
  Parameters:
    - DestServer: @activity('GetConfig').output.pipelineReturnValue.DestServer
```

**Scenario 3: Error Handling Pattern**

```
Parent Pipeline:
  
  Set Variable (Status = 'Running')
  
  Execute Pipeline: "Risky_ETL_Process"
  waitOnCompletion: true
  On Success path: Set Variable (Status = 'Success')
  On Failure path: Set Variable (Status = 'Failed')
  
  If Condition: @equals(variables('Status'), 'Failed')
    True path:
      → Execute Pipeline: "Error_Handler"
        Parameters:
          - FailedPipeline: 'Risky_ETL_Process'
          - ErrorTime: @utcnow()
          - RunId: @pipeline().RunId
      
      → Execute Pipeline: "Retry_Logic"
        waitOnCompletion: true
      
      → If retry also fails:
          Execute Pipeline: "Send_Alerts"
    
    False path:
      → Continue normal flow
```

**Scenario 4: Cascade Processing**

```
Master Pipeline:
  
  Execute Pipeline: "Load_Dimension_Tables"
  waitOnCompletion: true
  // Must complete before fact tables
  
  If Condition: Dimensions successful
    True:
      ForEach (dimensions):
        Execute Pipeline: "Load_Fact_Tables"
        waitOnCompletion: false  // Parallel fact loads
      
      Wait Activity (poll for all facts to complete)
      
      Execute Pipeline: "Load_Aggregates"
      // Depends on facts
    
    False:
      Fail pipeline (can't load facts without dimensions)
```

**Scenario 5: Multi-Source Aggregation**

```
Master Pipeline:
  
  // Launch all sources in parallel
  Execute Pipeline: "Extract_From_SQL"
  waitOnCompletion: false
  
  Execute Pipeline: "Extract_From_API"
  waitOnCompletion: false
  
  Execute Pipeline: "Extract_From_Files"
  waitOnCompletion: false
  
  // Wait for all to complete
  Until Activity:
    Expression: Check all source pipelines completed
    Activities:
      Web Activity: Call ADF monitoring API
      Wait: 30 seconds
  
  // Combine results
  Execute Pipeline: "Merge_All_Sources"
  waitOnCompletion: true
```

**Advanced Patterns:**

**Pattern 1: Dynamic Pipeline Selection**

```
Lookup: Get pipeline name from config
  Query: SELECT PipelineName FROM Config WHERE Scenario = 'LoadCustomers'
  Output: 'Pipeline_FullLoad' or 'Pipeline_IncrementalLoad'

Execute Pipeline:
  Pipeline: @{activity('Lookup1').output.firstRow.PipelineName}
  // Dynamically execute different pipelines based on config
```

**Pattern 2: Return Values from Child**

```
Child Pipeline:
  ... processing logic ...
  
  Set Variable: RowsProcessed = @activity('Copy1').output.rowsCopied
  
  // Return value to parent (using script/web activity)
  Web Activity: 
    URL: Logs RowsProcessed
  
  // Or use pipeline return value (if supported in your ADF version)

Parent Pipeline:
  Execute Pipeline: ChildPipeline
  
  Access output:
  @activity('ExecuteChild').output.pipelineReturnValue.RowsProcessed
```

**Pattern 3: Conditional Child Execution**

```
GetMetadata: Check if file exists

If Condition: File exists
  True:
    If Condition: File size > 100MB
      True: Execute Pipeline "Process_Large_File"
      False: Execute Pipeline "Process_Small_File"
  
  False:
    Execute Pipeline "Handle_Missing_File"
```

**Monitoring and Debugging:**

**Parent-Child Relationship:**
- In ADF Monitor, parent run shows child runs
- Click parent run → see "Invoked pipelines" section
- Each child has its own run ID
- Can debug child independently

**Access Child Output:**
```
// If waitOnCompletion = true
@activity('ExecuteChildPipeline').output.pipelineRunId
@activity('ExecuteChildPipeline').output.pipelineReturnValue
```

**Performance Considerations:**

1. **Parallel Child Pipelines:**
   - ForEach with Execute Pipeline (sequential: false)
   - Each child gets own compute resources
   - Limited by concurrency settings (batch count)

2. **Fire-and-Forget vs Wait:**
   - Fire-and-forget: Lower parent execution time, but lose status tracking
   - Wait: Better error handling, but parent waits for all children

3. **Depth Limit:**
   - Max nesting depth: Usually 5 levels (check Azure limits)
   - Parent → Child → Grandchild → Great-grandchild → ...
   - Don't go too deep (hard to debug)

**Common Mistakes:**

1. **Parameter Mismatch:**
   ```
   Parent passes: @pipeline().parameters.TableName
   Child expects: @pipeline().parameters.SourceTable
   Result: Error - parameter not found
   
   Solution: Match parameter names or map explicitly
   ```

2. **Circular Dependencies:**
   ```
   Pipeline_A executes Pipeline_B
   Pipeline_B executes Pipeline_A
   Result: Infinite loop or error
   ```

3. **Not Handling Child Failures:**
   ```
   Execute Pipeline with waitOnCompletion: true
   No failure path handling
   Result: Parent fails abruptly, no cleanup
   
   Solution: Add On Failure path with error handling
   ```

4. **Over-Nesting:**
   ```
   Too many levels: Master → Orchestrator → Controller → Processor → Loader
   Result: Hard to debug, confusing monitoring
   
   Better: Flatten hierarchy where possible
   ```

**Real Project Example:**

"In my retail analytics platform, we had 150 store databases, 30 product categories, daily loads:

**Challenge:** 
- 150 stores × 30 categories = 4500 combinations
- Can't create 4500 pipelines
- Must complete in 4-hour window

**Solution:**

```
Master_Daily_Load Pipeline:
  
  Lookup: Get active stores
    Query: SELECT StoreID FROM Stores WHERE IsActive = 1
  
  ForEach Store (sequential: false, batch: 50):
    
    Execute Pipeline: "Store_Daily_Load"
    Parameters:
      - StoreID: @item().StoreID
      - LoadDate: @utcnow()
    waitOnCompletion: true
    
    On Failure:
      - Log to SQL: Store, Error, Timestamp
      - Continue to next store (don't fail entire batch)

Store_Daily_Load Pipeline (child):
  Parameters: StoreID, LoadDate
  
  Lookup: Get categories for this store
    Query: SELECT CategoryID FROM StoreCategories 
           WHERE StoreID = @pipeline().parameters.StoreID
  
  ForEach Category (sequential: false, batch: 10):
    
    Execute Pipeline: "Load_Category_Data"
    Parameters:
      - StoreID: @pipeline().parameters.StoreID
      - CategoryID: @item().CategoryID
      - LoadDate: @pipeline().parameters.LoadDate
    waitOnCompletion: true

Load_Category_Data Pipeline (grandchild):
  Parameters: StoreID, CategoryID, LoadDate
  
  Copy Activity: Extract data
  Data Flow: Transform
  Copy Activity: Load to warehouse
  Stored Procedure: Update control table
```

**Results:**
- 3 pipeline templates handle 4500 combinations
- Parallel execution: 50 stores at a time, 10 categories per store
- Total time: 2.5 hours (met SLA)
- Easy to add new stores/categories (just config table update)
- Failed stores don't impact others
- Granular monitoring (can see exactly which store-category failed)

**Trap question - "Why not put all logic in one big pipeline instead of Execute Pipeline?"**

**Answer**: "One big pipeline has several problems:

1. **Maintainability**: 
   - 1000-line JSON is nightmare to edit
   - One mistake breaks entire pipeline
   - Hard to understand flow

2. **Reusability**: 
   - Duplicate logic across pipelines
   - Change logic = update multiple places

3. **Testing**:
   - Can't test individual components
   - Must run entire pipeline every time

4. **Parallel Execution**:
   - One pipeline = single execution context
   - Execute Pipeline in ForEach = true parallelism
   - Each child gets own compute

5. **Error Handling**:
   - One activity fails = entire pipeline fails
   - Child pipeline fails = can handle gracefully, continue others

6. **Monitoring**:
   - One run ID for everything
   - Execute Pipeline = separate run IDs, easier to debug

7. **Team Collaboration**:
   - Multiple developers can work on different child pipelines
   - Merge conflicts reduced

The only time I'd use one big pipeline is for very simple, linear processes with no reusability needs. For anything complex or reusable, Execute Pipeline is the right pattern."

**Pro Tip:** Always mention you use naming conventions for parent-child pipelines (e.g., Master_*, Sub_*, Child_*) and maintain pipeline documentation showing the hierarchy - shows organizational skills.

---

## Data Flows and Transformations

### Q16: Explain Mapping Data Flows. When do you use them vs Copy Activity?

**Critical decision point - shows architectural understanding:**

**Layman Answer:**
Copy Activity is like hiring movers who just transport boxes from point A to B. They might rearrange boxes, but they don't open them or change contents. Mapping Data Flows is like hiring a processing facility - they open boxes, sort contents, combine items from multiple boxes, apply business rules, and repackage everything before delivery.

**Technical Deep Dive:**

**Mapping Data Flow:**
- Spark-based, distributed data transformation engine
- Visual ETL designer (drag-and-drop transformations)
- Code-free for most scenarios (generates Spark code behind the scenes)
- Runs on Azure IR or Azure-SSIS IR
- Scales horizontally across cluster nodes

**Architecture:**
```
ADF Pipeline
  → Data Flow Activity
      → Spin up Spark cluster (or use existing)
      → Execute transformations
      → Write results
      → Shut down cluster (or keep warm)
```

**When to Use What:**

**Use Copy Activity when:**
- Simple column mapping (rename, reorder)
- Basic type conversions
- Filtering with WHERE clause
- Source-to-sink with minimal transformation
- Performance is critical (Copy is faster for simple moves)
- Cost sensitivity (Copy is cheaper)
- Binary file copy
- Schema-on-read scenarios

**Use Data Flow when:**
- Joins (merge data from multiple sources)
- Aggregations (SUM, AVG, COUNT, GROUP BY)
- Complex transformations (CASE WHEN, string manipulation, date calculations)
- Pivoting/Unpivoting
- Window functions (ROW_NUMBER, RANK, LAG, LEAD)
- Slowly Changing Dimensions (SCD Type 1 & 2)
- Data quality rules (deduplication, validation)
- Lookups and enrichment
- Conditional splits (route data to different outputs)
- Schema drift handling (dynamic schemas)

**Decision Matrix:**

| Scenario | Copy Activity | Data Flow |
|----------|---------------|-----------|
| Move 1TB as-is | ✅ Best choice | ❌ Overkill |
| Join 3 tables | ❌ Can't do | ✅ Use Join transform |
| Add calculated column | ⚠️ Limited | ✅ Derived Column |
| Group by + aggregate | ❌ Can't do | ✅ Aggregate transform |
| Filter rows | ✅ Query/filter | ✅ Filter transform |
| Pivot data | ❌ Can't do | ✅ Pivot transform |
| Simple rename columns | ✅ Column mapping | ⚠️ Overkill (but possible) |
| Deduplicate rows | ❌ Can't do | ✅ Aggregate + Window |
| SCD Type 2 | ❌ Can't do | ✅ Alter Row + Conditional Split |

**Data Flow Transformations (Complete List):**

**Source/Sink:**
- **Source**: Read data (supports 60+ connectors)
- **Sink**: Write data (can have multiple sinks)

**Schema Modifiers:**
- **Select**: Choose columns, rename, reorder, remove duplicates
- **Derived Column**: Add calculated columns (expressions)
- **Cast**: Change data types
- **Flatten**: Unnest hierarchical/JSON data
- **Parse**: Parse JSON/XML strings into columns

**Row Modifiers:**
- **Filter**: Keep/remove rows based on condition
- **Sort**: Order rows
- **Alter Row**: Mark rows for Insert/Update/Delete/Upsert
- **Distinct**: Remove duplicate rows

**Multiple Inputs:**
- **Join**: Merge datasets (Inner, Left, Right, Full, Cross)
- **Exists**: Check if rows exist in another dataset (semi-join)
- **Lookup**: Enrich data with reference table (one-to-one)
- **Union**: Combine datasets (UNION ALL)

**Multiple Outputs:**
- **Conditional Split**: Route rows to different outputs based on conditions
- **New Branch**: Create multiple streams from one source

**Aggregations:**
- **Aggregate**: GROUP BY with SUM, AVG, COUNT, MIN, MAX
- **Surrogate Key**: Generate sequential IDs
- **Pivot**: Rows to columns
- **Unpivot**: Columns to rows

**Advanced:**
- **Window**: ROW_NUMBER, RANK, DENSE_RANK, LAG, LEAD, cumulative SUM
- **External Call**: Call Azure Function/REST API for each row (use sparingly)
- **Assert**: Data quality checks (fail if condition not met)
- **Script**: Custom Spark SQL (for complex scenarios)

**Real-World Scenario - When I chose Data Flow:**

"In my e-commerce analytics project:

**Requirement:**
- Combine customer data from 3 sources (SQL, Cosmos DB, CSV files)
- Apply business rules (calculate customer lifetime value, segment customers)
- Handle duplicate customers across sources (deduplication logic)
- Create Slowly Changing Dimension for customer history
- Output to multiple destinations (Synapse DW, Data Lake)

**Why Copy Activity wasn't enough:**
- Can't join 3 sources
- Can't deduplicate with complex logic (same email but different names)
- Can't implement SCD Type 2
- Can't calculate CLV (requires aggregations and lookups)

**Data Flow Implementation:**

```
Source 1 (SQL - CustomerMaster):
  CustomerID, FirstName, LastName, Email, JoinDate

Source 2 (Cosmos - OnlineActivity):
  UserID, Email, PageViews, LastLogin

Source 3 (CSV - PurchaseHistory):
  Email, TotalSpent, OrderCount

↓

Join 1: CustomerMaster LEFT JOIN OnlineActivity ON Email

↓

Join 2: Result LEFT JOIN PurchaseHistory ON Email

↓

Derived Column:
  - FullName = concat(FirstName, ' ', LastName)
  - CustomerSegment = 
      CASE
        WHEN TotalSpent > 10000 THEN 'VIP'
        WHEN TotalSpent > 5000 THEN 'Premium'
        ELSE 'Standard'
      END
  - CLV = TotalSpent * 1.2  // Simple calculation
  - IsActive = LastLogin > addDays(currentDate(), -30)

↓

Aggregate (Deduplication):
  Group by: Email
  Aggregate:
    FirstName = first(FirstName)
    LastName = first(LastName)
    TotalPageViews = sum(PageViews)
    MostRecentLogin = max(LastLogin)

↓

Window (for SCD):
  - Partition by: Email
  - Sort by: JoinDate DESC
  - ROW_NUMBER() AS RowNum

↓

Conditional Split:
  - Current Records (RowNum == 1) → Sink 1 (Current table)
  - Historical Records (RowNum > 1) → Sink 2 (History table)

↓

Sink 1: Write to Synapse DW (Customer_Current table)
Sink 2: Write to Data Lake (parquet, partitioned by Year/Month)
```

**Result:**
- Processed 10 million customer records
- Execution time: 15 minutes on 16-core cluster
- Cost: ~$3 per run
- Data quality improved 95% (proper deduplication)
- SCD history maintained automatically

**Could NOT have done this with Copy Activity alone. Would have needed:**
- 5+ separate Copy Activities
- Staging tables in SQL
- Stored procedures for logic
- More complex, harder to maintain
- Slower (multiple round trips)

**Performance Optimization for Data Flows:**

1. **Cluster Configuration:**
   - **Compute type**: General Purpose vs Memory Optimized vs Compute Optimized
   - **Core count**: 8, 16, 32, 48, 80 cores (more cores = more parallelism)
   - **Time to live (TTL)**: Keep cluster warm between runs (0-240 minutes)

2. **Partitioning Strategy:**
   - **Round Robin**: Distribute evenly (default, good for most cases)
   - **Hash**: Partition by key (for joins/aggregates on same key)
   - **Dynamic Range**: For skewed data
   - **Fixed Range**: When you know data distribution
   - **Key**: For co-located partitions

3. **Optimization Settings:**
   - **Enable staging**: Use Blob for intermediate results
   - **Column pruning**: Select only needed columns early
   - **Predicate pushdown**: Push filters to source
   - **Broadcast join**: For small lookup tables (< 100MB)

4. **Monitoring:**
   - Data flow debug: Test with sample data
   - Partition visualization: See data distribution
   - Execution plan: Understand Spark operations

**Cost Optimization:**

```
Scenario: Daily data flow running 2 hours

Expensive approach:
- 80-core cluster
- TTL: 0 (spin up each time)
- Cost: 80 cores × 2 hours × $0.274/core-hour = $43.84/day
- Monthly: ~$1,315

Optimized approach:
- 16-core cluster (sufficient for workload)
- TTL: 60 minutes (keeps warm, reduces startup time)
- Cost: 16 cores × 2.2 hours × $0.274 = $9.64/day
- Monthly: ~$289

Savings: 78% cost reduction
```

**When NOT to use Data Flow:**

1. **Simple file copy**: Use Copy Activity (10× cheaper)
2. **Row-by-row processing with external API calls**: Use ForEach + Azure Function (Data Flow is batch-oriented)
3. **Real-time streaming**: Use Stream Analytics or Event Hubs
4. **Very small datasets (< 1MB)**: Overhead not worth it, use Copy Activity or Azure Function
5. **Extremely complex logic**: Consider Databricks (more flexibility with code)

**Hybrid Approach Example:**

```
Pipeline:
  1. Copy Activity: Extract raw data to staging (fast, cheap)
  
  2. If Condition: Check row count
     If > 100,000 rows:
       → Data Flow (worth the cluster startup cost)
     Else:
       → Stored Procedure (simpler for small batches)
  
  3. Copy Activity: Load transformed data to destination
```

**Trap question - "Why not use Databricks for everything instead of Data Flows?"**

**Answer**: "Databricks and Data Flows serve different needs:

**Use Data Flows when:**
- Your team prefers visual/low-code development
- Transformations are standard (joins, aggregates, filters)
- Need quick development without coding
- Tightly integrated with ADF (same UI, monitoring)
- Lower learning curve for non-developers

**Use Databricks when:**
- Complex machine learning or advanced analytics
- Need full Python/Scala/SQL flexibility
- Requires custom libraries or algorithms
- Real-time streaming with Spark Structured Streaming
- Team has strong Spark/Python skills
- Need notebook-based development

**In my projects, I often use both:**
- Data Flows for standard ETL (80% of pipelines)
- Databricks for ML feature engineering and complex analytics (20%)
- ADF orchestrates both: Copy → Data Flow → Databricks → Load

This gives best of both worlds: productivity where possible, power where needed."

**Pro Tip:** Always mention you profile data flows with debug mode before production deployment - shows you understand performance tuning is iterative.

---

### Q17: Walk me through implementing a Slowly Changing Dimension (SCD) Type 2 in ADF.

**This is a classic data warehousing question - demonstrate practical DW knowledge:**

**Layman Answer:**
SCD Type 2 is like maintaining a history book of changes. When a customer moves to a new address, instead of just updating their address (and losing history), you:
1. Mark their old address record as "historical" (close it)
2. Add a new record with the new address (mark it as "current")
3. Track when each version was valid

Now you can answer questions like "Where did customer live in 2022?" or "When did they move?"

**Technical Implementation in ADF:**

**Scenario: Customer Dimension**
```
Source data (daily feed):
CustomerID | Name     | City      | State
1001       | John Doe | Seattle   | WA
1002       | Jane Doe | Portland  | OR

Target (Data Warehouse):
SK | CustomerID | Name     | City      | State | IsCurrent | EffectiveFrom | EffectiveTo
1  | 1001       | John Doe | Boston    | MA    | 0         | 2023-01-01    | 2024-06-15
2  | 1001       | John Doe | Seattle   | WA    | 1         | 2024-06-16    | 9999-12-31
3  | 1002       | Jane Doe | Portland  | OR    | 1         | 2023-01-01    | 9999-12-31

SK = Surrogate Key (auto-incrementing)