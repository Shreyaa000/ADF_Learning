# ADF-14-Integration-Scenarios.md

## Introduction: Why Integration Matters in Enterprise Architecture

**Think of Azure Data Factory as the conductor of an orchestra.** Each Azure service is an instrument—Databricks handles complex transformations, Synapse manages analytics, Logic Apps orchestrates workflows, Azure Functions executes custom code, and Event Grid enables event-driven architecture. ADF conducts them all, ensuring they play together harmoniously.

In Big 4 consulting (Deloitte, PwC, EY, KPMG) and Fortune 500 companies, you'll never see ADF working in isolation. Modern data platforms are **composable architectures** where each service does what it does best, and ADF orchestrates the entire workflow.

### What You'll Learn

This document covers the **5 most common integration patterns** used in enterprise environments:

1. **ADF + Databricks**: Advanced transformations, machine learning, and complex data processing
2. **ADF + Synapse Analytics**: Data warehousing, analytics, and lakehouse architecture
3. **ADF + Logic Apps**: Workflow orchestration, notifications, and business process automation
4. **ADF + Azure Functions**: Custom logic, API integrations, and dynamic processing
5. **ADF + Event Grid**: Event-driven architecture and real-time data pipelines

---

## 1. ADF + Databricks Integration

### Why This Integration Exists

**Layman Explanation**: Imagine you're cooking a meal. ADF is your kitchen assistant who brings you ingredients (data) and cleans up afterward. Databricks is the master chef who performs complex cooking techniques (transformations) that your assistant can't do. Together, they create a perfect meal.

**Technical Reality**: ADF's native transformations (Copy Activity, Data Flows) are great for 80% of ETL tasks. But when you need:
- Complex business logic with Python/Scala
- Machine learning model training or scoring
- Advanced aggregations across billions of rows
- Custom algorithms not available in Data Flows
- Delta Lake operations (merge, time travel, optimization)

**That's when you call Databricks.**

### Common Use Cases in Big 4

1. **Customer 360 Analytics** (Deloitte pattern)
2. **Financial Risk Modeling** (KPMG pattern)
3. **Supply Chain Optimization** (PwC pattern)
4. **Fraud Detection ML Pipeline** (EY pattern)

---

### Scenario 1: Customer 360 Analytics Pipeline

#### Business Requirement

"A retail bank wants to create a unified customer profile combining:
- Transaction data (SQL Database, 500M records)
- Customer service interactions (Salesforce API)
- Web clickstream (ADLS Gen2, JSON files, 2TB)
- Social media sentiment (Twitter API + Azure Cognitive Services)

The data science team needs to build a churn prediction model, which requires complex feature engineering that ADF Data Flows cannot handle."

#### Architecture Flow

```
[Source Systems] 
    ↓
[ADF: Extract & Stage] → [ADLS Gen2: Bronze Layer]
    ↓
[ADF: Trigger Databricks Notebook] → [Databricks: Transform & Feature Engineering]
    ↓
[Databricks: Write to Delta Lake] → [ADLS Gen2: Silver Layer]
    ↓
[ADF: Load to Synapse] → [Synapse: Gold Layer for BI]
    ↓
[Power BI: Customer 360 Dashboard]
```

#### Step-by-Step Implementation

**Step 1: ADF Pipeline - Data Extraction**

Create a pipeline named `PL_Customer360_Master`:

```json
{
  "name": "PL_Customer360_Master",
  "activities": [
    {
      "name": "Extract_Transactions",
      "type": "Copy",
      "inputs": [{"referenceName": "DS_SQL_Transactions"}],
      "outputs": [{"referenceName": "DS_ADLS_Bronze_Transactions"}],
      "typeProperties": {
        "source": {
          "type": "SqlSource",
          "sqlReaderQuery": "SELECT * FROM Transactions WHERE ModifiedDate > '@{pipeline().parameters.WatermarkValue}'"
        },
        "sink": {
          "type": "ParquetSink",
          "storeSettings": {
            "type": "AzureBlobFSWriteSettings",
            "copyBehavior": "PreserveHierarchy"
          }
        }
      }
    },
    {
      "name": "Extract_Salesforce",
      "type": "Copy",
      "dependsOn": [{"activity": "Extract_Transactions", "dependencyConditions": ["Succeeded"]}],
      "inputs": [{"referenceName": "DS_Salesforce_Cases"}],
      "outputs": [{"referenceName": "DS_ADLS_Bronze_Cases"}]
    },
    {
      "name": "Trigger_Databricks_Transformation",
      "type": "DatabricksNotebook",
      "dependsOn": [
        {"activity": "Extract_Transactions", "dependencyConditions": ["Succeeded"]},
        {"activity": "Extract_Salesforce", "dependencyConditions": ["Succeeded"]}
      ],
      "typeProperties": {
        "notebookPath": "/Shared/Customer360/Transform_Customer_Features",
        "baseParameters": {
          "bronze_path": "@{pipeline().parameters.BronzePath}",
          "silver_path": "@{pipeline().parameters.SilverPath}",
          "run_date": "@{formatDateTime(utcnow(), 'yyyy-MM-dd')}"
        }
      },
      "linkedServiceName": {"referenceName": "LS_Databricks"}
    }
  ],
  "parameters": {
    "WatermarkValue": {"type": "string"},
    "BronzePath": {"type": "string", "defaultValue": "abfss://bronze@storage.dfs.core.windows.net"},
    "SilverPath": {"type": "string", "defaultValue": "abfss://silver@storage.dfs.core.windows.net"}
  }
}
```

**Step 2: Databricks Linked Service Configuration**

Critical settings for enterprise environments:

```json
{
  "name": "LS_Databricks",
  "type": "AzureDatabricks",
  "typeProperties": {
    "domain": "https://adb-1234567890123456.7.azuredatabricks.net",
    "authentication": "MSI",  // Managed Identity - NO ACCESS TOKENS!
    "workspaceResourceId": "/subscriptions/{sub-id}/resourceGroups/{rg}/providers/Microsoft.Databricks/workspaces/{workspace}",
    "existingClusterId": "0915-123456-abc123",  // For production
    "newClusterNodeType": "Standard_DS3_v2",    // For development
    "newClusterNumOfWorker": "2:8",             // Auto-scaling
    "newClusterSparkEnvVars": {
      "PYSPARK_PYTHON": "/databricks/python3/bin/python3"
    },
    "newClusterInitScripts": [
      "dbfs:/databricks/init_scripts/install_libraries.sh"
    ]
  }
}
```

**Pro Tip**: In Big 4 projects, **ALWAYS use existing cluster IDs for production**. New cluster provisioning adds 5-7 minutes to pipeline runtime. For dev/test, use new clusters with auto-termination.

**Step 3: Databricks Notebook - Feature Engineering**

Here's what the Databricks notebook does (you'll reference this in interviews):

```python
# Databricks Notebook: Transform_Customer_Features
# Language: Python

# Get parameters from ADF
dbutils.widgets.text("bronze_path", "")
dbutils.widgets.text("silver_path", "")
dbutils.widgets.text("run_date", "")

bronze_path = dbutils.widgets.get("bronze_path")
silver_path = dbutils.widgets.get("silver_path")
run_date = dbutils.widgets.get("run_date")

# Read from Bronze layer
df_transactions = spark.read.parquet(f"{bronze_path}/transactions/date={run_date}")
df_cases = spark.read.parquet(f"{bronze_path}/cases/date={run_date}")
df_clickstream = spark.read.json(f"{bronze_path}/clickstream/date={run_date}")

# Complex Feature Engineering (ADF can't do this)
from pyspark.sql.functions import *
from pyspark.sql.window import Window

# Calculate customer lifetime value
window_spec = Window.partitionBy("customer_id").orderBy("transaction_date")

df_features = df_transactions \
    .withColumn("running_total", sum("amount").over(window_spec)) \
    .withColumn("transaction_count", count("*").over(Window.partitionBy("customer_id"))) \
    .withColumn("avg_transaction", avg("amount").over(Window.partitionBy("customer_id"))) \
    .withColumn("days_since_last_transaction", 
                datediff(current_date(), max("transaction_date").over(Window.partitionBy("customer_id"))))

# Join with customer service data
df_enriched = df_features \
    .join(df_cases, "customer_id", "left") \
    .withColumn("support_tickets_count", count("case_id").over(Window.partitionBy("customer_id"))) \
    .withColumn("avg_resolution_days", avg("resolution_days").over(Window.partitionBy("customer_id")))

# Machine Learning Features
df_ml_features = df_enriched \
    .withColumn("churn_risk_score", 
                when((col("days_since_last_transaction") > 90) & (col("support_tickets_count") > 5), "High")
                .when((col("days_since_last_transaction") > 60) | (col("support_tickets_count") > 3), "Medium")
                .otherwise("Low"))

# Write to Silver layer as Delta Lake
df_ml_features.write \
    .format("delta") \
    .mode("overwrite") \
    .option("overwriteSchema", "true") \
    .save(f"{silver_path}/customer_features")

# Return status to ADF
dbutils.notebook.exit(f"SUCCESS: Processed {df_ml_features.count()} records")
```

**Step 4: Error Handling in ADF**

```json
{
  "name": "Trigger_Databricks_Transformation",
  "type": "DatabricksNotebook",
  "typeProperties": {
    "notebookPath": "/Shared/Customer360/Transform_Customer_Features",
    "baseParameters": { "..." }
  },
  "policy": {
    "timeout": "0.12:00:00",  // 12 hours max
    "retry": 2,
    "retryIntervalInSeconds": 300,  // 5 minutes between retries
    "secureOutput": false,
    "secureInput": false
  },
  "userProperties": [
    {"name": "NotebookName", "value": "Transform_Customer_Features"},
    {"name": "RunDate", "value": "@{pipeline().parameters.run_date}"}
  ]
}
```

#### Common Challenges and Solutions

**Challenge 1: Databricks Cluster Not Starting**

**Error Message**: 
```
"errorCode": "2103",
"message": "Cluster {cluster-id} does not exist or is not running"
```

**Root Causes**:
- Cluster was manually terminated
- Auto-termination kicked in
- Insufficient quota in Databricks workspace

**Your Solution**:
- "I implemented cluster pool strategy—pre-warmed clusters reduce start time from 7 minutes to 45 seconds"
- "Added retry logic with 3 attempts, 5-minute intervals"
- "Set up Azure Monitor alerts when cluster start failures exceed 2 in 1 hour"

**Challenge 2: Notebook Execution Timeout**

**The Problem**: "Processing 2TB of clickstream data was taking 4 hours, exceeding the ADF activity timeout."

**Your Solution**:
- "Implemented incremental processing: partitioned data by hour, processed in parallel using ForEach activity"
- "Optimized Databricks cluster: changed from Standard_DS3_v2 to Standard_DS4_v2 (more memory)"
- "Enabled Delta Lake Z-ordering on customer_id column—reduced query time by 60%"
- "Result: 4 hours → 1.2 hours"

**Challenge 3: Parameter Passing Issues**

**The Problem**: "Databricks notebook was receiving empty parameters from ADF, causing failures."

**Root Cause**: "Parameter names in ADF had spaces: 'bronze path' instead of 'bronze_path'"

**Your Solution**:
- "Standardized naming convention: snake_case for all parameters"
- "Added validation in Databricks notebook:
  ```python
  if not bronze_path:
      dbutils.notebook.exit('FAILURE: bronze_path parameter is empty')
  ```"
- "Implemented logging: wrote all input parameters to Delta table for audit trail"

#### Interview Questions for ADF + Databricks

**Q: "Why not do all transformations in ADF Data Flows?"**

✅ **Your Answer**: "Data Flows are excellent for structured transformations—joins, aggregations, filtering. But they have limitations:
1. **No custom Python/Scala libraries**: Needed scikit-learn for ML features
2. **Limited window functions**: Complex running totals and lag functions are difficult
3. **No Delta Lake native support**: Databricks provides ACID transactions, time travel
4. **Cost**: For heavy transformations, Databricks with spot instances was 40% cheaper than Data Flows at scale
5. **Data Science collaboration**: Our data scientists were already working in Databricks—kept all transformation logic in one place"

**Q: "How do you handle Databricks notebook failures?"**

✅ **Your Answer**: "Multi-layered approach:
1. **Within notebook**: Try-catch blocks, write failed records to 'quarantine' Delta table
2. **In ADF**: Retry policy (3 attempts, exponential backoff), timeout settings
3. **Monitoring**: Azure Monitor alerts on notebook failures, send to PagerDuty
4. **Debugging**: Enabled notebook execution logs, stored in ADLS for 30 days
5. **Recovery**: Implemented checkpoint/restart logic—notebook can resume from last successful stage"

**Q: "How do you secure the connection between ADF and Databricks?"**

✅ **Your Answer**: "Enterprise security pattern:
1. **Managed Identity**: ADF uses MSI to authenticate to Databricks—no secrets in code
2. **Private Link**: Databricks workspace in VNet, private endpoint from ADF
3. **RBAC**: ADF service principal has minimum permissions (Can Attach To, Can Restart)
4. **Key Vault**: Databricks PAT token stored in Key Vault (for legacy scenarios)
5. **Network isolation**: Databricks in isolated VNet, NSG rules restrict access"

---

### Scenario 2: Real-Time ML Scoring Pipeline

#### Business Requirement

"E-commerce company needs real-time fraud detection. As orders arrive in Event Hub, score them using ML model in Databricks and flag suspicious orders within 30 seconds."

#### Architecture

```
[Order Events] → [Event Hub] → [Azure Function: Pre-process]
    ↓
[ADLS Gen2: Landing Zone]
    ↓
[ADF Tumbling Window: Every 1 minute]
    ↓
[Databricks Notebook: Batch Scoring]
    ↓
[Azure SQL: Fraud Scores] → [Logic App: Send Alerts]
```

#### Key Implementation Points

**Databricks Notebook Activity Configuration**:

```json
{
  "name": "Score_Fraud_Model",
  "type": "DatabricksNotebook",
  "typeProperties": {
    "notebookPath": "/Production/FraudDetection/Score_Orders",
    "baseParameters": {
      "window_start": "@{formatDateTime(pipeline().parameters.windowStart, 'yyyy-MM-dd HH:mm:ss')}",
      "window_end": "@{formatDateTime(pipeline().parameters.windowEnd, 'yyyy-MM-dd HH:mm:ss')}",
      "model_path": "dbfs:/models/fraud_detection/v2.3"
    },
    "libraries": [
      {
        "pypi": {"package": "mlflow==2.5.0"}
      },
      {
        "jar": "dbfs:/jars/custom-fraud-library-1.0.jar"
      }
    ]
  },
  "linkedServiceName": {"referenceName": "LS_Databricks_Prod"},
  "policy": {
    "timeout": "0.00:05:00",  // 5 minutes - real-time requirement
    "retry": 0  // No retries for real-time—fail fast
  }
}
```

**Pro Tip**: For real-time scenarios in Big 4 projects, **timeout is aggressive and retry is 0**. You want to know immediately if there's a problem.

---

## 2. ADF + Synapse Analytics Integration

### Why This Integration Exists

**Layman Explanation**: If ADF is the postal service delivering packages (data), Synapse is the massive warehouse that stores, organizes, and lets you quickly find anything. Together, they create a data platform where you can store petabytes and query it in seconds.

**Technical Reality**: Synapse Analytics is Microsoft's unified analytics platform. It combines:
- **Dedicated SQL Pools**: MPP data warehouse (formerly SQL Data Warehouse)
- **Serverless SQL Pools**: Query data lake files without loading
- **Spark Pools**: Apache Spark for big data processing
- **Synapse Pipelines**: Built-in orchestration (yes, it's ADF under the hood!)

### Why Use ADF with Synapse When Synapse Has Pipelines?

**Interview Gold Question**: Interviewers LOVE asking this.

✅ **Your Answer**: "Great question. Here's the reality in enterprise environments:

1. **Separation of concerns**: ADF in a central 'orchestration' resource group, Synapse in 'analytics' resource group. Different teams manage them.
2. **Reusability**: One ADF can orchestrate multiple Synapse workspaces (dev, UAT, prod)
3. **Advanced triggers**: ADF has more mature tumbling window triggers with cross-pipeline dependencies
4. **Monitoring**: Centralized monitoring across all data services in ADF
5. **Politics**: Often, the data engineering team owns ADF; the analytics team owns Synapse. Integration required.

That said, for Synapse-only workloads, I use Synapse Pipelines. No reason to add ADF complexity."

---

### Scenario 1: Lakehouse Architecture with Synapse

#### Business Requirement

"Global manufacturing company needs a modern lakehouse:
- Raw sensor data from 5,000 IoT devices (ADLS Gen2, Parquet files, 10TB)
- ERP data from SAP (on-premises Oracle, 500GB)
- Supply chain data from multiple APIs
- Data scientists need to query raw files (Bronze layer)
- Business analysts need star schema (Gold layer) in Synapse Dedicated Pool"

#### Architecture (Microsoft Medallion Pattern)

```
[Source Systems]
    ↓
[ADF: Ingest] → [ADLS Gen2: Bronze Layer] (Raw data, Parquet)
    ↓
[ADF + Synapse Spark Pool: Transform] → [ADLS Gen2: Silver Layer] (Cleaned, Delta Lake)
    ↓
[ADF + Synapse Dedicated SQL Pool: Load] → [Synapse: Gold Layer] (Star Schema)
    ↓
[Power BI: Consumption]
```

#### Step-by-Step Implementation

**Step 1: Ingest to Bronze Layer**

```json
{
  "name": "PL_Ingest_To_Bronze",
  "activities": [
    {
      "name": "Copy_SAP_Data",
      "type": "Copy",
      "inputs": [{"referenceName": "DS_Oracle_SAP"}],
      "outputs": [{"referenceName": "DS_ADLS_Bronze_SAP"}],
      "typeProperties": {
        "source": {
          "type": "OracleSource",
          "oracleReaderQuery": "SELECT * FROM SAP_ORDERS WHERE LAST_UPDATE_DATE > TO_DATE('@{pipeline().parameters.watermark}', 'YYYY-MM-DD')"
        },
        "sink": {
          "type": "ParquetSink",
          "storeSettings": {
            "type": "AzureBlobFSWriteSettings",
            "maxConcurrentConnections": 8,
            "copyBehavior": "PreserveHierarchy"
          },
          "formatSettings": {
            "type": "ParquetWriteSettings",
            "maxRowsPerFile": 1000000,  // 1M rows per file for optimal query performance
            "fileExtension": ".parquet"
          }
        },
        "enableStaging": true,
        "stagingSettings": {
          "linkedServiceName": {"referenceName": "LS_BlobStaging"},
          "path": "staging/sap"
        },
        "dataIntegrationUnits": 32,  // High DIU for Oracle extraction
        "parallelCopies": 8
      }
    }
  ]
}
```

**Step 2: Transform to Silver using Synapse Spark**

```json
{
  "name": "Transform_To_Silver",
  "type": "SynapseNotebook",
  "dependsOn": [{"activity": "Copy_SAP_Data", "dependencyConditions": ["Succeeded"]}],
  "typeProperties": {
    "notebook": {
      "referenceName": "Transform_SAP_Orders",
      "type": "NotebookReference"
    },
    "parameters": {
      "bronze_path": {
        "value": "@concat('abfss://bronze@', pipeline().parameters.storageAccount, '.dfs.core.windows.net/sap/orders')",
        "type": "Expression"
      },
      "silver_path": {
        "value": "@concat('abfss://silver@', pipeline().parameters.storageAccount, '.dfs.core.windows.net/sap/orders')",
        "type": "Expression"
      }
    },
    "sparkPool": {
      "referenceName": "SparkPool01",
      "type": "BigDataPoolReference"
    },
    "executorSize": "Medium",
    "driverSize": "Medium",
    "numExecutors": 2
  },
  "linkedServiceName": {"referenceName": "LS_Synapse"}
}
```

**Synapse Spark Notebook** (PySpark):

```python
# Synapse Notebook: Transform_SAP_Orders

# Parameters
bronze_path = ""  # Set by ADF
silver_path = ""  # Set by ADF

# Read from Bronze
df_bronze = spark.read.parquet(bronze_path)

# Data Quality Transformations
from pyspark.sql.functions import *

df_cleaned = df_bronze \
    .filter(col("ORDER_DATE").isNotNull()) \
    .withColumn("ORDER_AMOUNT", col("ORDER_AMOUNT").cast("decimal(18,2)")) \
    .withColumn("CUSTOMER_ID", trim(col("CUSTOMER_ID"))) \
    .withColumn("LOAD_TIMESTAMP", current_timestamp()) \
    .dropDuplicates(["ORDER_ID"])

# Write to Silver as Delta Lake
df_cleaned.write \
    .format("delta") \
    .mode("append") \
    .option("mergeSchema", "true") \
    .partitionBy("ORDER_DATE") \
    .save(silver_path)

# Optimize Delta table
spark.sql(f"OPTIMIZE delta.`{silver_path}` ZORDER BY (CUSTOMER_ID)")
```

**Step 3: Load to Gold (Synapse Dedicated SQL Pool)**

```json
{
  "name": "Load_To_Gold_DimCustomer",
  "type": "Copy",
  "dependsOn": [{"activity": "Transform_To_Silver", "dependencyConditions": ["Succeeded"]}],
  "inputs": [{"referenceName": "DS_ADLS_Silver_Customers"}],
  "outputs": [{"referenceName": "DS_Synapse_DimCustomer"}],
  "typeProperties": {
    "source": {
      "type": "ParquetSource",
      "storeSettings": {
        "type": "AzureBlobFSReadSettings",
        "recursive": true,
        "wildcardFileName": "*.parquet"
      }
    },
    "sink": {
      "type": "SqlDWSink",
      "preCopyScript": "TRUNCATE TABLE stg.Customer",  // Staging table
      "writeBehavior": "Insert",
      "sqlWriterUseTableLock": false,
      "disableMetricsCollection": false,
      "tableOption": "autoCreate"
    },
    "enableStaging": true,
    "stagingSettings": {
      "linkedServiceName": {"referenceName": "LS_BlobStaging"},
      "path": "polybase-staging"
    },
    "enableSkipIncompatibleRow": true,
    "redirectIncompatibleRowSettings": {
      "linkedServiceName": {"referenceName": "LS_BlobErrorLogs"},
      "path": "errors/dim_customer"
    }
  }
}
```

**Step 4: Execute Stored Procedure (Merge SCD Type 2)**

```json
{
  "name": "Merge_DimCustomer_SCD2",
  "type": "SqlServerStoredProcedure",
  "dependsOn": [{"activity": "Load_To_Gold_DimCustomer", "dependencyConditions": ["Succeeded"]}],
  "typeProperties": {
    "storedProcedureName": "[dbo].[usp_Merge_DimCustomer_SCD2]",
    "storedProcedureParameters": {
      "RunDate": {
        "value": "@pipeline().parameters.run_date",
        "type": "DateTime"
      },
      "ExecutionId": {
        "value": "@pipeline().RunId",
        "type": "String"
      }
    }
  },
  "linkedServiceName": {"referenceName": "LS_Synapse_DedicatedPool"}
}
```

**Stored Procedure in Synapse** (SCD Type 2 implementation):

```sql
CREATE PROCEDURE [dbo].[usp_Merge_DimCustomer_SCD2]
    @RunDate DATETIME,
    @ExecutionId VARCHAR(100)
AS
BEGIN
    SET NOCOUNT ON;
    
    -- Close expired records
    UPDATE dbo.DimCustomer
    SET 
        EndDate = @RunDate,
        IsCurrent = 0,
        ModifiedDate = GETDATE(),
        ModifiedBy = @ExecutionId
    FROM dbo.DimCustomer tgt
    INNER JOIN stg.Customer src ON tgt.CustomerID = src.CustomerID
    WHERE tgt.IsCurrent = 1
      AND (
          tgt.CustomerName <> src.CustomerName OR
          tgt.Email <> src.Email OR
          tgt.Phone <> src.Phone
      );
    
    -- Insert new/changed records
    INSERT INTO dbo.DimCustomer (
        CustomerID, CustomerName, Email, Phone,
        StartDate, EndDate, IsCurrent, CreatedDate, CreatedBy
    )
    SELECT 
        src.CustomerID,
        src.CustomerName,
        src.Email,
        src.Phone,
        @RunDate AS StartDate,
        '9999-12-31' AS EndDate,
        1 AS IsCurrent,
        GETDATE() AS CreatedDate,
        @ExecutionId AS CreatedBy
    FROM stg.Customer src
    LEFT JOIN dbo.DimCustomer tgt ON src.CustomerID = tgt.CustomerID AND tgt.IsCurrent = 1
    WHERE tgt.CustomerID IS NULL  -- New customer
       OR (
          tgt.CustomerName <> src.CustomerName OR
          tgt.Email <> src.Email OR
          tgt.Phone <> src.Phone
       );  -- Changed customer
    
    -- Log execution
    INSERT INTO dbo.ETLLog (ExecutionId, TableName, RowsAffected, ExecutionDate)
    VALUES (@ExecutionId, 'DimCustomer', @@ROWCOUNT, GETDATE());
END;
```

#### Performance Optimization for Synapse

**Challenge**: "Loading 500GB from ADLS to Synapse Dedicated Pool was taking 3 hours."

**Your Investigation**:
- "Analyzed execution plan in Synapse, found data movement operations consuming 80% of time"
- "Copy activity wasn't using PolyBase despite enableStaging=true"

**Your Solution**:
1. **PolyBase Configuration**:
   ```json
   "enableStaging": true,
   "stagingSettings": {
     "linkedServiceName": {"referenceName": "LS_BlobStaging"},
     "path": "polybase-staging",
     "enableCompression": true
   },
   "polyBaseSettings": {
     "rejectType": "percentage",
     "rejectValue": 10,
     "rejectSampleValue": 100,
     "useTypeDefault": true
   }
   ```

2. **File Size Optimization**:
   - "Split large files into 60MB-1GB chunks for parallel loading"
   - "Used `maxRowsPerFile` in ParquetSink: 1 million rows per file"

3. **Table Distribution**:
   ```sql
   -- Changed from ROUND_ROBIN to HASH distribution
   CREATE TABLE dbo.FactOrders
   (
       OrderID INT NOT NULL,
       CustomerID INT NOT NULL,
       OrderDate DATE NOT NULL,
       Amount DECIMAL(18,2)
   )
   WITH (
       DISTRIBUTION = HASH(CustomerID),  -- Co-locate with DimCustomer
       CLUSTERED COLUMNSTORE INDEX
   );
   ```

4. **Resource Class**:
   - "Increased resource class for service account loading data from smallrc to xlargerc"
   - "Allocated more memory and concurrency slots"

**Result**: 3 hours → 45 minutes (75% improvement)

#### Interview Questions for ADF + Synapse

**Q: "When would you use Synapse Serverless SQL Pool vs. Dedicated SQL Pool?"**

✅ **Your Answer**: 

**Serverless (On-demand)**:
- **Use when**: Ad-hoc queries, data exploration, infrequent access
- **Pros**: Pay-per-query, no infrastructure management, instant start
- **Cons**: Slower for large aggregations, no indexes
- **Example**: "Data scientists querying raw Parquet files in data lake for exploration"

**Dedicated (Provisioned)**:
- **Use when**: Consistent workloads, star schema data warehouse, high-performance queries
- **Pros**: Predictable performance, supports indexes/stats, table distributions
- **Cons**: Always-on cost even when idle, requires capacity planning
- **Example**: "Production star schema serving 500 Power BI reports, 10,000 users"

**In my last project**: "We used Serverless for Bronze/Silver layers (exploration), Dedicated for Gold layer (production reporting). Reduced costs by 40% vs. all-Dedicated approach."

**Q: "How do you handle schema evolution in a lakehouse?"**

✅ **Your Answer**: "Multi-layered strategy:

1. **Bronze Layer**: Schema-on-read, store raw files as-is
2. **Silver Layer**: 
   - Enable Delta Lake `mergeSchema` option
   - Automated schema evolution tracking table
   - Alert data team when new columns detected
3. **Gold Layer (Synapse)**:
   - Use ALTER TABLE ADD COLUMN for backward-compatible changes
   - For breaking changes, create new table version: DimCustomer_v2
   - Maintain view for abstraction: vw_DimCustomer_Latest
4. **Testing**: 
   - Schema validation pipeline before production deployment
   - Compare schemas: source vs. target, alert on mismatches"

---

## 3. ADF + Logic Apps Integration

### Why This Integration Exists

**Layman Explanation**: ADF is excellent at moving and transforming data, but terrible at human interactions. Logic Apps is the opposite—great at sending emails, creating tickets, calling webhooks, but not designed for heavy data processing. Together, they create intelligent workflows: ADF does the data work, Logic Apps handles the notifications and business process automation.

**Technical Reality**: Logic Apps is Azure's iPaaS (Integration Platform as a Service) with 400+ connectors:
- Email (Outlook, Gmail, SendGrid)
- Collaboration (Teams, Slack, SharePoint)
- Ticketing (ServiceNow, Jira, Azure DevOps)
- Monitoring (PagerDuty, Opsgenie)
- Business systems (Dynamics 365, SAP, Salesforce)

### Common Use Cases in Big 4

1. **Approval Workflows**: Data quality issues require human review before processing
2. **Intelligent Notifications**: Send different alerts based on pipeline failure severity
3. **Incident Management**: Create ServiceNow/Jira tickets automatically on failures
4. **Business Process Automation**: Trigger downstream business processes after successful data loads

---

### Scenario 1: Intelligent Pipeline Monitoring and Alerting

#### Business Requirement

"Financial services company needs sophisticated monitoring:
- **Success notifications**: Daily summary email with processing metrics
- **Warning alerts**: Teams message if pipeline takes >2x normal duration
- **Critical failures**: Create PagerDuty incident AND ServiceNow ticket
- **Data quality issues**: Send approval request to data steward before loading bad data
- **Weekly report**: Power BI report emailed to 50 executives every Monday 7 AM"

#### Architecture

```
[ADF Pipeline Execution]
    ↓
[Web Activity: Call Logic App with Status]
    ↓
[Logic App: Intelligent Router]
    ├─→ [Success] → [Format Email] → [Send Daily Summary]
    ├─→ [Warning] → [Post to Teams] → [Log to Table Storage]
    ├─→ [Failure] → [Create PagerDuty Incident + ServiceNow Ticket + SMS to On-Call]
    └─→ [Data Quality Issue] → [Approval Workflow] → [Update ADF Variable]
```

#### Step-by-Step Implementation

**Step 1: ADF Pipeline with Web Activity**

```json
{
  "name": "PL_CustomerLoad_WithMonitoring",
  "activities": [
    {
      "name": "Load_Customer_Data",
      "type": "Copy",
      "typeProperties": { "..." }
    },
    {
      "name": "Notify_Success",
      "type": "WebActivity",
      "dependsOn": [{"activity": "Load_Customer_Data", "dependencyConditions": ["Succeeded"]}],
      "typeProperties": {
        "url": "https://prod-12.eastus.logic.azure.com/workflows/abc123/triggers/manual/paths/invoke?api-version=2016-10-01&sp=%2Ftriggers%2Fmanual%2Frun&sv=1.0&sig=xyz789",
        "method": "POST",
        "headers": {
          "Content-Type": "application/json"
        },
        "body": {
          "pipelineName": "@{pipeline().Pipeline}",
          "runId": "@{pipeline().RunId}",
          "status": "Success",
          "startTime": "@{pipeline().TriggerTime}",
          "endTime": "@{utcnow()}",
          "rowsRead": "@{activity('Load_Customer_Data').output.rowsRead}",
          "rowsCopied": "@{activity('Load_Customer_Data').output.rowsCopied}",
          "duration": "@{activity('Load_Customer_Data').output.copyDuration}",
          "throughput": "@{activity('Load_Customer_Data').output.throughput}",
          "dataRead": "@{activity('Load_Customer_Data').output.dataRead}",
          "dataWritten": "@{activity('Load_Customer_Data').output.dataWritten}"
        }
      }
    },
    {
      "name": "Notify_Failure",
      "type": "WebActivity",
      "dependsOn": [{"activity": "Load_Customer_Data", "dependencyConditions": ["Failed"]}],
      "typeProperties": {
        "url": "https://prod-12.eastus.logic.azure.com/workflows/abc123/triggers/manual/paths/invoke?api-version=2016-10-01&sp=%2Ftriggers%2Fmanual%2Frun&sv=1.0&sig=xyz789",
        "method": "POST",
        "body": {
          "pipelineName": "@{pipeline().Pipeline}",
          "runId": "@{pipeline().RunId}",
          "status": "Failed",
          "errorCode": "@{activity('Load_Customer_Data').error.errorCode}",
          "errorMessage": "@{activity('Load_Customer_Data').error.message}",
          "failureType": "@{activity('Load_Customer_Data').error.failureType}"
        }
      }
    }
  ]
}
```

**Pro Tip**: In Big 4 projects, **NEVER hardcode Logic App URLs**. Store them in Key Vault and reference via `@pipeline().parameters.logicAppUrl`.

**Step 2: Logic App - Intelligent Router**

```json
{
  "definition": {
    "$schema": "https://schema.management.azure.com/providers/Microsoft.Logic/schemas/2016-06-01/workflowdefinition.json#",
    "triggers": {
      "manual": {
        "type": "Request",
        "kind": "Http",
        "inputs": {
          "schema": {
            "type": "object",
            "properties": {
              "pipelineName": {"type": "string"},
              "runId": {"type": "string"},
              "status": {"type": "string"},
              "errorCode": {"type": "string"},
              "errorMessage": {"type": "string"},
              "rowsRead": {"type": "integer"},
              "rowsCopied": {"type": "integer"}
            }
          }
        }
      }
    },
    "actions": {
      "Switch_On_Status": {
        "type": "Switch",
        "expression": "@triggerBody()?['status']",
        "cases": {
          "Success": {
            "case": "Success",
            "actions": {
              "Log_To_Table_Storage": {
                "type": "ApiConnection",
                "inputs": {
                  "host": {
                    "connection": {
                      "name": "@parameters('$connections')['azuretables']['connectionId']"
                    }
                  },
                  "method": "post",
                  "path": "/Tables/@{encodeURIComponent('PipelineExecutionLog')}/entities",
                  "body": {
                    "PartitionKey": "@{formatDateTime(utcNow(), 'yyyy-MM-dd')}",
                    "RowKey": "@{triggerBody()?['runId']}",
                    "PipelineName": "@{triggerBody()?['pipelineName']}",
                    "Status": "Success",
                    "RowsProcessed": "@{triggerBody()?['rowsCopied']}",
                    "ExecutionTime": "@{utcNow()}"
                  }
                }
              },
              "Check_If_Daily_Summary_Needed": {
                "type": "If",
                "expression": {
                  "and": [
                    {"equals": ["@formatDateTime(utcNow(), 'HH')", "08"]},
                    {"equals": ["@formatDateTime(utcNow(), 'mm')", "00"]}
                  ]
                },
                "actions": {
                  "Send_Daily_Summary_Email": {
                    "type": "ApiConnection",
                    "inputs": {
                      "host": {
                        "connection": {
                          "name": "@parameters('$connections')['office365']['connectionId']"
                        }
                      },
                      "method": "post",
                      "path": "/v2/Mail",
                      "body": {
                        "To": "datateam@company.com",
                        "Subject": "Daily Pipeline Summary - @{formatDateTime(utcNow(), 'yyyy-MM-dd')}",
                        "Body": "<html>...</html>",
                        "Importance": "Normal"
                      }
                    }
                  }
                }
              }
            }
          },
          "Failed": {
            "case": "Failed",
            "actions": {
              "Get_Pipeline_Classification": {
                "type": "ApiConnection",
                "inputs": {
                  "host": {
                    "connection": {
                      "name": "@parameters('$connections')['sql']['connectionId']"
                    }
                  },
                  "method": "post",
                  "path": "/v2/datasets/@{encodeURIComponent(encodeURIComponent('default'))},@{encodeURIComponent(encodeURIComponent('default'))}/query/sql",
                  "body": {
                    "query": "SELECT Severity, OnCallTeam, EmailList FROM dbo.PipelineClassification WHERE PipelineName = '@{triggerBody()?['pipelineName']}'"
                  }
                }
              },
              "Switch_On_Severity": {
                "type": "Switch",
                "expression": "@body('Get_Pipeline_Classification')?['resultSets']?['Table1']?[0]?['Severity']",
                "cases": {
                  "Critical": {
                    "case": "Critical",
                    "actions": {
                      "Create_PagerDuty_Incident": {
                        "type": "Http",
                        "inputs": {
                          "method": "POST",
                          "uri": "https://api.pagerduty.com/incidents",
                          "headers": {
                            "Authorization": "Token token=@{body('Get_KeyVault_Secret')?['value']}",
                            "Content-Type": "application/json"
                          },
                          "body": {
                            "incident": {
                              "type": "incident",
                              "title": "ADF Pipeline Failure: @{triggerBody()?['pipelineName']}",
                              "service": {
                                "id": "PXXXXXX",
                                "type": "service_reference"
                              },
                              "urgency": "high",
                              "body": {
                                "type": "incident_body",
                                "details": "Pipeline: @{triggerBody()?['pipelineName']}\nRun ID: @{triggerBody()?['runId']}\nError: @{triggerBody()?['errorMessage']}"
                              }
                            }
                          }
                        }
                      },
                      "Create_ServiceNow_Incident": {
                        "type": "ApiConnection",
                        "inputs": {
                          "host": {
                            "connection": {
                              "name": "@parameters('$connections')['servicenow']['connectionId']"
                            }
                          },
                          "method": "post",
                          "path": "/api/now/v2/table/incident",
                          "body": {
                            "short_description": "ADF Pipeline Failure: @{triggerBody()?['pipelineName']}",
                            "description": "Pipeline Run ID: @{triggerBody()?['runId']}\nError Code: @{triggerBody()?['errorCode']}\nError Message: @{triggerBody()?['errorMessage']}",
                            "urgency": "1",
                            "impact": "1",
                            "assignment_group": "Data Engineering",
                            "category": "Data Pipeline"
                          }
                        }
                      },
                      "Send_SMS_To_OnCall": {
                        "type": "ApiConnection",
                        "inputs": {
                          "host": {
                            "connection": {
                              "name": "@parameters('$connections')['twilio']['connectionId']"
                            }
                          },
                          "method": "post",
                          "path": "/Messages.json",
                          "body": {
                            "body": "CRITICAL: ADF Pipeline @{triggerBody()?['pipelineName']} failed. Check PagerDuty.",
                            "to": "@{body('Get_Pipeline_Classification')?['resultSets']?['Table1']?[0]?['OnCallPhone']}"
                          }
                        }
                      }
                    }
                  },
                  "Warning": {
                    "case": "Warning",
                    "actions": {
                      "Post_To_Teams": {
                        "type": "ApiConnection",
                        "inputs": {
                          "host": {
                            "connection": {
                              "name": "@parameters('$connections')['teams']['connectionId']"
                            }
                          },
                          "method": "post",
                          "path": "/v3/conversations/@{encodeURIComponent('19:abc123@thread.tacv2')}/activities",
                          "body": {
                            "messageBody": "⚠️ **Pipeline Warning**\n\n**Pipeline**: @{triggerBody()?['pipelineName']}\n**Status**: Failed\n**Error**: @{triggerBody()?['errorMessage']}\n**Run ID**: @{triggerBody()?['runId']}\n\n[View in ADF](https://adf.azure.com/monitoring/pipelineruns/@{triggerBody()?['runId']})",
                            "recipient": {
                              "channelId": "19:abc123@thread.tacv2"
                            }
                          }
                        }
                      }
                    }
                  }
                }
              }
            }
          }
        }
      }
    }
  }
}
```

**Step 3: Data Quality Approval Workflow**

```json
{
  "name": "PL_CustomerLoad_WithApproval",
  "activities": [
    {
      "name": "Validate_Data_Quality",
      "type": "Lookup",
      "typeProperties": {
        "source": {
          "type": "SqlSource",
          "sqlReaderQuery": "SELECT COUNT(*) AS InvalidRecords FROM stg.Customer WHERE Email NOT LIKE '%@%.%' OR Phone IS NULL"
        },
        "dataset": {"referenceName": "DS_AzureSQL_Staging"}
      }
    },
    {
      "name": "Check_Quality_Threshold",
      "type": "IfCondition",
      "dependsOn": [{"activity": "Validate_Data_Quality", "dependencyConditions": ["Succeeded"]}],
      "typeProperties": {
        "expression": {
          "value": "@greater(activity('Validate_Data_Quality').output.firstRow.InvalidRecords, 100)",
          "type": "Expression"
        },
        "ifTrueActivities": [
          {
            "name": "Request_Approval",
            "type": "WebActivity",
            "typeProperties": {
              "url": "@pipeline().parameters.ApprovalLogicAppUrl",
              "method": "POST",
              "body": {
                "pipelineName": "@{pipeline().Pipeline}",
                "runId": "@{pipeline().RunId}",
                "invalidRecords": "@{activity('Validate_Data_Quality').output.firstRow.InvalidRecords}",
                "approver": "data-steward@company.com"
              }
            }
          },
          {
            "name": "Wait_For_Approval",
            "type": "Until",
            "dependsOn": [{"activity": "Request_Approval", "dependencyConditions": ["Succeeded"]}],
            "typeProperties": {
              "expression": {
                "value": "@not(equals(variables('ApprovalStatus'), 'Pending'))",
                "type": "Expression"
              },
              "activities": [
                {
                  "name": "Check_Approval_Status",
                  "type": "Lookup",
                  "typeProperties": {
                    "source": {
                      "type": "SqlSource",
                      "sqlReaderQuery": "SELECT ApprovalStatus FROM dbo.ApprovalLog WHERE RunId = '@{pipeline().RunId}'"
                    }
                  }
                },
                {
                  "name": "Update_Variable",
                  "type": "SetVariable",
                  "typeProperties": {
                    "variableName": "ApprovalStatus",
                    "value": "@{activity('Check_Approval_Status').output.firstRow.ApprovalStatus}"
                  }
                },
                {
                  "name": "Wait_30_Seconds",
                  "type": "Wait",
                  "typeProperties": {
                    "waitTimeInSeconds": 30
                  }
                }
              ],
              "timeout": "0.01:00:00"  // 1 hour max wait
            }
          }
        ],
        "ifFalseActivities": [
          {
            "name": "Set_Auto_Approved",
            "type": "SetVariable",
            "typeProperties": {
              "variableName": "ApprovalStatus",
              "value": "Approved"
            }
          }
        ]
      }
    },
    {
      "name": "Load_If_Approved",
      "type": "IfCondition",
      "dependsOn": [{"activity": "Check_Quality_Threshold", "dependencyConditions": ["Succeeded"]}],
      "typeProperties": {
        "expression": {
          "value": "@equals(variables('ApprovalStatus'), 'Approved')",
          "type": "Expression"
        },
        "ifTrueActivities": [
          {
            "name": "Load_To_Production",
            "type": "Copy",
            "typeProperties": { "..." }
          }
        ],
        "ifFalseActivities": [
          {
            "name": "Move_To_Quarantine",
            "type": "Copy",
            "typeProperties": { "..." }
          }
        ]
      }
    }
  ],
  "variables": {
    "ApprovalStatus": {
      "type": "String",
      "defaultValue": "Pending"
    }
  }
}
```

**Logic App - Approval Workflow**:

```json
{
  "actions": {
    "Send_Approval_Email": {
      "type": "ApiConnection",
      "inputs": {
        "host": {
          "connection": {
            "name": "@parameters('$connections')['office365']['connectionId']"
          }
        },
        "method": "post",
        "path": "/v2/Mail",
        "body": {
          "To": "@{triggerBody()?['approver']}",
          "Subject": "APPROVAL REQUIRED: Data Quality Issue in @{triggerBody()?['pipelineName']}",
          "Body": "<html><body><h2>Data Quality Issue Detected</h2><p><strong>Pipeline:</strong> @{triggerBody()?['pipelineName']}</p><p><strong>Invalid Records:</strong> @{triggerBody()?['invalidRecords']}</p><p><strong>Action Required:</strong> Please review the data and approve or reject the load.</p><p><a href='https://approval.company.com/approve?runId=@{triggerBody()?['runId']}'>APPROVE</a> | <a href='https://approval.company.com/reject?runId=@{triggerBody()?['runId']}'>REJECT</a></p></body></html>",
          "Importance": "High"
        }
      }
    },
    "Insert_Approval_Request": {
      "type": "ApiConnection",
      "inputs": {
        "host": {
          "connection": {
            "name": "@parameters('$connections')['sql']['connectionId']"
          }
        },
        "method": "post",
        "path": "/v2/datasets/@{encodeURIComponent(encodeURIComponent('default'))},@{encodeURIComponent(encodeURIComponent('default'))}/tables/@{encodeURIComponent(encodeURIComponent('[dbo].[ApprovalLog]'))}/items",
        "body": {
          "RunId": "@{triggerBody()?['runId']}",
          "PipelineName": "@{triggerBody()?['pipelineName']}",
          "ApprovalStatus": "Pending",
          "Approver": "@{triggerBody()?['approver']}",
          "RequestedDate": "@{utcNow()}",
          "InvalidRecords": "@{triggerBody()?['invalidRecords']}"
        }
      }
    }
  }
}
```

#### Real-World Challenge

**The Problem**: "Approval workflow was timing out after 1 hour. Sometimes approvers took 2-3 hours to respond, causing pipeline failures."

**Your Solution**:
1. **Split the workflow**:
   - Pipeline 1: Validation + Request Approval (completes immediately)
   - Pipeline 2: Load Data (triggered by Logic App after approval)
   
2. **Implemented callback pattern**:
   ```json
   {
     "name": "Register_Callback",
     "type": "WebHook",
     "typeProperties": {
       "url": "@pipeline().parameters.ApprovalWebhookUrl",
       "method": "POST",
       "body": {
         "callbackUrl": "@{listCallbackUrl()}",
         "pipelineName": "@{pipeline().Pipeline}",
         "runId": "@{pipeline().RunId}"
       },
       "timeout": "7.00:00:00"  // 7 days
     }
   }
   ```

3. **Logic App calls webhook after approval**:
   - Approver clicks link in email
   - Azure Function updates database
   - Logic App detects status change
   - Calls ADF webhook URL to resume pipeline

**Result**: No more timeouts, approvals can take days if needed

#### Interview Questions for ADF + Logic Apps

**Q: "Why use Logic Apps instead of Azure Functions for notifications?"**

✅ **Your Answer**: "Both have use cases, but Logic Apps wins for most notification scenarios:

**Logic Apps Advantages**:
1. **400+ pre-built connectors**: Email, Teams, ServiceNow, Twilio—no code needed
2. **Visual designer**: Business analysts can modify workflows without developers
3. **Built-in retry and error handling**: Automatic retries with exponential backoff
4. **Cost**: Pay-per-execution, cheaper for low-volume notifications
5. **Approval workflows**: Built-in approval actions with timeout handling

**Azure Functions Better For**:
1. **Complex logic**: Multi-step data transformation
2. **High throughput**: 1000s of executions per second
3. **Custom protocols**: Proprietary APIs without connectors
4. **Advanced authentication**: Custom token generation, certificate-based auth

**In my last project**: Used Logic Apps for 90% of notifications (email, Teams, ServiceNow), Azure Functions for custom PagerDuty integration requiring complex authentication."

**Q: "How do you secure the connection between ADF and Logic Apps?"**

✅ **Your Answer**: "Defense-in-depth approach:

1. **Logic App Authentication**:
   - Generate SAS token with IP restrictions
   - Store URL in Key Vault, reference in ADF: `@pipeline().parameters.logicAppUrl`
   - Rotate SAS tokens quarterly

2. **Network Security**:
   - Logic App behind Private Link (if supported in subscription)
   - NSG rules allowing only ADF IP ranges

3. **Payload Validation**:
   - Logic App validates incoming schema
   - Checks for required fields before processing
   - Logs all incoming requests for audit

4. **Managed Identity** (where supported):
   - Logic App authenticates to downstream services (SQL, Key Vault) using MSI
   - No credentials in workflow definition

5. **Secrets Management**:
   - All API keys, connection strings in Key Vault
   - Logic App references via Key Vault connector"

---

## 4. ADF + Azure Functions Integration

### Why This Integration Exists

**Layman Explanation**: Azure Functions is like having a programmer on-call who can execute any custom code you need—but only when you need it, and only for as long as it takes. ADF calls this programmer when it encounters something it can't do natively.

**Technical Reality**: Azure Functions fills the gaps in ADF's capabilities:
- Custom data validation logic
- Complex API integrations (authentication, pagination, rate limiting)
- Dynamic configuration (query external systems to determine pipeline behavior)
- File manipulation (unzip, rename, parse custom formats)
- External system integration (legacy SOAP services, proprietary protocols)

### Common Use Cases in Big 4

1. **Dynamic Pipeline Configuration**: Query metadata database to determine which tables to load
2. **Custom File Processing**: Parse fixed-width files, mainframe data, EDI documents
3. **API Integration**: OAuth2 token management, complex pagination, rate limiting
4. **Data Validation**: Business rules too complex for ADF expressions

---

### Scenario 1: Metadata-Driven Pipeline with Azure Functions

#### Business Requirement

"Retail company has 500+ tables across 50 databases. They want ONE master pipeline that loads any table based on configuration. Configuration includes:
- Source query (might be table name, view, or complex SQL)
- Target table name (might include date suffix)
- Load type (Full, Incremental, CDC)
- Business rules (e.g., 'don't load if source row count < 1000')
- Custom transformations (e.g., 'mask SSN column')

ADF expressions can't handle this complexity."

#### Architecture

```
[ADF Master Pipeline]
    ↓
[Azure Function: Get Load Configuration]
    ↓ (Returns JSON with 50 table configs)
[ForEach Activity: Iterate Tables]
    ├─→ [Azure Function: Generate Source Query]
    ├─→ [Copy Activity: Load Data]
    └─→ [Azure Function: Validate & Log]
```

#### Step-by-Step Implementation

**Step 1: Azure Function - Get Load Configuration**

```csharp
// Azure Function: GetLoadConfiguration
[FunctionName("GetLoadConfiguration")]
public static async Task<IActionResult> Run(
    [HttpTrigger(AuthorizationLevel.Function, "post", Route = null)] HttpRequest req,
    ILogger log)
{
    string requestBody = await new StreamReader(req.Body).ReadToEndAsync();
    dynamic data = JsonConvert.DeserializeObject(requestBody);
    
    string loadGroup = data?.loadGroup;  // From ADF parameter
    DateTime runDate = data?.runDate ?? DateTime.UtcNow;
    
    // Query configuration database
    var connectionString = Environment.GetEnvironmentVariable("ConfigDbConnectionString");
    var configurations = new List<LoadConfig>();
    
    using (var connection = new SqlConnection(connectionString))
    {
        await connection.OpenAsync();
        var command = new SqlCommand(@"
            SELECT 
                TableConfigID,
                SourceServer,
                SourceDatabase,
                SourceSchema,
                SourceTable,
                SourceQuery,
                TargetSchema,
                TargetTable,
                LoadType,
                WatermarkColumn,
                WatermarkValue,
                PartitionColumn,
                IsActive,
                ValidationRules,
                TransformationRules,
                Priority
            FROM dbo.TableConfiguration
            WHERE LoadGroup = @LoadGroup
              AND IsActive = 1
            ORDER BY Priority", connection);
        
        command.Parameters.AddWithValue("@LoadGroup", loadGroup);
        
        using (var reader = await command.ExecuteReaderAsync())
        {
            while (await reader.ReadAsync())
            {
                var config = new LoadConfig
                {
                    TableConfigID = reader.GetInt32(0),
                    SourceServer = reader.GetString(1),
                    SourceDatabase = reader.GetString(2),
                    SourceSchema = reader.GetString(3),
                    SourceTable = reader.GetString(4),
                    SourceQuery = reader.IsDBNull(5) ? null : reader.GetString(5),
                    TargetSchema = reader.GetString(6),
                    TargetTable = reader.GetString(7),
                    LoadType = reader.GetString(8),
                    WatermarkColumn = reader.IsDBNull(9) ? null : reader.GetString(9),
                    WatermarkValue = reader.IsDBNull(10) ? null : reader.GetString(10),
                    PartitionColumn = reader.IsDBNull(11) ? null : reader.GetString(11),
                    ValidationRules = reader.IsDBNull(13) ? null : reader.GetString(13),
                    TransformationRules = reader.IsDBNull(14) ? null : reader.GetString(14)
                };
                
                configurations.Add(config);
            }
        }
    }
    
    log.LogInformation($"Retrieved {configurations.Count} table configurations for load group: {loadGroup}");
    
    return new OkObjectResult(new
    {
        loadGroup = loadGroup,
        runDate = runDate,
        tableCount = configurations.Count,
        tables = configurations
    });
}
```

**Step 2: ADF Pipeline - Master Load Pipeline**

```json
{
  "name": "PL_MasterLoad_MetadataDriven",
  "activities": [
    {
      "name": "Get_Load_Configuration",
      "type": "AzureFunctionActivity",
      "typeProperties": {
        "functionName": "GetLoadConfiguration",
        "method": "POST",
        "headers": {
          "Content-Type": "application/json"
        },
        "body": {
          "loadGroup": "@pipeline().parameters.LoadGroup",
          "runDate": "@formatDateTime(utcnow(), 'yyyy-MM-dd HH:mm:ss')"
        }
      },
      "linkedServiceName": {"referenceName": "LS_AzureFunction"}
    },
    {
      "name": "ForEach_Table",
      "type": "ForEach",
      "dependsOn": [{"activity": "Get_Load_Configuration", "dependencyConditions": ["Succeeded"]}],
      "typeProperties": {
        "items": {
          "value": "@activity('Get_Load_Configuration').output.tables",
          "type": "Expression"
        },
        "isSequential": false,
        "batchCount": 10,  // Process 10 tables in parallel
        "activities": [
          {
            "name": "Generate_Source_Query",
            "type": "AzureFunctionActivity",
            "typeProperties": {
              "functionName": "GenerateSourceQuery",
              "method": "POST",
              "body": {
                "tableConfig": "@item()",
                "runDate": "@pipeline().parameters.RunDate"
              }
            },
            "linkedServiceName": {"referenceName": "LS_AzureFunction"}
          },
          {
            "name": "Load_Table_Data",
            "type": "Copy",
            "dependsOn": [{"activity": "Generate_Source_Query", "dependencyConditions": ["Succeeded"]}],
            "inputs": [{"referenceName": "DS_Dynamic_Source"}],
            "outputs": [{"referenceName": "DS_Dynamic_Target"}],
            "typeProperties": {
              "source": {
                "type": "SqlSource",
                "sqlReaderQuery": "@{activity('Generate_Source_Query').output.sourceQuery}",
                "queryTimeout": "02:00:00"
              },
              "sink": {
                "type": "SqlSink",
                "preCopyScript": "@{activity('Generate_Source_Query').output.preCopyScript}",
                "writeBatchSize": 10000,
                "writeBatchTimeout": "00:30:00"
              },
              "enableStaging": false,
              "translator": "@{activity('Generate_Source_Query').output.columnMapping}"
            }
          },
          {
            "name": "Validate_And_Log",
            "type": "AzureFunctionActivity",
            "dependsOn": [{"activity": "Load_Table_Data", "dependencyConditions": ["Succeeded"]}],
            "typeProperties": {
              "functionName": "ValidateAndLog",
              "method": "POST",
              "body": {
                "tableConfig": "@item()",
                "rowsRead": "@activity('Load_Table_Data').output.rowsRead",
                "rowsCopied": "@activity('Load_Table_Data').output.rowsCopied",
                "copyDuration": "@activity('Load_Table_Data').output.copyDuration",
                "runId": "@pipeline().RunId"
              }
            },
            "linkedServiceName": {"referenceName": "LS_AzureFunction"}
          }
        ]
      }
    }
  ],
  "parameters": {
    "LoadGroup": {
      "type": "string",
      "defaultValue": "DailyLoad"
    },
    "RunDate": {
      "type": "string",
      "defaultValue": "@formatDateTime(utcnow(), 'yyyy-MM-dd')"
    }
  }
}
```

**Step 3: Azure Function - Generate Source Query**

```csharp
[FunctionName("GenerateSourceQuery")]
public static async Task<IActionResult> Run(
    [HttpTrigger(AuthorizationLevel.Function, "post", Route = null)] HttpRequest req,
    ILogger log)
{
    string requestBody = await new StreamReader(req.Body).ReadToEndAsync();
    dynamic data = JsonConvert.DeserializeObject(requestBody);
    
    var tableConfig = data?.tableConfig;
    string loadType = tableConfig?.LoadType;
    string sourceTable = tableConfig?.SourceTable;
    string watermarkColumn = tableConfig?.WatermarkColumn;
    string watermarkValue = tableConfig?.WatermarkValue;
    DateTime runDate = data?.runDate ?? DateTime.UtcNow;
    
    string sourceQuery = "";
    string preCopyScript = "";
    string columnMapping = "";
    
    // Generate query based on load type
    switch (loadType)
    {
        case "Full":
            sourceQuery = $"SELECT * FROM {tableConfig.SourceSchema}.{sourceTable}";
            preCopyScript = $"TRUNCATE TABLE {tableConfig.TargetSchema}.{tableConfig.TargetTable}";
            break;
            
        case "Incremental":
            if (string.IsNullOrEmpty(watermarkColumn))
            {
                return new BadRequestObjectResult("Incremental load requires WatermarkColumn");
            }
            sourceQuery = $@"
                SELECT * 
                FROM {tableConfig.SourceSchema}.{sourceTable}
                WHERE {watermarkColumn} > '{watermarkValue}'
                ORDER BY {watermarkColumn}";
            preCopyScript = "";  // No truncate for incremental
            break;
            
        case "CDC":
            sourceQuery = $@"
                SELECT * 
                FROM cdc.fn_cdc_get_all_changes_{sourceTable}(
                    sys.fn_cdc_map_time_to_lsn('smallest greater than', '{watermarkValue}'),
                    sys.fn_cdc_map_time_to_lsn('largest less than or equal', GETDATE()),
                    'all'
                )";
            break;
            
        case "Partition":
            string partitionColumn = tableConfig?.PartitionColumn;
            sourceQuery = $@"
                SELECT * 
                FROM {tableConfig.SourceSchema}.{sourceTable}
                WHERE {partitionColumn} = '{runDate:yyyy-MM-dd}'";
            break;
            
        default:
            return new BadRequestObjectResult($"Unknown load type: {loadType}");
    }
    
    // Apply transformation rules if specified
    if (!string.IsNullOrEmpty(tableConfig?.TransformationRules?.ToString()))
    {
        var transformRules = JsonConvert.DeserializeObject<dynamic>(tableConfig.TransformationRules.ToString());
        
        // Example: Mask SSN column
        if (transformRules?.maskColumns != null)
        {
            foreach (var column in transformRules.maskColumns)
            {
                sourceQuery = sourceQuery.Replace(
                    $"SELECT *", 
                    $"SELECT *, CASE WHEN {column} IS NOT NULL THEN 'XXX-XX-' + RIGHT({column}, 4) ELSE NULL END AS {column}_Masked");
            }
        }
    }
    
    // Generate column mapping if needed
    columnMapping = GenerateColumnMapping(tableConfig);
    
    log.LogInformation($"Generated source query for table: {sourceTable}, Load Type: {loadType}");
    
    return new OkObjectResult(new
    {
        sourceQuery = sourceQuery,
        preCopyScript = preCopyScript,
        columnMapping = columnMapping,
        loadType = loadType
    });
}

private static string GenerateColumnMapping(dynamic tableConfig)
{
    // Simplified example - in real implementation, query INFORMATION_SCHEMA
    var mapping = new
    {
        type = "TabularTranslator",
        mappings = new[]
        {
            new { source = new { name = "CustomerID" }, sink = new { name = "CustomerID" } },
            new { source = new { name = "FirstName" }, sink = new { name = "FirstName" } },
            new { source = new { name = "LastName" }, sink = new { name = "LastName" } }
        }
    };
    
    return JsonConvert.SerializeObject(mapping);
}
```

**Step 4: Azure Function - Validate And Log**

```csharp
[FunctionName("ValidateAndLog")]
public static async Task<IActionResult> Run(
    [HttpTrigger(AuthorizationLevel.Function, "post", Route = null)] HttpRequest req,
    ILogger log)
{
    string requestBody = await new StreamReader(req.Body).ReadToEndAsync();
    dynamic data = JsonConvert.DeserializeObject(requestBody);
    
    var tableConfig = data?.tableConfig;
    int rowsRead = data?.rowsRead ?? 0;
    int rowsCopied = data?.rowsCopied ?? 0;
    int copyDuration = data?.copyDuration ?? 0;
    string runId = data?.runId;
    
    // Apply validation rules
    bool isValid = true;
    var validationErrors = new List<string>();
    
    if (!string.IsNullOrEmpty(tableConfig?.ValidationRules?.ToString()))
    {
        var validationRules = JsonConvert.DeserializeObject<dynamic>(tableConfig.ValidationRules.ToString());
        
        // Example rule: Minimum row count
        if (validationRules?.minRowCount != null)
        {
            int minRowCount = validationRules.minRowCount;
            if (rowsCopied < minRowCount)
            {
                isValid = false;
                validationErrors.Add($"Row count {rowsCopied} is below minimum threshold {minRowCount}");
            }
        }
        
        // Example rule: Row count delta
        if (validationRules?.maxRowDelta != null)
        {
            int maxDelta = validationRules.maxRowDelta;
            int previousRowCount = await GetPreviousRowCount(tableConfig.TableConfigID);
            int delta = Math.Abs(rowsCopied - previousRowCount);
            
            if (delta > maxDelta)
            {
                isValid = false;
                validationErrors.Add($"Row count delta {delta} exceeds maximum threshold {maxDelta}");
            }
        }
    }
    
    // Log to database
    var connectionString = Environment.GetEnvironmentVariable("ConfigDbConnectionString");
    using (var connection = new SqlConnection(connectionString))
    {
        await connection.OpenAsync();
        var command = new SqlCommand(@"
            INSERT INTO dbo.LoadExecutionLog (
                RunId, TableConfigID, SourceTable, TargetTable,
                RowsRead, RowsCopied, CopyDuration, IsValid, ValidationErrors,
                ExecutionDate
            ) VALUES (
                @RunId, @TableConfigID, @SourceTable, @TargetTable,
                @RowsRead, @RowsCopied, @CopyDuration, @IsValid, @ValidationErrors,
                GETDATE()
            )", connection);
        
        command.Parameters.AddWithValue("@RunId", runId);
        command.Parameters.AddWithValue("@TableConfigID", tableConfig.TableConfigID);
        command.Parameters.AddWithValue("@SourceTable", $"{tableConfig.SourceSchema}.{tableConfig.SourceTable}");
        command.Parameters.AddWithValue("@TargetTable", $"{tableConfig.TargetSchema}.{tableConfig.TargetTable}");
        command.Parameters.AddWithValue("@RowsRead", rowsRead);
        command.Parameters.AddWithValue("@RowsCopied", rowsCopied);
        command.Parameters.AddWithValue("@CopyDuration", copyDuration);
        command.Parameters.AddWithValue("@IsValid", isValid);
        command.Parameters.AddWithValue("@ValidationErrors", JsonConvert.SerializeObject(validationErrors));
        
        await command.ExecuteNonQueryAsync();
    }
    
    // Update watermark if incremental load
    if (tableConfig?.LoadType == "Incremental" && isValid)
    {
        await UpdateWatermark(tableConfig.TableConfigID, DateTime.UtcNow);
    }
    
    log.LogInformation($"Validation complete for table: {tableConfig.SourceTable}, Valid: {isValid}");
    
    return new OkObjectResult(new
    {
        isValid = isValid,
        validationErrors = validationErrors,
        rowsCopied = rowsCopied
    });
}

private static async Task<int> GetPreviousRowCount(int tableConfigId)
{
    // Implementation to query previous execution log
    return 0;  // Placeholder
}

private static async Task UpdateWatermark(int tableConfigId, DateTime newWatermark)
{
    // Implementation to update watermark in configuration table
}
```

#### Azure Function Linked Service Configuration

```json
{
  "name": "LS_AzureFunction",
  "type": "AzureFunctionLinkedService",
  "typeProperties": {
    "functionAppUrl": "https://adf-helper-functions.azurewebsites.net",
    "authentication": "MSI",  // Managed Identity
    "resourceId": "/subscriptions/{sub-id}/resourceGroups/{rg}/providers/Microsoft.Web/sites/adf-helper-functions"
  }
}
```

**Pro Tip**: For Big 4 projects, **ALWAYS use Managed Identity** for Azure Function authentication. Never use function keys in production.

#### Real-World Challenges

**Challenge 1: Azure Function Cold Start**

**The Problem**: "First execution of Azure Function took 8 seconds due to cold start, causing pipeline delays."

**Your Solution**:
- "Enabled 'Always On' for Function App (requires Premium or App Service Plan)"
- "Implemented function warmup trigger that runs every 5 minutes"
- "For critical paths, used Durable Functions with pre-warmed instances"
- "Result: Cold start 8 seconds → Warm start 300ms"

**Challenge 2: Function Timeout**

**The Problem**: "Complex validation logic was timing out after 5 minutes (consumption plan limit)."

**Your Solution**:
- "Moved to Premium Plan with 60-minute timeout"
- "Refactored validation: simple checks in Function, complex checks in async Service Bus pattern"
- "Implemented pagination: validate in batches, return partial results"

**Challenge 3: Handling Transient Failures**

**The Problem**: "Occasional network blips caused Function calls to fail, failing entire pipeline."

**Your Solution in ADF**:
```json
{
  "name": "Call_Azure_Function",
  "type": "AzureFunctionActivity",
  "typeProperties": { "..." },
  "policy": {
    "timeout": "0.00:10:00",
    "retry": 3,
    "retryIntervalInSeconds": 30,
    "secureOutput": false,
    "secureInput": false
  }
}
```

**Your Solution in Azure Function**:
```csharp
// Implement Polly for retry logic inside function
var retryPolicy = Policy
    .Handle<SqlException>()
    .Or<HttpRequestException>()
    .WaitAndRetryAsync(
        retryCount: 3,
        sleepDurationProvider: retryAttempt => TimeSpan.FromSeconds(Math.Pow(2, retryAttempt)),
        onRetry: (exception, timeSpan, retryCount, context) =>
        {
            log.LogWarning($"Retry {retryCount} after {timeSpan.TotalSeconds} seconds due to {exception.Message}");
        });

await retryPolicy.ExecuteAsync(async () =>
{
    // Your code here
});
```

---

### Scenario 2: OAuth2 API Integration

#### Business Requirement

"Integrate with Salesforce Marketing Cloud API which requires:
- OAuth2 authentication (token expires every 20 minutes)
- Rate limiting (100 requests per minute)
- Pagination (1000 records per page, max 10,000 pages)
- Retry logic with exponential backoff"

**Why Azure Functions**: ADF Web Activity can't handle OAuth2 token refresh, rate limiting, or complex pagination automatically.

#### Implementation

**Azure Function - Salesforce Integration**:

```csharp
[FunctionName("GetSalesforceData")]
public static async Task<IActionResult> Run(
    [HttpTrigger(AuthorizationLevel.Function, "post", Route = null)] HttpRequest req,
    ILogger log)
{
    string requestBody = await new StreamReader(req.Body).ReadToEndAsync();
    dynamic data = JsonConvert.DeserializeObject(requestBody);
    
    string objectType = data?.objectType;  // e.g., "Contact", "Lead"
    DateTime sinceDate = data?.sinceDate ?? DateTime.UtcNow.AddDays(-1);
    
    // Get OAuth2 token (with caching)
    string accessToken = await GetAccessTokenAsync(log);
    
    var allRecords = new List<dynamic>();
    int pageNumber = 1;
    bool hasMorePages = true;
    
    using (var httpClient = new HttpClient())
    {
        httpClient.DefaultRequestHeaders.Authorization = 
            new System.Net.Http.Headers.AuthenticationHeaderValue("Bearer", accessToken);
        
        while (hasMorePages && pageNumber <= 10000)
        {
            // Rate limiting: max 100 requests per minute
            await RateLimiter.WaitAsync(log);
            
            string apiUrl = $"https://api.salesforce.com/services/data/v55.0/query?" +
                           $"q=SELECT Id,Name,Email,CreatedDate FROM {objectType} " +
                           $"WHERE CreatedDate > {sinceDate:yyyy-MM-ddTHH:mm:ssZ} " +
                           $"LIMIT 1000 OFFSET {(pageNumber - 1) * 1000}";
            
            try
            {
                var response = await httpClient.GetAsync(apiUrl);
                
                if (response.StatusCode == System.Net.HttpStatusCode.Unauthorized)
                {
                    // Token expired, refresh and retry
                    accessToken = await GetAccessTokenAsync(log, forceRefresh: true);
                    httpClient.DefaultRequestHeaders.Authorization = 
                        new System.Net.Http.Headers.AuthenticationHeaderValue("Bearer", accessToken);
                    response = await httpClient.GetAsync(apiUrl);
                }
                
                response.EnsureSuccessStatusCode();
                
                string responseBody = await response.Content.ReadAsStringAsync();
                dynamic result = JsonConvert.DeserializeObject(responseBody);
                
                if (result?.records != null)
                {
                    foreach (var record in result.records)
                    {
                        allRecords.Add(record);
                    }
                    
                    hasMorePages = result.records.Count == 1000;
                    pageNumber++;
                    
                    log.LogInformation($"Retrieved page {pageNumber}, Total records: {allRecords.Count}");
                }
                else
                {
                    hasMorePages = false;
                }
            }
            catch (Exception ex)
            {
                log.LogError($"Error on page {pageNumber}: {ex.Message}");
                throw;
            }
        }
    }
    
    // Write to ADLS Gen2
    string outputPath = await WriteToDataLake(allRecords, objectType, sinceDate, log);
    
    return new OkObjectResult(new
    {
        recordCount = allRecords.Count,
        pageCount = pageNumber - 1,
        outputPath = outputPath
    });
}

private static async Task<string> GetAccessTokenAsync(ILogger log, bool forceRefresh = false)
{
    // Check cache first (using IMemoryCache or Redis)
    string cacheKey = "salesforce_access_token";
    
    if (!forceRefresh && TokenCache.TryGetValue(cacheKey, out string cachedToken))
    {
        return cachedToken;
    }
    
    // Get credentials from Key Vault
    var kvClient = new SecretClient(
        new Uri(Environment.GetEnvironmentVariable("KeyVaultUrl")),
        new DefaultAzureCredential());
    
    var clientId = (await kvClient.GetSecretAsync("Salesforce-ClientId")).Value.Value;
    var clientSecret = (await kvClient.GetSecretAsync("Salesforce-ClientSecret")).Value.Value;
    
    // Request new token
    using (var httpClient = new HttpClient())
    {
        var tokenRequest = new FormUrlEncodedContent(new[]
        {
            new KeyValuePair<string, string>("grant_type", "client_credentials"),
            new KeyValuePair<string, string>("client_id", clientId),
            new KeyValuePair<string, string>("client_secret", clientSecret)
        });
        
        var response = await httpClient.PostAsync(
            "https://login.salesforce.com/services/oauth2/token", 
            tokenRequest);
        
        response.EnsureSuccessStatusCode();
        
        string responseBody = await response.Content.ReadAsStringAsync();
        dynamic tokenResponse = JsonConvert.DeserializeObject(responseBody);
        
        string accessToken = tokenResponse.access_token;
        
        // Cache for 18 minutes (tokens valid for 20 minutes)
        TokenCache.Set(cacheKey, accessToken, TimeSpan.FromMinutes(18));
        
        log.LogInformation("Successfully obtained new access token");
        
        return accessToken;
    }
}

private static class RateLimiter
{
    private static SemaphoreSlim _semaphore = new SemaphoreSlim(100, 100);  // 100 concurrent requests
    private static List<DateTime> _requestTimes = new List<DateTime>();
    
    public static async Task WaitAsync(ILogger log)
    {
        await _semaphore.WaitAsync();
        
        try
        {
            var now = DateTime.UtcNow;
            var oneMinuteAgo = now.AddMinutes(-1);
            
            // Remove requests older than 1 minute
            _requestTimes.RemoveAll(t => t < oneMinuteAgo);
            
            // If we've made 100 requests in the last minute, wait
            if (_requestTimes.Count >= 100)
            {
                var oldestRequest = _requestTimes.Min();
                var waitTime = oldestRequest.AddMinutes(1) - now;
                
                if (waitTime > TimeSpan.Zero)
                {
                    log.LogInformation($"Rate limit reached, waiting {waitTime.TotalSeconds} seconds");
                    await Task.Delay(waitTime);
                }
            }
            
            _requestTimes.Add(now);
        }
        finally
        {
            _semaphore.Release();
        }
    }
}

private static async Task<string> WriteToDataLake(
    List<dynamic> records, 
    string objectType, 
    DateTime sinceDate, 
    ILogger log)
{
    var dataLakeServiceClient = new DataLakeServiceClient(
        new Uri(Environment.GetEnvironmentVariable("DataLakeUrl")),
        new DefaultAzureCredential());
    
    var fileSystemClient = dataLakeServiceClient.GetFileSystemClient("bronze");
    
    string fileName = $"salesforce/{objectType}/{sinceDate:yyyy-MM-dd}/{Guid.NewGuid()}.json";
    var fileClient = fileSystemClient.GetFileClient(fileName);
    
    string jsonContent = JsonConvert.SerializeObject(records, Formatting.Indented);
    byte[] byteArray = Encoding.UTF8.GetBytes(jsonContent);
    
    using (var stream = new MemoryStream(byteArray))
    {
        await fileClient.UploadAsync(stream, overwrite: true);
    }
    
    log.LogInformation($"Wrote {records.Count} records to {fileName}");
    
    return fileName;
}
```

**ADF Pipeline Using This Function**:

```json
{
  "name": "PL_Ingest_Salesforce",
  "activities": [
    {
      "name": "Get_Salesforce_Data",
      "type": "AzureFunctionActivity",
      "typeProperties": {
        "functionName": "GetSalesforceData",
        "method": "POST",
        "body": {
          "objectType": "@pipeline().parameters.ObjectType",
          "sinceDate": "@formatDateTime(addDays(utcnow(), -1), 'yyyy-MM-ddTHH:mm:ssZ')"
        }
      },
      "linkedServiceName": {"referenceName": "LS_AzureFunction"},
      "policy": {
        "timeout": "0.02:00:00",  // 2 hours for large datasets
        "retry": 2,
        "retryIntervalInSeconds": 300
      }
    },
    {
      "name": "Copy_To_Synapse",
      "type": "Copy",
      "dependsOn": [{"activity": "Get_Salesforce_Data", "dependencyConditions": ["Succeeded"]}],
      "inputs": [{"referenceName": "DS_ADLS_Salesforce_JSON"}],
      "outputs": [{"referenceName": "DS_Synapse_Salesforce"}],
      "typeProperties": {
        "source": {
          "type": "JsonSource",
          "storeSettings": {
            "type": "AzureBlobFSReadSettings",
            "recursive": true,
            "wildcardFileName": "@{activity('Get_Salesforce_Data').output.outputPath}"
          }
        },
        "sink": {
          "type": "SqlDWSink"
        }
      }
    }
  ]
}
```

#### Interview Questions for ADF + Azure Functions

**Q: "When should you use Azure Functions vs. Web Activity in ADF?"**

✅ **Your Answer**:

**Use Web Activity when**:
- Simple REST API with basic authentication
- No complex logic needed
- Single request-response pattern
- Response < 4MB (ADF limit)
- Timeout < 7 days

**Use Azure Functions when**:
- OAuth2 or complex authentication
- Rate limiting required
- Pagination with unknown page count
- Custom retry logic needed
- Response processing (parsing, transformation)
- Need to write to multiple destinations
- Business logic too complex for ADF expressions

**In my last project**: "Used Web Activity for 70% of API calls (simple REST), Azure Functions for 30% (Salesforce OAuth2, complex pagination, rate limiting)."

**Q: "How do you handle Azure Function failures in ADF pipelines?"**

✅ **Your Answer**: "Defense-in-depth approach:

1. **Inside Function**:
   - Try-catch blocks for all external calls
   - Polly retry policy for transient failures
   - Structured logging to Application Insights
   - Return meaningful error codes

2. **In ADF**:
   - Retry policy: 3 attempts, exponential backoff
   - Timeout appropriate to function complexity
   - If Condition to check function output status
   - Execute Pipeline activity to call error handling pipeline

3. **Monitoring**:
   - Azure Monitor alerts on Function failures
   - Application Insights queries for error patterns
   - Custom metrics: success rate, duration percentiles

4. **Recovery**:
   - Idempotent functions (can retry safely)
   - Checkpoint pattern: save progress, resume on retry
   - Dead letter queue for failed messages"

---

## 5. ADF + Event Grid Integration

### Why This Integration Exists

**Layman Explanation**: Traditional pipelines are like scheduled buses—they run whether passengers (data) are waiting or not. Event Grid is like Uber—pipelines run only when data arrives. This saves cost and reduces latency.

**Technical Reality**: Event Grid is Azure's event routing service. It enables event-driven architecture where actions happen in response to events:
- File uploaded to Blob Storage → Trigger ADF pipeline
- Message arrives in Service Bus → Process data
- Custom application event → Orchestrate workflow

### Common Use Cases in Big 4

1. **Real-Time File Processing**: Process files immediately after upload
2. **Event-Driven ETL**: Load data only when source system signals readiness
3. **Cross-System Integration**: React to events from external systems
4. **Microservices Orchestration**: Coordinate multiple services

---

### Scenario 1: Event-Driven File Processing

#### Business Requirement

"E-commerce company receives order files from 50 vendors throughout the day via SFTP. Files arrive unpredictably (10 AM, 3 PM, midnight). Current solution polls every 15 minutes, wasting compute. Need immediate processing when files arrive."

#### Architecture

```
[Vendor SFTP] → [Azure Logic App: SFTP Connector]
    ↓
[Upload to ADLS Gen2: /landing/vendor_{ID}/]
    ↓
[Event Grid: BlobCreated Event]
    ↓
[ADF Event-Based Trigger]
    ↓
[ADF Pipeline: Validate → Transform → Load]
```

#### Step-by-Step Implementation

**Step 1: Event Grid System Topic (For Blob Storage)**

```bash
# Azure CLI to create Event Grid System Topic
az eventgrid system-topic create \
  --name eg-adls-events \
  --resource-group rg-data-platform \
  --source /subscriptions/{sub-id}/resourceGroups/rg-data-platform/providers/Microsoft.Storage/storageAccounts/adlsgen2account \
  --topic-type Microsoft.Storage.StorageAccounts \
  --location eastus
```

**Step 2: ADF Event-Based Trigger**

```json
{
  "name": "ET_BlobCreated_OrderFiles",
  "properties": {
    "type": "BlobEventsTrigger",
    "typeProperties": {
      "blobPathBeginsWith": "/landing/blob/vendor_",
      "blobPathEndsWith": ".csv",
      "ignoreEmptyBlobs": true,
      "scope": "/subscriptions/{sub-id}/resourceGroups/rg-data-platform/providers/Microsoft.Storage/storageAccounts/adlsgen2account",
      "events": [
        "Microsoft.Storage.BlobCreated"
      ]
    },
    "pipelines": [
      {
        "pipelineReference": {
          "referenceName": "PL_Process_Vendor_Order_File",
          "type": "PipelineReference"
        },
        "parameters": {
          "fileName": "@trigger().outputs.body.data.url",
          "fileSize": "@trigger().outputs.body.data.contentLength",
          "eventTime": "@trigger().outputs.body.eventTime"
        }
      }
    ]
  }
}
```

**Step 3: ADF Pipeline - Event-Driven Processing**

```json
{
  "name": "PL_Process_Vendor_Order_File",
  "activities": [
    {
      "name": "Parse_File_Metadata",
      "type": "SetVariable",
      "typeProperties": {
        "variableName": "VendorID",
        "value": "@{split(split(pipeline().parameters.fileName, '/')[3], '_')[1]}"
      }
    },
    {
      "name": "Validate_File_Size",
      "type": "IfCondition",
      "dependsOn": [{"activity": "Parse_File_Metadata", "dependencyConditions": ["Succeeded"]}],
      "typeProperties": {
        "expression": {
          "value": "@greater(pipeline().parameters.fileSize, 0)",
          "type": "Expression"
        },
        "ifTrueActivities": [
          {
            "name": "Get_Vendor_Configuration",
            "type": "Lookup",
            "typeProperties": {
              "source": {
                "type": "SqlSource",
                "sqlReaderQuery": "SELECT * FROM dbo.VendorConfiguration WHERE VendorID = '@{variables('VendorID')}'"
              },
              "dataset": {"referenceName": "DS_AzureSQL_Config"}
            }
          },
          {
            "name": "Copy_To_Staging",
            "type": "Copy",
            "dependsOn": [{"activity": "Get_Vendor_Configuration", "dependencyConditions": ["Succeeded"]}],
            "inputs": [{"referenceName": "DS_ADLS_Landing"}],
            "outputs": [{"referenceName": "DS_ADLS_Staging"}],
            "typeProperties": {
              "source": {
                "type": "DelimitedTextSource",
                "storeSettings": {
                  "type": "AzureBlobFSReadSettings",
                  "recursive": false,
                  "wildcardFileName": "@{pipeline().parameters.fileName}"
                }
              },
              "sink": {
                "type": "ParquetSink"
              }
            }
          },
          {
            "name": "Validate_Schema",
            "type": "AzureFunctionActivity",
            "dependsOn": [{"activity": "Copy_To_Staging", "dependencyConditions": ["Succeeded"]}],
            "typeProperties": {
              "functionName": "ValidateFileSchema",
              "method": "POST",
              "body": {
                "fileName": "@pipeline().parameters.fileName",
                "vendorId": "@variables('VendorID')",
                "expectedSchema": "@activity('Get_Vendor_Configuration').output.firstRow.ExpectedSchema"
              }
            }
          },
          {
            "name": "Process_If_Valid",
            "type": "IfCondition",
            "dependsOn": [{"activity": "Validate_Schema", "dependencyConditions": ["Succeeded"]}],
            "typeProperties": {
              "expression": {
                "value": "@activity('Validate_Schema').output.isValid",
                "type": "Expression"
              },
              "ifTrueActivities": [
                {
                  "name": "Transform_And_Load",
                  "type": "ExecutePipeline",
                  "typeProperties": {
                    "pipeline": {"referenceName": "PL_Transform_Vendor_Orders"},
                    "parameters": {
                      "fileName": "@pipeline().parameters.fileName",
                      "vendorId": "@variables('VendorID')"
                    },
                    "waitOnCompletion": true
                  }
                },
                {
                  "name": "Move_To_Processed",
                  "type": "Copy",
                  "dependsOn": [{"activity": "Transform_And_Load", "dependencyConditions": ["Succeeded"]}],
                  "inputs": [{"referenceName": "DS_ADLS_Landing"}],
                  "outputs": [{"referenceName": "DS_ADLS_Processed"}],
                  "typeProperties": {
                    "source": {
                      "type": "DelimitedTextSource",
                      "storeSettings": {
                        "type": "AzureBlobFSReadSettings",
                        "recursive": false,
                        "wildcardFileName": "@{pipeline().parameters.fileName}"
                      }
                    },
                    "sink": {
                      "type": "DelimitedTextSink",
                      "storeSettings": {
                        "type": "AzureBlobFSWriteSettings",
                        "copyBehavior": "PreserveHierarchy"
                      }
                    }
                  }
                }
              ],
              "ifFalseActivities": [
                {
                  "name": "Move_To_Quarantine",
                  "type": "Copy",
                  "inputs": [{"referenceName": "DS_ADLS_Landing"}],
                  "outputs": [{"referenceName": "DS_ADLS_Quarantine"}],
                  "typeProperties": {
                    "source": {
                      "type": "DelimitedTextSource"
                    },
                    "sink": {
                      "type": "DelimitedTextSink"
                    }
                  }
                },
                {
                  "name": "Send_Validation_Failure_Alert",
                  "type": "WebActivity",
                  "dependsOn": [{"activity": "Move_To_Quarantine", "dependencyConditions": ["Succeeded"]}],
                  "typeProperties": {
                    "url": "@pipeline().parameters.LogicAppUrl",
                    "method": "POST",
                    "body": {
                      "alertType": "ValidationFailure",
                      "fileName": "@pipeline().parameters.fileName",
                      "vendorId": "@variables('VendorID')",
                      "errors": "@activity('Validate_Schema').output.errors"
                    }
                  }
                }
              ]
            }
          }
        ],
        "ifFalseActivities": [
          {
            "name": "Log_Empty_File",
            "type": "SqlServerStoredProcedure",
            "typeProperties": {
              "storedProcedureName": "[dbo].[usp_LogEmptyFile]",
              "storedProcedureParameters": {
                "FileName": {"value": "@pipeline().parameters.fileName"},
                "EventTime": {"value": "@pipeline().parameters.eventTime"}
              }
            },
            "linkedServiceName": {"referenceName": "LS_AzureSQL_Config"}
          }
        ]
      }
    }
  ],
  "parameters": {
    "fileName": {"type": "string"},
    "fileSize": {"type": "int"},
    "eventTime": {"type": "string"}
  },
  "variables": {
    "VendorID": {"type": "String"}
  }
}
```

#### Event Grid Trigger Configuration Details

**Key Properties Explained**:

1. **blobPathBeginsWith**: Filter events to specific container/folder
   - `/landing/blob/vendor_` matches only files in vendor folders
   - Reduces unnecessary pipeline triggers

2. **blobPathEndsWith**: Filter by file extension
   - `.csv` ensures only CSV files trigger pipeline
   - Prevents triggering on metadata files, logs, etc.

3. **ignoreEmptyBlobs**: Skip empty files
   - Prevents wasted pipeline runs
   - Empty files often indicate incomplete uploads

4. **events**: Which blob events to monitor
   - `Microsoft.Storage.BlobCreated`: New file uploaded
   - `Microsoft.Storage.BlobDeleted`: File deleted (if needed)
   - `Microsoft.Storage.BlobRenamed`: File renamed (ADLS Gen2 only)

#### Real-World Challenges

**Challenge 1: Duplicate Event Triggers**

**The Problem**: "Event Grid was triggering pipeline twice for the same file upload, causing duplicate data loads."

**Root Cause**: 
- "Large files uploaded in chunks trigger multiple BlobCreated events"
- "SFTP connector creates temp file, then renames—two events"

**Your Solution**:
1. **Idempotent Pipeline Design**:
   ```json
   {
     "name": "Check_If_Already_Processed",
     "type": "Lookup",
     "typeProperties": {
       "source": {
         "type": "SqlSource",
         "sqlReaderQuery": "SELECT COUNT(*) AS ProcessedCount FROM dbo.ProcessedFiles WHERE FileName = '@{pipeline().parameters.fileName}' AND ProcessedDate > DATEADD(hour, -1, GETDATE())"
       }
     }
   }
   ```

2. **If Condition to Skip Duplicates**:
   ```json
   {
     "name": "Process_If_Not_Duplicate",
     "type": "IfCondition",
     "typeProperties": {
       "expression": {
         "value": "@equals(activity('Check_If_Already_Processed').output.firstRow.ProcessedCount, 0)",
         "type": "Expression"
       }
     }
   }
   ```

3. **Event Grid Advanced Filtering**:
   ```json
   {
     "advancedFilters": [
       {
         "operatorType": "NumberGreaterThan",
         "key": "data.contentLength",
         "value": 0
       },
       {
         "operatorType": "StringNotContains",
         "key": "data.url",
         "values": [".tmp", ".temp", "~"]
       }
     ]
   }
   ```

**Result**: Duplicate processing eliminated, 100% data accuracy

**Challenge 2: Event Delivery Delays**

**The Problem**: "Files uploaded at 9:00 AM, but Event Grid triggered pipeline at 9:05 AM—5 minute delay unacceptable for real-time requirements."

**Root Cause**:
- "Event Grid has eventual consistency, typically <1 second but can spike"
- "Network latency between storage account and Event Grid"

**Your Solution**:
1. **For True Real-Time (<1 second)**:
   - "Switched to Azure Functions with Blob Trigger instead of Event Grid"
   - "Function processes file directly, then calls ADF via REST API"
   
   ```csharp
   [FunctionName("BlobTriggerFunction")]
   public static async Task Run(
       [BlobTrigger("landing/vendor_{vendorId}/{name}", Connection = "AzureWebJobsStorage")] Stream myBlob,
       string name,
       string vendorId,
       ILogger log)
   {
       log.LogInformation($"Blob trigger processed: {name}, Size: {myBlob.Length} bytes");
       
       // Call ADF REST API to trigger pipeline
       var adfClient = new DataFactoryManagementClient(new DefaultAzureCredential())
       {
           SubscriptionId = Environment.GetEnvironmentVariable("SubscriptionId")
       };
       
       var parameters = new Dictionary<string, object>
       {
           { "fileName", name },
           { "vendorId", vendorId },
           { "fileSize", myBlob.Length }
       };
       
       await adfClient.Pipelines.CreateRunAsync(
           resourceGroupName: "rg-data-platform",
           factoryName: "adf-prod",
           pipelineName: "PL_Process_Vendor_Order_File",
           parameters: parameters);
   }
   ```

2. **For Near Real-Time (1-5 seconds acceptable)**:
   - "Kept Event Grid but added monitoring"
   - "Azure Monitor alert if event-to-execution latency > 10 seconds"

**Challenge 3: Event Grid Throttling**

**The Problem**: "During peak hours, 1000+ files uploaded per minute. Event Grid throttled, some events lost."

**Event Grid Limits**:
- 5,000 events per second per topic (standard tier)
- 1,000 events per second per subscription

**Your Solution**:
1. **Batching Strategy**:
   ```json
   {
     "name": "ET_BlobCreated_Batched",
     "properties": {
       "type": "BlobEventsTrigger",
       "typeProperties": {
         "blobPathBeginsWith": "/landing/blob/vendor_",
         "blobPathEndsWith": ".csv",
         "scope": "...",
         "events": ["Microsoft.Storage.BlobCreated"]
       },
       "pipelines": [
         {
           "pipelineReference": {
             "referenceName": "PL_Process_Batch",
             "type": "PipelineReference"
           }
         }
       ]
     }
   }
   ```

2. **Pipeline Processes Multiple Files**:
   ```json
   {
     "name": "PL_Process_Batch",
     "activities": [
       {
         "name": "Get_All_Pending_Files",
         "type": "GetMetadata",
         "typeProperties": {
           "dataset": {"referenceName": "DS_ADLS_Landing"},
           "fieldList": ["childItems"],
           "storeSettings": {
             "type": "AzureBlobFSReadSettings",
             "recursive": true
           }
         }
       },
       {
         "name": "ForEach_File",
         "type": "ForEach",
         "typeProperties": {
           "items": "@activity('Get_All_Pending_Files').output.childItems",
           "isSequential": false,
           "batchCount": 20
         }
       }
     ]
   }
   ```

3. **Tumbling Window Trigger as Backup**:
   - "Added tumbling window trigger (every 5 minutes) to catch any missed events"
   - "Checks for unprocessed files, processes them"
   - "Safety net for Event Grid failures"

#### Event Grid vs. Schedule Triggers: Cost Comparison

**Scenario**: 50 vendors, average 10 files per day per vendor = 500 files/day

**Schedule Trigger (Every 15 minutes)**:
- Executions per day: 96 (24 hours × 4)
- Files found: 500
- Wasted executions: 96 - 500 = -404 (many executions find multiple files)
- Pipeline runs: 96
- **Cost**: ~$48/month (96 runs/day × $0.50/1000 runs × 30 days)

**Event-Based Trigger**:
- Executions per day: 500 (only when files arrive)
- Files found: 500
- Wasted executions: 0
- Pipeline runs: 500
- **Cost**: ~$7.50/month (500 runs/day × $0.50/1000 runs × 30 days)
- **Event Grid cost**: ~$0.60/month (500 events/day × $0.60/million events × 30 days)
- **Total**: ~$8.10/month

**Savings**: $48 - $8.10 = **$39.90/month (83% reduction)**

**Plus**:
- Reduced latency: 15 minutes → <1 second
- No polling overhead
- Better resource utilization

---

### Scenario 2: Custom Event-Driven Architecture

#### Business Requirement

"Manufacturing company has IoT devices sending telemetry. When temperature exceeds threshold, external monitoring system publishes custom event. ADF should immediately:
1. Query device history from SQL
2. Aggregate last 24 hours of data
3. Run ML model in Databricks for anomaly detection
4. If anomaly confirmed, trigger maintenance workflow"

#### Architecture

```
[IoT Monitoring System]
    ↓
[Custom Event Grid Topic]
    ↓
[Event Grid Subscription]
    ↓
[Azure Function: Event Handler]
    ↓
[ADF REST API: Trigger Pipeline]
    ↓
[ADF Pipeline: Query → Aggregate → ML → Action]
```

**Why Azure Function in the Middle?**
- ADF Event-Based Triggers only support Blob Storage events
- Custom events require webhook endpoint
- Azure Function provides webhook, enriches event data, calls ADF

#### Implementation

**Step 1: Custom Event Grid Topic**

```bash
# Create custom Event Grid topic
az eventgrid topic create \
  --name egt-iot-alerts \
  --resource-group rg-data-platform \
  --location eastus

# Get endpoint and key
az eventgrid topic show \
  --name egt-iot-alerts \
  --resource-group rg-data-platform \
  --query "endpoint" --output tsv

az eventgrid topic key list \
  --name egt-iot-alerts \
  --resource-group rg-data-platform \
  --query "key1" --output tsv
```

**Step 2: Azure Function - Event Grid Webhook**

```csharp
[FunctionName("EventGridWebhook")]
public static async Task<IActionResult> Run(
    [HttpTrigger(AuthorizationLevel.Function, "post", Route = null)] HttpRequest req,
    ILogger log)
{
    string requestBody = await new StreamReader(req.Body).ReadToEndAsync();
    var events = JsonConvert.DeserializeObject<EventGridEvent[]>(requestBody);
    
    foreach (var eventGridEvent in events)
    {
        // Handle validation event (required by Event Grid)
        if (eventGridEvent.EventType == "Microsoft.EventGrid.SubscriptionValidationEvent")
        {
            var validationData = JsonConvert.DeserializeObject<SubscriptionValidationEventData>(
                eventGridEvent.Data.ToString());
            
            return new OkObjectResult(new
            {
                validationResponse = validationData.ValidationCode
            });
        }
        
        // Handle custom IoT alert event
        if (eventGridEvent.EventType == "IoT.Device.TemperatureAlert")
        {
            dynamic eventData = JsonConvert.DeserializeObject(eventGridEvent.Data.ToString());
            
            string deviceId = eventData.deviceId;
            double temperature = eventData.temperature;
            double threshold = eventData.threshold;
            DateTime eventTime = eventGridEvent.EventTime;
            
            log.LogInformation($"Temperature alert for device {deviceId}: {temperature}°C (threshold: {threshold}°C)");
            
            // Enrich event with device metadata
            var deviceMetadata = await GetDeviceMetadata(deviceId, log);
            
            // Trigger ADF pipeline
            var adfClient = new DataFactoryManagementClient(new DefaultAzureCredential())
            {
                SubscriptionId = Environment.GetEnvironmentVariable("SubscriptionId")
            };
            
            var parameters = new Dictionary<string, object>
            {
                { "deviceId", deviceId },
                { "temperature", temperature },
                { "threshold", threshold },
                { "eventTime", eventTime.ToString("yyyy-MM-dd HH:mm:ss") },
                { "deviceLocation", deviceMetadata.Location },
                { "deviceType", deviceMetadata.Type },
                { "severity", CalculateSeverity(temperature, threshold) }
            };
            
            var runResponse = await adfClient.Pipelines.CreateRunAsync(
                resourceGroupName: "rg-data-platform",
                factoryName: "adf-prod",
                pipelineName: "PL_IoT_Anomaly_Detection",
                parameters: parameters);
            
            log.LogInformation($"Triggered ADF pipeline, Run ID: {runResponse.RunId}");
            
            // Store event for audit
            await StoreEventAudit(deviceId, eventTime, runResponse.RunId, log);
        }
    }
    
    return new OkObjectResult("Events processed successfully");
}

private static async Task<DeviceMetadata> GetDeviceMetadata(string deviceId, ILogger log)
{
    var connectionString = Environment.GetEnvironmentVariable("SqlConnectionString");
    
    using (var connection = new SqlConnection(connectionString))
    {
        await connection.OpenAsync();
        var command = new SqlCommand(
            "SELECT Location, DeviceType FROM dbo.Devices WHERE DeviceID = @DeviceId", 
            connection);
        command.Parameters.AddWithValue("@DeviceId", deviceId);
        
        using (var reader = await command.ExecuteReaderAsync())
        {
            if (await reader.ReadAsync())
            {
                return new DeviceMetadata
                {
                    Location = reader.GetString(0),
                    Type = reader.GetString(1)
                };
            }
        }
    }
    
    return new DeviceMetadata { Location = "Unknown", Type = "Unknown" };
}

private static string CalculateSeverity(double temperature, double threshold)
{
    double delta = temperature - threshold;
    
    if (delta > 20) return "Critical";
    if (delta > 10) return "High";
    if (delta > 5) return "Medium";
    return "Low";
}

private static async Task StoreEventAudit(string deviceId, DateTime eventTime, string runId, ILogger log)
{
    var connectionString = Environment.GetEnvironmentVariable("SqlConnectionString");
    
    using (var connection = new SqlConnection(connectionString))
    {
        await connection.OpenAsync();
        var command = new SqlCommand(@"
            INSERT INTO dbo.EventAudit (DeviceID, EventTime, ADFRunId, ProcessedTime)
            VALUES (@DeviceId, @EventTime, @RunId, GETDATE())", connection);
        
        command.Parameters.AddWithValue("@DeviceId", deviceId);
        command.Parameters.AddWithValue("@EventTime", eventTime);
        command.Parameters.AddWithValue("@RunId", runId);
        
        await command.ExecuteNonQueryAsync();
    }
}

public class DeviceMetadata
{
    public string Location { get; set; }
    public string Type { get; set; }
}

public class SubscriptionValidationEventData
{
    public string ValidationCode { get; set; }
}

public class EventGridEvent
{
    public string Id { get; set; }
    public string EventType { get; set; }
    public string Subject { get; set; }
    public DateTime EventTime { get; set; }
    public object Data { get; set; }
    public string DataVersion { get; set; }
}
```

**Step 3: Event Grid Subscription**

```bash
# Create Event Grid subscription pointing to Azure Function
az eventgrid event-subscription create \
  --name es-iot-to-function \
  --source-resource-id /subscriptions/{sub-id}/resourceGroups/rg-data-platform/providers/Microsoft.EventGrid/topics/egt-iot-alerts \
  --endpoint-type webhook \
  --endpoint https://func-iot-handler.azurewebsites.net/api/EventGridWebhook?code={function-key} \
  --event-delivery-schema eventgridschema \
  --included-event-types IoT.Device.TemperatureAlert \
  --max-delivery-attempts 3 \
  --event-ttl 1440
```

**Step 4: ADF Pipeline - Anomaly Detection**

```json
{
  "name": "PL_IoT_Anomaly_Detection",
  "activities": [
    {
      "name": "Query_Device_History",
      "type": "Lookup",
      "typeProperties": {
        "source": {
          "type": "SqlSource",
          "sqlReaderQuery": "SELECT * FROM dbo.DeviceTelemetry WHERE DeviceID = '@{pipeline().parameters.deviceId}' AND Timestamp >= DATEADD(hour, -24, GETDATE()) ORDER BY Timestamp"
        },
        "dataset": {"referenceName": "DS_AzureSQL_IoT"},
        "firstRowOnly": false
      }
    },
    {
      "name": "Write_To_ADLS",
      "type": "Copy",
      "dependsOn": [{"activity": "Query_Device_History", "dependencyConditions": ["Succeeded"]}],
      "inputs": [{"referenceName": "DS_AzureSQL_IoT"}],
      "outputs": [{"referenceName": "DS_ADLS_IoT_Staging"}],
      "typeProperties": {
        "source": {
          "type": "SqlSource",
          "sqlReaderQuery": "SELECT * FROM dbo.DeviceTelemetry WHERE DeviceID = '@{pipeline().parameters.deviceId}' AND Timestamp >= DATEADD(hour, -24, GETDATE())"
        },
        "sink": {
          "type": "ParquetSink"
        }
      }
    },
    {
      "name": "Run_Anomaly_Detection_Model",
      "type": "DatabricksNotebook",
      "dependsOn": [{"activity": "Write_To_ADLS", "dependencyConditions": ["Succeeded"]}],
      "typeProperties": {
        "notebookPath": "/Production/IoT/AnomalyDetection",
        "baseParameters": {
          "device_id": "@pipeline().parameters.deviceId",
          "data_path": "@{activity('Write_To_ADLS').output.filePath}",
          "current_temperature": "@pipeline().parameters.temperature",
          "severity": "@pipeline().parameters.severity"
        }
      },
      "linkedServiceName": {"referenceName": "LS_Databricks"}
    },
    {
      "name": "Check_Anomaly_Result",
      "type": "IfCondition",
      "dependsOn": [{"activity": "Run_Anomaly_Detection_Model", "dependencyConditions": ["Succeeded"]}],
      "typeProperties": {
        "expression": {
          "value": "@equals(activity('Run_Anomaly_Detection_Model').output.runOutput, 'ANOMALY_DETECTED')",
          "type": "Expression"
        },
        "ifTrueActivities": [
          {
            "name": "Create_Maintenance_Ticket",
            "type": "WebActivity",
            "typeProperties": {
              "url": "@pipeline().parameters.ServiceNowUrl",
              "method": "POST",
              "headers": {
                "Content-Type": "application/json",
                "Authorization": "Bearer @{activity('Get_ServiceNow_Token').output.token}"
              },
              "body": {
                "short_description": "Device @{pipeline().parameters.deviceId} anomaly detected",
                "description": "Temperature: @{pipeline().parameters.temperature}°C, Location: @{pipeline().parameters.deviceLocation}",
                "urgency": "@{if(equals(pipeline().parameters.severity, 'Critical'), '1', '2')}",
                "category": "IoT Device Maintenance"
              }
            }
          },
          {
            "name": "Send_Alert_To_Teams",
            "type": "WebActivity",
            "typeProperties": {
              "url": "@pipeline().parameters.LogicAppUrl",
              "method": "POST",
              "body": {
                "alertType": "AnomalyDetected",
                "deviceId": "@pipeline().parameters.deviceId",
                "temperature": "@pipeline().parameters.temperature",
                "location": "@pipeline().parameters.deviceLocation",
                "severity": "@pipeline().parameters.severity"
              }
            }
          }
        ],
        "ifFalseActivities": [
          {
            "name": "Log_False_Positive",
            "type": "SqlServerStoredProcedure",
            "typeProperties": {
              "storedProcedureName": "[dbo].[usp_LogFalsePositive]",
              "storedProcedureParameters": {
                "DeviceID": {"value": "@pipeline().parameters.deviceId"},
                "Temperature": {"value": "@pipeline().parameters.temperature"},
                "EventTime": {"value": "@pipeline().parameters.eventTime"}
              }
            },
            "linkedServiceName": {"referenceName": "LS_AzureSQL_IoT"}
          }
        ]
      }
    }
  ],
  "parameters": {
    "deviceId": {"type": "string"},
    "temperature": {"type": "float"},
    "threshold": {"type": "float"},
    "eventTime": {"type": "string"},
    "deviceLocation": {"type": "string"},
    "deviceType": {"type": "string"},
    "severity": {"type": "string"},
    "ServiceNowUrl": {"type": "string"},
    "LogicAppUrl": {"type": "string"}
  }
}
```

#### Publishing Events from External Systems

**Example: IoT Monitoring System Publishing to Event Grid**

```csharp
// External IoT monitoring system code
public async Task PublishTemperatureAlert(string deviceId, double temperature, double threshold)
{
    var topicEndpoint = "https://egt-iot-alerts.eastus-1.eventgrid.azure.net/api/events";
    var topicKey = GetFromKeyVault("EventGridTopicKey");
    
    var httpClient = new HttpClient();
    httpClient.DefaultRequestHeaders.Add("aeg-sas-key", topicKey);
    
    var eventGridEvent = new
    {
        id = Guid.NewGuid().ToString(),
        eventType = "IoT.Device.TemperatureAlert",
        subject = $"devices/{deviceId}",
        eventTime = DateTime.UtcNow.ToString("o"),
        dataVersion = "1.0",
        data = new
        {
            deviceId = deviceId,
            temperature = temperature,
            threshold = threshold,
            location = GetDeviceLocation(deviceId),
            timestamp = DateTime.UtcNow
        }
    };
    
    var events = new[] { eventGridEvent };
    var json = JsonConvert.SerializeObject(events);
    var content = new StringContent(json, Encoding.UTF8, "application/json");
    
    var response = await httpClient.PostAsync(topicEndpoint, content);
    response.EnsureSuccessStatusCode();
    
    Console.WriteLine($"Event published for device {deviceId}");
}
```

---

## Interview Questions for ADF + Event Grid

**Q: "When would you use Event-Based Triggers vs. Schedule Triggers vs. Tumbling Window Triggers?"**

✅ **Your Answer**: "Each has specific use cases:

**Event-Based Triggers**:
- **Use when**: Data arrives unpredictably, need immediate processing
- **Pros**: No polling, instant reaction, cost-efficient
- **Cons**: Limited to blob storage events (native), requires Azure Function for custom events
- **Example**: "Vendor files arriving throughout the day, process immediately"

**Schedule Triggers**:
- **Use when**: Predictable schedule, batch processing, source doesn't support events
- **Pros**: Simple, reliable, supports cron expressions
- **Cons**: Polling overhead, latency (runs on schedule even if no data)
- **Example**: "Daily data warehouse refresh at 2 AM"

**Tumbling Window Triggers**:
- **Use when**: Need to process time-based windows, dependencies between windows
- **Pros**: Automatic retry, window dependencies, backfill support
- **Cons**: More complex than schedule triggers
- **Example**: "Hourly aggregations with dependencies (hour 2 depends on hour 1)"

**In my last project**: "Used all three—Event Grid for real-time file processing (70% of pipelines), Schedule for nightly batch loads (20%), Tumbling Window for hourly aggregations with dependencies (10%)."

**Q: "How do you handle Event Grid failures and ensure no data loss?"**

✅ **Your Answer**: "Multi-layered approach:

1. **Event Grid Configuration**:
   - Max delivery attempts: 3
   - Event TTL: 24 hours
   - Dead letter destination: Blob Storage for failed events

2. **Idempotent Pipeline Design**:
   - Check if file already processed before starting
   - Use unique identifiers (file name + upload timestamp)
   - Atomic operations (all-or-nothing)

3. **Safety Net - Scheduled Reconciliation**:
   - Tumbling window trigger runs every hour
   - Queries for unprocessed files (compare landing vs. processed)
   - Processes any missed files
   - Alerts if gap > threshold

4. **Monitoring**:
   - Azure Monitor alerts on Event Grid delivery failures
   - Custom metric: event-to-execution latency
   - Dashboard showing: events received, pipelines triggered, success rate

5. **Dead Letter Processing**:
   - Separate pipeline monitors dead letter container
   - Analyzes failures, retries if transient
   - Creates tickets for persistent failures

**Real incident**: 'Event Grid had regional outage for 2 hours. Our reconciliation pipeline detected 47 unprocessed files, processed them automatically. Zero data loss, users never noticed.'"

**Q: "What are the cost implications of Event Grid vs. polling?"**

✅ **Your Answer**: "Significant cost differences:

**Event Grid Costs**:
- $0.60 per million events (first 100K free per month)
- Example: 10,000 events/day = 300K/month = $0.12/month
- ADF pipeline runs: Only when data arrives

**Polling (Schedule Trigger Every 5 Minutes)**:
- 288 pipeline runs per day (24 × 60 / 5)
- If only 100 files arrive per day: 188 wasted runs (65% waste)
- ADF cost: 288 runs/day × 30 days × $0.50/1000 = $4.32/month
- **Event Grid would be**: 100 runs/day × 30 days × $0.50/1000 + $0.18 = $1.68/month
- **Savings**: 61%

**Additional Benefits**:
- Reduced latency: 5 minutes → <1 second
- Lower compute costs (no empty runs)
- Better resource utilization

**When Polling Makes Sense**:
- Source doesn't support events (on-premises FTP)
- Predictable schedule (daily batch)
- Very low volume (cost difference negligible)

**In my last project**: 'Migrated 50 pipelines from 15-minute polling to Event Grid. Monthly cost: $240 → $85 (65% reduction). Latency: 7.5 minutes average → 2 seconds. ROI in first month.'"

---

## Summary: Integration Patterns Decision Matrix

| **Integration** | **Use When** | **Complexity** | **Cost** | **Latency** | **Big 4 Usage** |
|----------------|-------------|----------------|----------|-------------|----------------|
| **ADF + Databricks** | Complex transformations, ML, Delta Lake | High | Medium-High | Minutes | 90% of projects |
| **ADF + Synapse** | Data warehousing, analytics, lakehouse | Medium | High | Minutes-Hours | 80% of projects |
| **ADF + Logic Apps** | Notifications, approvals, workflow automation | Low | Low | Seconds | 70% of projects |
| **ADF + Azure Functions** | Custom logic, API integration, dynamic config | Medium | Low | Seconds | 60% of projects |
| **ADF + Event Grid** | Real-time processing, event-driven architecture | Medium | Very Low | <1 second | 40% of projects |

---

## Key Takeaways for Interviews

### What Interviewers Want to Hear

1. **You understand the "why"**: Don't just say "I used Databricks." Say "I used Databricks because ADF Data Flows couldn't handle the complex feature engineering for our ML model, and Databricks provided Delta Lake for ACID transactions."

2. **You've solved real problems**: "Event Grid was triggering duplicate pipelines. I implemented idempotent design with a processed files table and advanced filtering to eliminate duplicates."

3. **You think about cost**: "We evaluated Synapse Dedicated vs. Serverless. For our use case—ad-hoc queries on 10TB—Serverless saved $12K/month vs. always-on Dedicated pool."

4. **You know the trade-offs**: "Logic Apps are great for notifications, but for high-throughput scenarios (>1000 executions/second), Azure Functions with Service Bus is more cost-effective."

5. **You've worked in enterprise environments**: "We used Managed Identity for all service-to-service authentication. No secrets in code. Key Vault for external API keys. Private Link for network isolation."

### Red Flags to Avoid

❌ "I used Databricks for everything" (shows you don't understand when NOT to use it)
❌ "I hardcoded connection strings in pipelines" (security nightmare)
❌ "I've never used Event Grid" (missing modern architecture patterns)
❌ "I don't know the cost difference" (not production-ready)
❌ "I've only used ADF in isolation" (not enterprise experience)

### Pro Tips for Interviews

✅ **Prepare 2-3 detailed integration stories**: Pick your best Databricks, Synapse, and Logic Apps examples. Know every detail.

✅ **Draw architecture diagrams**: Practice drawing on whiteboard. Interviewers love visual thinkers.

✅ **Mention specific error codes**: "Error 2103: Cluster does not exist" shows real experience.

✅ **Quantify everything**: "Reduced latency from 15 minutes to 2 seconds" is better than "made it faster."

✅ **Show continuous learning**: "I recently implemented Event Grid for the first time. It reduced our costs by 65%."

---

## Hands-On Practice Exercises

### Exercise 1: ADF + Databricks Pipeline

**Task**: Build a pipeline that:
1. Copies 1GB CSV from Blob to ADLS Gen2
2. Triggers Databricks notebook to convert to Delta Lake
3. Runs data quality checks
4. Loads to Synapse if quality passes

**Expected Time**: 2 hours

**Validation**: 
- Pipeline completes in <10 minutes
- Delta table has ZORDER optimization
- Data quality failures go to quarantine

### Exercise 2: Event-Driven Architecture

**Task**: Implement event-driven file processing:
1. Upload file to Blob Storage
2. Event Grid triggers ADF pipeline within 5 seconds
3. Pipeline validates, transforms, loads
4. Logic App sends success/failure notification

**Expected Time**: 3 hours

**Validation**:
- End-to-end latency <30 seconds
- Duplicate events handled gracefully
- Failed events go to dead letter queue

### Exercise 3: Metadata-Driven Framework

**Task**: Build metadata-driven pipeline using Azure Functions:
1. Configuration table with 10 source tables
2. Azure Function generates dynamic queries
3. ADF ForEach loads all tables in parallel
4. Validation and logging for each table

**Expected Time**: 4 hours

**Validation**:
- Single pipeline handles all tables
- Adding new table requires only config change
- Execution log captures all metrics

---

## Conclusion

**You now have the knowledge to confidently discuss:**

✅ Why and when to integrate ADF with Databricks, Synapse, Logic Apps, Azure Functions, and Event Grid
✅ Real-world implementation patterns used in Big 4 consulting and Fortune 500 companies
✅ Common challenges and how to solve them
✅ Cost implications and optimization strategies
✅ Security best practices for enterprise environments
✅ How to speak about 5+ years of experience in interviews

**Next Steps**:

1. **Practice**: Build each integration pattern in your own Azure subscription
2. **Document**: Create your own "project stories" based on these patterns
3. **Prepare**: Write out answers to the interview questions in your own words
4. **Review**: Go through the other ADF documentation files for complete coverage

**Remember**: In interviews, it's not about memorizing JSON. It's about understanding **why** you made architectural decisions, **how** you solved problems, and **what** the business impact was.

**Good luck with your interviews! You've got this.** 🚀

---

## Additional Resources

- **Microsoft Learn**: [ADF Integration Patterns](https://learn.microsoft.com/azure/data-factory/)
- **GitHub**: [ADF Sample Pipelines](https://github.com/Azure/Azure-DataFactory)
- **Community**: [ADF Stack Overflow](https://stackoverflow.com/questions/tagged/azure-data-factory)

---

**Document Version**: 1.0  
**Last Updated**: November 2025  
**Author**: Enterprise Data Engineering Team  
**Feedback**: Please submit issues or suggestions via GitHub